{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Clinical Cancer Genomics Course","text":"<p>This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting.  The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. </p> <p>The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory.</p> <p>     The setup and practical exercises are a modified version of an existing course generously shared under an MIT and Creative Commons license.     This is the first version of the Clinical Cancer Genomics course, later updated iterations of the course will be shared according to the same principle.  </p>"},{"location":"#learning-outcomes","title":"Learning Outcomes","text":"<p>At the end of this course the student will be able to:</p> <ul> <li>Show a basic insight into the cancer genome.</li> <li>Understand how the cancer genome can be interrogated through tissues and liquid biopsies.</li> <li>Understand how to apply technology to obtain relevant information from the cancer genome.</li> <li>Understand the file formats used in high throughput sequencing.</li> <li>use the command line and running bioinformatic tools.</li> <li>Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline.</li> <li>Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes.</li> <li>Call somatic and germline variation.</li> <li>Curate somatic and germline variation for a clinical setting.</li> <li>Annotate somatic and germline variation.</li> <li>Visualise data in R.</li> <li>Use online resources such as genome browsers and portals for variant annotation.</li> </ul>"},{"location":"#contents-of-the-course","title":"Contents of the course","text":"<p>An introduction to the cancer genome and mutational processes in cancer. </p> <ul> <li>Overview of disease heterogeneity \u2013 the concept of cancer subtypes. </li> <li>The clinical impact of analysing the cancer genome. </li> <li>The concept of personalized therapy by tumour profiling. </li> <li>Intra-patient tumour heterogeneity. <ul> <li>How to enable cancer genomics through tissues and liquid biopsies </li> <li>How to apply to high-throughput methodology to interrogate the cancer genome. </li> <li>Illumina sequencing file formats. </li> </ul> </li> <li>Bioinformatics pipelines. </li> <li>Processing of DNA and RNA sequencing data. </li> <li>QC of both DNA and RNA sequencing data </li> <li>Calling somatic and germline variation: <ul> <li>Point mutations and indels.  </li> <li>Copy-number alterations. </li> <li>Structural variation. </li> </ul> </li> <li>File formats for variant calling. </li> <li>Annotating somatic and germline variation. </li> <li>How to curate somatic- and germline variation for clinical use.</li> </ul>"},{"location":"#literature-and-other-teaching-material","title":"Literature and other teaching material","text":"<p>Recommended reading before the course:</p> <ol> <li> Clinical cancer genomic profiling </li> <li> Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples </li> </ol> <p>Note:</p> <ol> <li>Lectures in the morning with computer exercises in the afternoon.</li> <li>100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session.</li> <li>Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam.</li> </ol>"},{"location":"#additional-information","title":"Additional Information","text":"<p> 2023-04-24 to 2023-04-28 Monday - Friday ( 09:00 - 17:00 )  johan.lindberg@ki.se</p>"},{"location":"annotations/","title":"HUMAN GENOME ANNOTATION FILES","text":""},{"location":"annotations/#files-for-annotation-of-variation-in-the-human-genome","title":"Files for annotation of variation in the human genome","text":"<pre><code>cd ~/workspace/inputs/references/\nmkdir -p gatk\ncd gatk\n\n#In case gsutil needs to be installed\nconda install gsutil\n\n# SNP calibration call sets - dbsnp, hapmap, omni, and 1000G\n# Runtime: &lt; 2min\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf .\n# Runtime: ~ 2min\nbgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz .\n\n# Indel calibration call sets - dbsnp, Mills\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz .\n\n# Interval lists that can be used to parallelize certain GATK tasks\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list .\ngsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ .\n\n# list the files we just downloaded\nls -lh\n</code></pre>"},{"location":"annotations/#index-the-variation-files","title":"Index the variation files","text":"<pre><code>cd ~/workspace/inputs/references/gatk/\n\n#SNP calibration call sets - dbsnp, hapmap, omni, and 1000G\n# Runtime: ~ 4min\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz\n# Runtime: ~ 3min\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n\n#Indel calibration call sets - dbsnp, Mills\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\n</code></pre>"},{"location":"annotations/#interval-files-and-coordinates-for-the-exome-sequencing-assay","title":"Interval files and coordinates for the exome sequencing assay","text":"<pre><code># change directories\nmkdir -p ~/workspace/inputs/references/exome\ncd ~/workspace/inputs/references/exome\n\n# download the files\nwget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\nunzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\n\n# remove the zip\nrm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\n\n# Lift-over of the Roche coordinates from hg19 to the hg38 assembly.\n# download the software\ncd ~/workspace/bin\nwget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver\nchmod +x liftOver\n\n# change to the appropriate directory\ncd ~/workspace/inputs/references/exome\n\n# download the chain file\nwget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz\n\n# run liftover\nliftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed  hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed\n\nliftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed  hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed\n\n# create a version in standard bed format (chr, start, stop)\ncut -f 1-3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed\n\ncut -f 1-3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed\n\n# take a quick look at the format of these files\nhead SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed\nhead SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed\n</code></pre>"},{"location":"annotations/#calculate-the-size-of-the-seqcap-v3-exome","title":"Calculate the size of the SeqCap v3 exome","text":"<pre><code>#This can be done in many ways - give it a try yourself before trying the code below and compare results\n\n# first sort the bed files and store the sorted versions\nbedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed\nbedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed\n\n# now merge the bed files to collapse any overlapping regions so they are not double counted.\nbedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed\n\nbedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed\n\n# finally use a Perl one liner to determine the size of the files in Mb\nFILES=(SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed)\necho ${FILES[0]}\n\nfor FILE in ${FILES[@]}\ndo\necho \"--------------------------------------------------------\"\necho $FILE\n#With merge\ncat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(&lt;&gt;){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";'\ndone\n# note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable?\n\n# now create a subset of these bed files\ngrep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed &gt; exome_regions.bed\n#When creating files, make a habit to investigate the output to avoid downstream confusion\nhead -n 10 exome_regions.bed\n\ngrep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed &gt; probe_regions.bed\nhead -n 10 probe_regions.bed\n\n# clean up intermediate files\n#rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed\n</code></pre>"},{"location":"annotations/#create-an-inverval-list-for-the-exome-bed-files","title":"Create an inverval list for the exome bed files","text":"<pre><code># first for the complete exome and probe bed file\ncd ~/workspace/inputs/references/\nmkdir temp\ncd temp\nwget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict\ncd ~/workspace/inputs/references/exome\njava -jar $PICARD BedToIntervalList I=SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O=SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD=~/workspace/inputs/references/temp/ref_genome.dict\njava -jar $PICARD BedToIntervalList I=SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O=SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD=~/workspace/inputs/references/temp/ref_genome.dict\nrm -fr ~/workspace/inputs/references/temp/\n\n# next for our subset exome and probe regions file\ncd ~/workspace/inputs/references/exome\njava -jar /usr/local/bin/picard.jar BedToIntervalList I=exome_regions.bed O=exome_regions.bed.interval_list SD=~/workspace/inputs/references/genome/ref_genome.dict\njava -jar /usr/local/bin/picard.jar BedToIntervalList I=probe_regions.bed O=probe_regions.bed.interval_list SD=~/workspace/inputs/references/genome/ref_genome.dict\n</code></pre>"},{"location":"installation/","title":"INSTALLATION NOTES","text":"<p>This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies.</p>"},{"location":"installation/#prepare-for-installation","title":"Prepare for installation","text":"<p>For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started.</p> <pre><code># make a bin directory\nmkdir -p ~/workspace/bin\n</code></pre>"},{"location":"installation/#install-samtools","title":"Install Samtools","text":"<p>Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable.</p> <pre><code># change to bin directory\ncd ~/workspace/bin\n\n# download and extract the source code\nwget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2\ntar --bzip2 -xvf samtools-1.14.tar.bz2\n\n# configure and compile\ncd samtools-1.14/\n./configure --prefix=/home/ubuntu/workspace/bin/samtools-1.14/\nmake\nmake install\nln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools\n\n# check instalation\n~/workspace/bin/samtools --help\n</code></pre>"},{"location":"installation/#install-picard","title":"Install PICARD","text":"<p>PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file.</p> <pre><code># change to the bin directory and download the jar file\ncd ~/workspace/bin\nwget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar\n\n# check the installation\njava -jar ~/workspace/bin/picard.jar -h\n</code></pre>"},{"location":"installation/#install-bwa","title":"Install BWA","text":"<p>BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa.</p> <pre><code># change to the bin folder, download, and extract the source code\ncd ~/workspace/bin\nwget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2\ntar --bzip2 -xvf bwa-0.7.17.tar.bz2\n\n\n# build the software\ncd  bwa-0.7.17\nmake\n\n# make symlink\nln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa\n\n# check the installation\n~/workspace/bin/bwa\n</code></pre>"},{"location":"installation/#install-gatk-4","title":"Install GATK 4","text":"<p>GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate.</p> <pre><code># download and unzip\ncd ~/workspace/bin\nwget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip\nunzip gatk-4.2.3.0.zip\n\n# make sure ubuntu user can create their own conda environments\nsudo chown -R ubuntu:ubuntu /home/ubuntu/.conda\n\n# create conda environment for gatk\ncd gatk-4.2.3.0/\nconda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk\n\n# make symlink\nln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk\n\n# test installation\nconda activate ~/workspace/bin/conda/gatk\n~/workspace/bin/gatk\n\n# to exit the virtual environment\nconda deactivate\n</code></pre>"},{"location":"installation/#install-vep-934","title":"Install VEP 93.4","text":"<p>VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes.</p> <p>Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this.</p> <p>When prompted by the install step below use these answers:</p> <p>Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER]</p> <pre><code># download and unzip vep\ncd ~/workspace/bin\nwget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip\nunzip 104.3.zip\n\n# run the INSTALL.pl script provided by VEP\ncd ensembl-vep-release-104.3/\n/usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI'\n/usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache\n#1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER]\n#2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked)\n#3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER]\n#4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER]\n#5. Do you want to install any plugins (y/n)? n [ENTER]\n\n# make a symlink\nln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep\n\n# test the Installation\n~/workspace/bin/vep --help\n</code></pre>"},{"location":"installation/#install-varscan","title":"Install Varscan","text":"<p>Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar</p> <pre><code># Install Varscan\ncd ~/workspace/bin\ncurl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar\njava -jar ~/workspace/bin/VarScan.v2.4.2.jar\n</code></pre>"},{"location":"installation/#install-bcftools","title":"Install BCFtools","text":"<p>BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory.</p> <pre><code>cd ~/workspace/bin\ncurl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2\ntar --bzip2 -xvf bcftools-1.14.tar.bz2\n\n#install the software\ncd bcftools-1.14\nmake -j\nmake prefix=~/workspace/bin/bcftools-1.14 install\nln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools\n\n# test installation\n~/workspace/bin/bcftools -h\n</code></pre>"},{"location":"installation/#install-strelka","title":"Install Strelka","text":"<p>Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version.</p> <pre><code># download and extract\ncd ~/workspace/bin\n\nconda create --name strelka-env python=2.7\n\ncurl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2\ntar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2\n\n# test installation\npython2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h\n</code></pre>"},{"location":"installation/#install-sambamba","title":"Install Sambamba","text":"<p>Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive.</p> <pre><code># download and extract\ncd ~/workspace/bin\ncurl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2\ntar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2\n\n# create symlink\nln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba\n\n# test installation\n~/workspace/bin/sambamba\n</code></pre>"},{"location":"installation/#install-hisat2","title":"Install HISAT2","text":"<p>HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed.</p> <pre><code># download and extract\ncd ~/workspace/bin\nwget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip\nunzip hisat2-2.1.0-Linux_x86_64.zip\n\n# create symlink\nln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2\n\n# test installation\n~/workspace/bin/hisat2 --help\n</code></pre>"},{"location":"installation/#install-stringtie","title":"Install StringTie","text":"<p>StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find.</p> <pre><code># download and extract\ncd ~/workspace/bin\nwget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz\ntar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz\n\n# make symlink\nln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie\n\n# test installation\n~/workspace/bin/stringtie -h\n</code></pre>"},{"location":"installation/#install-gffcompare","title":"Install Gffcompare","text":"<p>Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink.</p> <pre><code># download and extract\ncd ~/workspace/bin\nwget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz\ntar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz\n\n# make symlink\nln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare\n\n# check Installation\n~/workspace/bin/gffcompare\n</code></pre>"},{"location":"installation/#install-r","title":"Install R","text":"<p>R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location.</p> <pre><code>## download and extract\ncd ~/workspace/bin\nwget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz\ntar -zxvf R-3.5.1.tar.gz\n\n## configure the installation, build the code\ncd R-3.5.1\n./configure --prefix=/home/ubuntu/workspace/bin --with-x=no\nmake\nmake install\n\n## make symlinks\nln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript\nln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R\n\n## test installation\ncd ~/workspace/bin\n~/workspace/bin/Rscript --version\n\n## install additional packages\n~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")'\n</code></pre>"},{"location":"installation/#install-copycat","title":"Install copyCat","text":"<p>copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository.</p> <pre><code># Install R Library dependencies\n~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))'\n\n# install copyCat\n~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")'\n</code></pre>"},{"location":"installation/#install-cnvnator","title":"Install CNVnator","text":"<p>CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory.</p> <pre><code># download and decompress\ncd ~/workspace/bin\n\n#download and install dependency package \"root\" from Cern (https://root.cern/install/). \n\ncurl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz\ntar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz\nsource root/bin/thisroot.sh\n\n\nwget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip\nunzip CNVnator_v0.3.3.zip\n\n# make the samtools dependency distributed with CNVnator\ncd CNVnator_v0.3.3/src/samtools\nmake\n\n# make CNVnator\ncd ../\nmake\n\n# make a symlink\nln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator\n\n# test installation\n~/workspace/bin/cnvnator\n</code></pre>"},{"location":"installation/#install-cnvkit","title":"Install CNVkit","text":"<p>CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up.</p> <pre><code>## download and unzip\ncd ~/workspace/bin\nwget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip\nunzip v0.9.9.zip\n\n## add conda channels\nconda config --add channels defaults\nconda config --add channels conda-forge\nconda config --add channels bioconda\n\n## create conda environment\nconda create -n cnvkit python=3\nln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py\n\n# test installation\nsource activate cnvkit\n#install all dependencies\n~/workspace/bin/cnvkit.py --help\n\n# to exit the virtual environment\nsource deactivate\n</code></pre>"},{"location":"installation/#install-kallisto","title":"Install Kallisto","text":"<p>Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github.</p> <pre><code># download and extract\ncd ~/workspace/bin\nwget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz\ntar -zxvf kallisto_linux-v0.46.2.tar.gz\nmv kallisto kallisto_linux-v0.46.2\n\n# make symlink\nln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto\n\n# test installation\n~/workspace/bin/kallisto\n</code></pre>"},{"location":"installation/#install-pizzly","title":"Install Pizzly","text":"<p>Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started.</p> <pre><code># download and extract\ncd ~/workspace/bin\nmkdir pizzly-v0.37.3\ncd pizzly-v0.37.3\nwget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz\ntar -zxvf pizzly_linux.tar.gz\n\n# make symlink\nln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly\n\n# test executable\n~/workspace/bin/pizzly --help\n</code></pre>"},{"location":"installation/#manta","title":"Manta","text":"<p>Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints.</p> <pre><code># download and extract\ncd ~/workspace/bin\nwget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2\ntar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2\n\n#we can use strelka-env for this also\n\nconda activate strelka-env\n\n# test installation\npython2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help\n\nconda deactivate\n</code></pre>"},{"location":"installation/#mosdepth","title":"mosdepth","text":"<p>mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation.</p> <pre><code># add the bioconda channel\nconda config --add channels bioconda\n\n# install mosdepth with the conda package manager\nconda install mosdepth\n</code></pre>"},{"location":"installation/#bam-readcount","title":"bam-readcount","text":"<p>bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead.</p> <pre><code># install bam-readcount\ncd ~/workspace/bin\ngit clone https://github.com/genome/bam-readcount.git\nmv bam-readcount bam-readcount-latest\ncd bam-readcount-latest\nexport SAMTOOLS_ROOT=/home/ubuntu//workspace/bin/samtools-1.14\ncmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest\nmake\n\n# create symlink\nln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount\n\n# test installation\n~/workspace/bin/bam-readcount\n</code></pre>"},{"location":"installation/#vt","title":"vt","text":"<p>vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants.</p> <pre><code>#install vt\ncd ~/workspace/bin\nconda install -c bioconda vt\n\n# create symlink\nln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt\n\n# test installation\n~/workspace/bin/vt\n</code></pre>"},{"location":"installation/#vcf-annotation-tools","title":"vcf-annotation-tools","text":"<p>VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files.</p> <pre><code>#install vcf-annotation-tools\npip install vcf-annotation-tools\n\n#testing Installation\nvcf-readcount-annotator -h\n</code></pre>"},{"location":"installation/#install-seqtk","title":"Install seqtk","text":"<p>Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module.</p> <pre><code># Download\ncd ~/workspace/bin\ngit clone https://github.com/lh3/seqtk.git seqtk.v1\n\n# Install\ncd seqtk.v1\nmake\n# (Ignore warning message)\nmake install\n\n# Check install\nln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk\n~/workspace/bin/seqtk\n</code></pre>"},{"location":"installation/#activate-the-tools-environment","title":"Activate the tools environment","text":"<pre><code>source .bashrc\n</code></pre>"},{"location":"references/","title":"HUMAN GENOME REFERENCE FILES","text":""},{"location":"references/#download-a-refernce-file-for-human-genome","title":"Download a refernce file for human genome","text":"<pre><code># Make sure CHRS environment variable is set. \necho $CHRS\n\n# Create a directory for reference genome files and enter this dir.\nmkdir -p ~/workspace/inputs/references/genome\ncd ~/workspace/inputs/references/genome\n\n# Dowload human reference genome files from the course data server.\nwget http://genomedata.org/pmbio-workshop/references/genome/$CHRS/ref_genome.tar\n\n# Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose,\n#  `f` for file).\ntar -xvf ref_genome.tar\n\n# View contents.\ntree\n\n# Remove the archive.\nrm -f ref_genome.tar\n\n# Uncompress the reference genome FASTA file.\ngunzip ref_genome.fa.gz\n\n# View contents.\ntree\n\n# Check the chromosome headers in the fasta file.\ncat ref_genome.fa | grep -P \"^&gt;\"\n</code></pre>"},{"location":"references/#split-the-long-fasta-by-chromosome","title":"Split the long fasta by chromosome","text":"<pre><code># Make new directory and change directories.\nmkdir -p ~/workspace/inputs/references/genome/ref_genome_split/\ncd ~/workspace/inputs/references/genome\n\n# Split.\nfaSplit byname ref_genome.fa ./ref_genome_split/\n</code></pre>"},{"location":"references/#explore-the-contents-of-the-reference-genome-file","title":"Explore the contents of the reference genome file","text":"<pre><code># View the first 10 lines of this file. Note the header line starting with `&gt;`. \n# Why does the sequence look like this?\ncd ~/workspace/inputs/references/genome\nhead -n 10 ref_genome.fa\n\n# Pull out only the header lines.\ngrep \"&gt;\" ref_genome.fa\n\n# How many lines and characters are in this file?\nwc ref_genome.fa\n\n# How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome.\ngrep -v \"&gt;\" ref_genome.fa | wc\n\n# How long does that command take to run?\ntime grep -v \"&gt;\" ref_genome.fa | wc\n\n# View 10 lines from approximately the middle of this file\nhead -n 2500000 ref_genome.fa | tail\n\n# What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)?\n# Runtime: ~30s\ncat ref_genome.fa | grep -v \"&gt;\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}'\n# What does each of these bases refer to? What are the \"unexpected bases\"?\n</code></pre>"},{"location":"references/#index-the-fasta-files","title":"Index the fasta files","text":"<pre><code># first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though!\ncd ~/workspace/inputs/references/genome\nrm -f ref_genome.fa.fai ref_genome.dict\n\n# Use samtools to create a fasta index file.\nsamtools faidx ref_genome.fa\n\n# View the contents of the index file.\nhead ref_genome.fa.fai\n\n# Use picard to create a dictionary file.\njava -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict\n\n# View the content of the dictionary file.\ncat ref_genome.dict\n\n#Also index the split chromosomes.\nsamtools faidx ./ref_genome_split/chr6.fa\nsamtools faidx ./ref_genome_split/chr17.fa\n\n# Create reference index for the genome to use BWA\nbwa index ref_genome.fa\n</code></pre>"},{"location":"references/#transcriptome-reference-files","title":"Transcriptome reference files","text":"<pre><code># Make sure CHRS environment variable is set.\necho $CHRS\n\n# Create a directory for transcriptome input files.\nmkdir -p ~/workspace/inputs/references/transcriptome\ncd ~/workspace/inputs/references/transcriptome\n\n# Download the files.\nwget http://genomedata.org/pmbio-workshop/references/transcriptome/$CHRS/ref_transcriptome.gtf\nwget http://genomedata.org/pmbio-workshop/references/transcriptome/$CHRS/ref_transcriptome.fa\n\n# Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display.\nless -p start_codon -S ref_transcriptome.gtf\n</code></pre>"},{"location":"references/#explore-the-contents-of-the-transcriptome-reference-files","title":"Explore the contents of the transcriptome reference files","text":"<pre><code>#How many chromsomes are represented?\ncut -f1 ref_transcriptome.gtf | sort | uniq -c\n\n# How many unique gene IDs are in the .gtf file?\n# We can use a perl command-line command to find out:\nperl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l\n\n# what are all the feature types listed in the third column of the GTF?\n# how does the following command (3 commands piped together) answer that question?\ncut -f 3 ref_transcriptome.gtf | sort | uniq -c\n</code></pre>"},{"location":"references/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome","text":"<pre><code>cd ~/workspace/inputs/references/transcriptome\n\n# Create a database of observed splice sites represented in our reference transcriptome GTF\n~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf &gt; splicesites.tsv\nhead splicesites.tsv\n\n# Create a database of exon regions in our reference transcriptome GTF\n~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf &gt; exons.tsv\nhead exons.tsv\n\n# build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps\n# specify to use 8 threads with the `-p 8` option\n# run time for this index is ~5 minutes\n~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome\n</code></pre>"},{"location":"references/#create-a-reference-transcriptome-index-for-use-with-kallisto","title":"Create a reference transcriptome index for use with Kallisto","text":"<pre><code>cd ~/workspace/inputs/references/transcriptome\nmkdir kallisto\ncd kallisto\n\n# tidy up the headers to just include the ensembl transcript ids\ncat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \"&gt;$1\\n\"}else{print $_}' &gt; ref_transcriptome_clean.fa\n\n# run time for this index is ~30 seconds\nkallisto index --index=ref_transcriptome_kallisto_index ref_transcriptome_clean.fa\n</code></pre>"},{"location":"schedule/","title":"Schedule","text":""},{"location":"schedule/#day-1-2023-04-24-monday","title":"Day 1 : 2023-04-24 - Monday","text":"Time Topic Responsible Location 09:00-9:45 Course overview <ul> <li>Learning outcomes</li> <li>Examination, finish labs (computer work and questions during the labwork) and show a teacher.</li> </ul> Johan Wargentin 09:45-10:00 Break 10:00-10:45                          An Introduction to the cancer genome and mutational processes in cancer (conti). Johan Wargentin 10:45-11:00 Break 11:00-11:45 Practical considerations for performing cancer genomics. Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Lab sessions <ul> <li>Basic unix</li> <li>AWK</li> <li>Investigate the workspace directories, raw data and install R, R-packages</li> <li>ggplot tutorial</li> </ul> Johan &amp; Rebecka &amp; Venki Wargentin"},{"location":"schedule/#day-2-2023-04-25-tuesday","title":"Day 2 : 2023-04-25 - Tuesday","text":"Time Topic Responsible Location 09:00-9:45                          Liquid biopsies                       Johan North 3 09:45-10:00 Break 10:00-10:45                          The clinical impact of analysing the cancer genome.                      Felix North 3 10:45-11:00 Break 11:00-11:45 Lab intro. <ul> <li>Add new human genome reference paper.</li> <li>Redo (read about alignment etc)</li> Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Lab sessions <ul> <li>Looking into the human genome reference.</li> <li>Annotation files needed for bioinformatic processing.</li> <li>Executing commands for processing and checking the quality of DNA sequencing data.</li> <li>Introduction to the Integrative Genomics Viewer (IGV).</li> </ul> Johan &amp; Rebecka &amp; Venki Wargentin"},{"location":"schedule/#day-3-2023-04-26-wednesday","title":"Day 3 : 2023-04-26 - Wednesday","text":"Time Topic Responsible Location 09:00-9:45                          Somatic and germline variant callers.                      Rebecka Wargentin 09:45-10:00 Break 10:00-10:45                          RNA-Seq Data Analysis.                      Sinja Wargentin 10:45-11:00 Break 11:00-11:45                         Processing and QC of DNA- and RNA sequencing data.                       Rebecka Wargentin 12:00-13:00 Lunch 13:00-17:00 Practical session <ul> <li>Calling somatic and germline mutations.</li> <li>Filtering variants. </li> <li>Familiarizing with the variant formats.</li> <li>Visualising variants in IGV.</li> </ul> Johan &amp; Rebecka &amp; Venki &amp; Sinja Wargentin"},{"location":"schedule/#day-4-2023-04-27-thursday","title":"Day 4 : 2023-04-27 - Thursday","text":"Time Topic Responsible Location 09:00-9:45                          Somatic and germline copy-number alterations.                      Markus Wargentin 09:45-10:00 Break 10:00-10:45                          Structural variation.                      Johan Wargentin 10:45-11:00 Break 11:00-11:45                          Lab introduction.                      Markus &amp; Johan &amp; Sinja Wargentin 12:00-13:00 Lunch 13:00-17:00 Lab sessions <ul> <li>RNA analysis, expression and fusion calling.</li> <li>Analysis of copy number alterations. </li> <li>Identification of structural variants.</li> <li>Inspecting variants</li> </ul> Johan &amp; Markus &amp; Sinja Wargentin"},{"location":"schedule/#day-5-2023-04-28-friday","title":"Day 5 : 2023-04-28 - Friday","text":"Time Topic Responsible Location 09:00-9:45                          Continue labwork from yesterday.                      Johan Wargentin 09:45-10:00 Break 10:00-10:45                          How to curate somatic and germline variation for clinical use.                      Johan Wargentin 10:45-11:00 Break 11:00-11:45                          Clinical Interpretation of genomic data.                          Databases, visualization and interpretation.                      Venki Wargentin 12:00-13:00 Lunch 13:00-13:45                            Bioinformatics pipelines &amp; HTC computing environments                              Venki Wargentin 13:45-14:00 Break 14:00-17:00  --continue leftover labwork. <ul> <li>Data visualisation/interpretation exercises.</li> <li>Finishing off any remaining labwork from previous days.</li> <li>Seek out and brainstorm personal projects with course teachers.</li> </ul> Johan &amp; Rebecka &amp; Venki &amp; Sinja Wargentin"},{"location":"aws/aws_ec2_instances/","title":"AWS Login","text":""},{"location":"aws/aws_ec2_instances/#connect-to-your-linux-instance-using-an-ssh-client","title":"Connect to your Linux instance using an SSH client","text":"<pre><code>ssh -i course-setup-student-key.pem &lt;Instance ID&gt;\n</code></pre> <p>Note: Please use the new <code>course-setup-student-key.pem/course-setup-student-key.ppk</code> key file for aws login.</p>"},{"location":"aws/aws_ec2_instances/#list-of-user-and-instance-details","title":"List of user and instance details","text":"Name Instance ID Status sarath - STOPPED"},{"location":"lab_session/R_for_bioinformatics/","title":"Programming in R for Bioinformatics","text":"<p>Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer.</p>"},{"location":"lab_session/R_for_bioinformatics/#what-is-r","title":"What is R?","text":"<p>R is a language and environment for statistical computing and graphics developed in 1993. It provides a wide variety of statistical and graphical techniques (linear and nonlinear modeling, statistical tests, time series analysis, classification, clustering, \u2026), and is highly extensible, meaning that the user community can write new R tools. It is a GNU project (Free and Open Source).</p> <p>The R language has its roots in the S language and environment which was developed at Bell Laboratories (formerly AT&amp;T, now Lucent Technologies) by John Chambers and colleagues. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and now, R is developed by the R Development Core Team, of which Chambers is a member. R is named partly after the first names of the first two R authors (Robert Gentleman and Ross Ihaka), and partly as a play on the name of S. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.</p>"},{"location":"lab_session/R_for_bioinformatics/#some-of-rs-strengths","title":"Some of R\u2019s strengths:","text":"<ol> <li>The ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.</li> <li>It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.</li> <li>R can be extended (easily) via packages.</li> <li>R has its own LaTeX-like documentation format, which is used to supply comprehensive documentation, both on-line in a number of formats and in hardcopy.</li> <li>It has a vast community both in academia and in business.</li> <li>It\u2019s FREE!</li> </ol>"},{"location":"lab_session/R_for_bioinformatics/#the-r-environment","title":"The R environment","text":"<p>R is an integrated suite of software facilities for data manipulation, calculation and graphical display. It includes</p> <ul> <li>an effective data handling and storage facility,</li> <li>a suite of operators for calculations on arrays, in particular matrices,</li> <li>a large, coherent, integrated collection of intermediate tools for data analysis,</li> <li>graphical facilities for data analysis and display either on-screen or on hardcopy, and</li> <li>a well-developed, and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities.</li> <li>The term \u201cenvironment\u201d is intended to characterize it as a fully planned and coherent system, rather than an incremental accretion of very specific and inflexible tools, as is frequently the case with other data analysis software.</li> </ul> <p>R, like S, is designed around a true computer language, and it allows users to add additional functionality by defining new functions. Much of the system is itself written in the R dialect of S, which makes it easy for users to follow the algorithmic choices made. For computationally-intensive tasks, C, C++ and Fortran code can be linked and called at run time. Advanced users can write C code to manipulate R objects directly.</p> <p>Many users think of R as a statistics system. The R group prefers to think of it of an environment within which statistical techniques are implemented.</p>"},{"location":"lab_session/R_for_bioinformatics/#the-r-homepage","title":"The R Homepage","text":"<p>The R homepage has a wealth of information on it,</p> <p>R-project.org</p> <p>On the homepage you can learn more about R</p> <ul> <li>Download R</li> <li>Get Documentation (official and user supplied)</li> <li>Get access to CRAN \u2018Comprehensive R archival network\u2019</li> <li>RStudio</li> <li>RStudio started in 2010, to offer R a more full featured integrated development environment (IDE) and modeled after matlabs IDE.</li> </ul>"},{"location":"lab_session/R_for_bioinformatics/#rstudio-has-many-features","title":"RStudio has many features:","text":"<ul> <li>syntax highlighting</li> <li>code completion</li> <li>smart indentation</li> <li>\u201cProjects\u201d</li> <li>workspace browser and data viewer</li> <li>embedded plots</li> <li>Markdown notebooks, Sweave authoring and knitr with one click pdf or html</li> <li>runs on all platforms and over the web</li> <li>etc. etc. etc.</li> </ul> <p>RStudio and its team have contributed to many R packages. These include:</p> <p>Tidyverse \u2013 R packages for data science, including ggplot2, dplyr, tidyr, and purrr Shiny \u2013 An interactive web technology RMarkdown \u2013 Insert R code into markdown documents knitr \u2013 Dynamic reports combining R, TeX, Markdown &amp; HTML packrat \u2013 Package dependency tool devtools \u2013 Package development tool</p>"},{"location":"lab_session/R_for_bioinformatics/#1-getting-started","title":"1. Getting started","text":"<p>Let\u2019s start RStudio</p> <p></p>"},{"location":"lab_session/R_for_bioinformatics/#2-open-a-new-rscript-file","title":"2. Open a new RScript File","text":"<p>File -&gt; New File -&gt; R Script</p> <p>RStudio_newfile</p> <p>Then save the new empty file as Intro2R.R</p> <p>File -&gt; Save as -&gt; Intro2R.R</p>"},{"location":"lab_session/R_for_bioinformatics/#3-basics-of-your-environment","title":"3. Basics of your environment","text":"<p>The R prompt is the \u2018&gt;\u2019 , when R is expecting more (command is not complete) you see a \u2018+\u2019</p> <p></p>"},{"location":"lab_session/R_for_bioinformatics/#4-writing-and-running-r-commands","title":"4. Writing and running R commands","text":"<p>In the source editor (top left by default) type</p> <p><code>getwd()</code> Then on the line Control + Enter (Linux/Windows), Command + Enter (Mac) to execute the line.</p>"},{"location":"lab_session/R_for_bioinformatics/#5-the-assignment-operator-vs-equals","title":"5. The assignment operator ( &lt;- ) vs equals ( = )","text":"<p>The assignment operator is used assign data to a variable</p> <p><pre><code>x &lt;- 1:10\nx\n[1]  1  2  3  4  5  6  7  8  9 10\n</code></pre> In this case, the equal sign works as well</p> <p><pre><code>x = 1:10\nx\n[1]  1  2  3  4  5  6  7  8  9 10\n</code></pre> But you should NEVER EVER DO THIS</p> <p><pre><code>1:10 -&gt; x\nx\n[1]  1  2  3  4  5  6  7  8  9 10\n</code></pre> The two act the same in most cases. The difference in assignment operators is clearer when you use them to set an argument value in a function call. For example:</p> <p><pre><code>rm(x)  # first remove the previously defined x variable\nmedian(x = 1:10)\nx \nError: object 'x' not found\n</code></pre> In this case, x is declared within the scope of the function, so it does not exist in the user workspace.</p> <p><pre><code>median(x &lt;- 1:10)\nx\n[1]  1  2  3  4  5  6  7  8  9 10\n</code></pre> In this case, x is declared in the user workspace, so you can use it after the function call has been completed. There is a general preference among the R community for using &lt;- for assignment (other than in function signatures)</p>"},{"location":"lab_session/R_for_bioinformatics/#6-the-rstudio-cheat-sheets","title":"6. The RStudio Cheat Sheets","text":"<p>rstudio-ide.pdf</p> <p>spend 15m getting to know RStudio a little</p>"},{"location":"lab_session/R_for_bioinformatics/#import-and-export-data-in-r","title":"Import and export data in R","text":"<p>R base function read.table() is a general funciton that can be used to read a file in table format. The data will be imported as a data frame. To download the data, run the following in your terminal/commnad line if you have a Mac or Linux machine. If you have Windows, you can skip download and instead use the option below to read the file in R directly from the Internet.</p> <p><pre><code># Download the raw_counts.txt file into your local machine using wget\ncd ~/my_wokingdir  # replace \"~/my_workingdir\" with the output of \"getwd()\" in your R session\nwget https://course-cg-5534.s3.amazonaws.com/awk_exercise/raw_counts.txt\n</code></pre> <pre><code># To read a local file. If you have downloaded the raw_counts.txt file to your local machine, you may use the following command to read it in, by providing the full path for the file location. The way to specify the full path is the same as taught in the command line session. Here we assume raw_counts.txt is in our current working directory\ndata &lt;- read.table(file=\"./raw_counts.txt\", sep=\"\\t\", header=T, stringsAsFactors=F)\n\n# There is a very convenient way to read files from the internet.\ndata &lt;- read.table(file=\"https://course-cg-5534.s3.amazonaws.com/awk_exercise/raw_counts.txt\", sep=\"\\t\", header=T, stringsAsFactors=F)\n</code></pre></p> <p>Take a look at the beginning part of the data frame.</p> <pre><code>head(data)\n</code></pre> <pre><code>##            C61  C62  C63  C64  C91  C92  C93 C94 I561 I562 I563 I564 I591 I592\n## AT1G01010  322  346  256  396  372  506  361 342  638  488  440  479  770  430\n## AT1G01020  149   87  162  144  189  169  147 108  163  141  119  147  182  156\n## AT1G01030   15   32   35   22   24   33   21  35   18    8   54   35   23    8\n## AT1G01040  687  469  568  651  885  978  794 862  799  769  725  715  811  567\n## AT1G01046    1    1    5    4    5    3    0   2    4    3    1    0    2    8\n## AT1G01050 1447 1032 1083 1204 1413 1484 1138 938 1247 1516  984 1044 1374 1355\n##           I593 I594 I861 I862 I863 I864 I891 I892 I893 I894\n## AT1G01010  656  467  143  453  429  206  567  458  520  474\n## AT1G01020  153  177   43  144  114   50  161  195  157  144\n## AT1G01030   16   24   42   17   22   39   26   28   39   30\n## AT1G01040  831  694  345  575  605  404  735  651  725  591\n## AT1G01046    8    1    0    4    0    3    5    7    0    5\n## AT1G01050 1437 1577  412 1338 1051  621 1434 1552 1248 1186\n</code></pre> <p>Depending on the format of the file, several variants of read.table() are available to make reading a file easier.</p> <p>read.csv(): for reading \"comma separated value\" files (.csv).</p> <p>read.csv2(): variant used in countries that use a comma \",\" as decimal point and a semicolon \";\" as field separators.</p> <p>read.delim(): for reading \"tab separated value\" files (\".txt\"). By default, point(\".\") is used as decimal point.</p> <p>read.delim2(): for reading \"tab separated value\" files (\".txt\"). By default, comma (\",\") is used as decimal point.</p> <p>Choosing the correct function (or parameters) is important! If we use <code>read.csv()</code> to read our tab-delimited file, it becomes a mess.</p> <pre><code>data2 &lt;- read.csv(file=\"./raw_counts.txt\", stringsAsFactors=F)\n\nhead(data2)\n</code></pre> <pre><code>##                                         C61.C62.C63.C64.C91.C92.C93.C94.I561.I562.I563.I564.I591.I592.I593.I594.I861.I862.I863.I864.I891.I892.I893.I894\n## 1                     AT1G01010\\t322\\t346\\t256\\t396\\t372\\t506\\t361\\t342\\t638\\t488\\t440\\t479\\t770\\t430\\t656\\t467\\t143\\t453\\t429\\t206\\t567\\t458\\t520\\t474\n## 2                        AT1G01020\\t149\\t87\\t162\\t144\\t189\\t169\\t147\\t108\\t163\\t141\\t119\\t147\\t182\\t156\\t153\\t177\\t43\\t144\\t114\\t50\\t161\\t195\\t157\\t144\n## 3                                               AT1G01030\\t15\\t32\\t35\\t22\\t24\\t33\\t21\\t35\\t18\\t8\\t54\\t35\\t23\\t8\\t16\\t24\\t42\\t17\\t22\\t39\\t26\\t28\\t39\\t30\n## 4                     AT1G01040\\t687\\t469\\t568\\t651\\t885\\t978\\t794\\t862\\t799\\t769\\t725\\t715\\t811\\t567\\t831\\t694\\t345\\t575\\t605\\t404\\t735\\t651\\t725\\t591\n## 5                                                                     AT1G01046\\t1\\t1\\t5\\t4\\t5\\t3\\t0\\t2\\t4\\t3\\t1\\t0\\t2\\t8\\t8\\t1\\t0\\t4\\t0\\t3\\t5\\t7\\t0\\t5\n## 6 AT1G01050\\t1447\\t1032\\t1083\\t1204\\t1413\\t1484\\t1138\\t938\\t1247\\t1516\\t984\\t1044\\t1374\\t1355\\t1437\\t1577\\t412\\t1338\\t1051\\t621\\t1434\\t1552\\t1248\\t1186\n</code></pre> <p>However, the <code>read.csv()</code> function is appropriate for a comma-delimited file.</p> <p>Since the data contained in these files is the same, we don't need to keep data2 copy.</p> <pre><code>rm(data2)\n</code></pre> <p>R base function write.table() can be used to export data to a file.</p> <pre><code># To write to a file called \"output.txt\" in your current working directory.\nwrite.table(data[1:20,], file=\"output.txt\", sep=\"\\t\", quote=F, row.names=T, col.names=T)\n</code></pre> <p>It is also possible to export data to a csv file.</p> <p>write.csv()</p> <p>write.csv2()</p>"},{"location":"lab_session/R_for_bioinformatics/#basic-statistics-in-r","title":"Basic statistics in R","text":"Description   R_function   Mean   mean()   Standard deviation   sd()   Variance   var()   Minimum   min()   Maximum   max()   Median   median()   Range of values:  minimum and maximum   range()   Sample quantiles   quantile()   Generic function   summary()   Interquartile range   IQR()  <p>Calculate the mean expression for each sample.</p> <pre><code>apply(data, 2, mean)\n</code></pre> <pre><code>##      C61      C62      C63      C64      C91      C92      C93      C94     I561     I562     I563     I564     I591     I592     I593     I594     I861     I862     I863     I864     I891     I892     I893     I894 \n## 391.9998 336.4872 333.7007 380.6545 364.6587 407.0191 361.3672 314.1931 398.8421 380.4970 382.0019 378.7685 387.7994 349.4061 400.9421 385.1493 219.8517 379.0522 341.6387 271.0391 395.3089 426.0254 350.8965 358.8508 \n</code></pre> <p>Calculate the range of expression for each sample.</p> <pre><code>apply(data, 2, range)\n</code></pre> <pre><code>##        C61   C62   C63   C64   C91   C92   C93   C94   I561  I562  I563  I564   I591  I592   I593  I594  I861   I862  I863  I864   I891  I892   I893  I894\n## [1,]     0     0     0     0     0     0     0     0      0     0     0     0      0     0      0     0     0      0     0     0      0     0      0     0\n## [2,] 81764 89072 43781 64539 51516 68279 64407 53799 116414 90133 69623 76426 111873 73071 114566 89630 69853 122114 98449 51835 102672 80998 116025 89270\n</code></pre> <p>Calculate the quantiles of each samples.</p> <pre><code>apply(data, 2, quantile)\n</code></pre> <pre><code>##        C61   C62   C63   C64   C91   C92   C93   C94   I561  I562  I563  I564   I591  I592   I593  I594  I861   I862  I863  I864   I891  I892      I893  I894\n## 0%       0     0     0     0     0     0     0     0      0     0     0     0      0     0      0     0     0      0     0     0      0     0      0.00     0\n## 25%      0     0     0     0     0     0     0     0      0     0     0     0      0     0      0     0     0      0     0     0      0     0      0.00     0\n## 50%     43    38    45    47    48    45    47    39     41    45    47    45     48    41     45    43    21     49    33    31     46    49     44.00    41\n## 75%    330   270   294   331   326   344   311   266    327   333   314   316    330   298    338   333   149    327   274   211    333   354    300.75   304\n## 100% 81764 89072 43781 64539 51516 68279 64407 53799 116414 90133 69623 76426 111873 73071 114566 89630 69853 122114 98449 51835 102672 80998 116025.00 89270\n</code></pre>"},{"location":"lab_session/R_for_bioinformatics/#simple-data-visualization","title":"Simple data visualization","text":"<p>Scatter plot and line plot can be produced using the function plot().</p> <pre><code>x &lt;- c(1:50)\ny &lt;- 1 + sqrt(x)/2\nplot(x,y)\n</code></pre> <p></p> <pre><code>plot(x,y, type=\"l\")\n</code></pre> <p></p> <pre><code># plot both the points and lines\n## first plot points\nplot(x,y)\nlines(x,y, type=\"l\")\n</code></pre> <p></p> <pre><code>## lines() can only be used to add information to a graph, while it cannot produce a graph on its own.\n</code></pre> <p>boxplot() can be used to summarize data.</p> <pre><code>boxplot(data, xlab=\"Sample ID\", ylab=\"Raw Counts\")\n</code></pre> <p></p> <p>add more details to the plot.</p> <pre><code>boxplot(data, xlab=\"Sample ID\", ylab=\"Raw Counts\", main=\"Expression levels\", col=\"blue\", border=\"black\")\n</code></pre> <p></p> <pre><code>x &lt;- rnorm(1000)\nboxplot(x)\n</code></pre> <p></p> <p>hist() can be used to create histograms of data.</p> <pre><code>hist(x)\n</code></pre> <p></p> <pre><code># use user defined break points\nhist(x, breaks=seq(range(x)[1]-1, range(x)[2]+1, by=0.5))\n</code></pre> <p></p> <pre><code># clear plotting device/area\ndev.off()\n</code></pre> <pre><code>## null device \n##           1\n</code></pre>"},{"location":"lab_session/R_for_bioinformatics/#r-packages-for-rnaseq-analysis","title":"R - packages for RNAseq Analysis","text":"<p>The following packages need to be installed in your local machine using Rstudio to process RNAseq data in upcoming lab sessions.</p> <p>R-packages list:</p> <ul> <li><code>dplyr</code></li> <li><code>cowplot</code></li> <li><code>reshape2</code></li> <li><code>ggplot2</code></li> <li><code>jsonlite</code></li> </ul> <pre><code># create a list of packages need to be installed\npackages &lt;- c(\"dplyr\", \"cowplot\", \"reshape2\", \"ggplot2\", \"jsonlite\")\n\ninstall.packages(packages, dependencies = TRUE)\n</code></pre> <p>R - Bioconductor Packages:</p> <ul> <li><code>BiocManager</code></li> <li><code>biomaRt</code></li> <li><code>Biostrings</code></li> <li><code>ensembldb</code></li> <li><code>EnsDb.Hsapiens.v86</code></li> <li><code>chimeraviz</code> </li> </ul> <pre><code># install BiocManager \nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\ninstall.packages(\"BiocManager\")\n\nBiocManager::install(\"biomaRt\")\n\nBiocManager::install(\"Biostrings\")\n\nBiocManager::install(\"ensembldb\")\n\nBiocManager::install(\"EnsDb.Hsapiens.v86\")\n\nBiocManager::install(\"chimeraviz\")\n</code></pre>"},{"location":"lab_session/analysis_of_cnv/","title":"Copy number analysis","text":""},{"location":"lab_session/analysis_of_cnv/#introduction","title":"Introduction","text":"<p>There are plenty of methods and code packages available for copy number analysis, and you will likely be able to find one for free that suits your particular application. Although tailored for different types of sequence data (or microarrays), general processing steps of all copy number analysis tools include:</p> <ol> <li>Quantification of sequence read depth (or array intensity)     throughout the reference genome as a measure of DNA abundance in the     sample</li> <li>Removal of systematic noise using features such as GC content and     mapability</li> <li>Removal of systematic noise using normal (typicaly non-cancer)     reference samples</li> <li>Segmentation - partitioning of the reference genome so that each     segment can be assigned one copy number</li> <li>Copy number calling, assigning each segment some estimate of the     number of copies per cell</li> <li>Combine the measure of DNA abundance with SNP allele ratios to     support copy number estimates</li> </ol> <p>In this exercise you will perform the above steps using R, with a BAM file of aligned reads and a VCF file of SNPs as input.</p>"},{"location":"lab_session/analysis_of_cnv/#download-data-and-prepare-r","title":"Download data and prepare R","text":"<p>The data used in this exercise are available for download here. Extract the content on your computer. Before starting, make sure you have a recent version of R, RStudio and the following packages installed:</p>"},{"location":"lab_session/analysis_of_cnv/#from-cran","title":"From CRAN","text":"<ul> <li>data.table</li> <li>ggplot2</li> <li>patchwork</li> <li>PSCBS</li> <li>stringr</li> </ul> <p>Just open RStudio and type in the R console, for each one:</p> <pre><code>install.packages(\"package_name\")\n</code></pre>"},{"location":"lab_session/analysis_of_cnv/#from-bioconductor","title":"From Bioconductor","text":"<ul> <li>bamsignals</li> <li>biomaRt</li> <li>bioStrings</li> <li>BSgenome</li> <li>BSgenome.Hsapiens.UCSC.hg19</li> <li>DNAcopy</li> <li>GenomicRanges</li> <li>Rsamtools</li> <li>VariantAnnotation</li> </ul> <p>Type once:</p> <pre><code>if (!requireNamespace(\"BiocManager\", quietly = TRUE))\ninstall.packages(\"BiocManager\")\n</code></pre> <p>Then, for each package:</p> <pre><code>BiocManager::install(\"package_name\")\n</code></pre> <p>Several packages from the Bioconductor repository are used in this exercise, for example GenomicRanges which facilitates working with anything located on a genome.</p>"},{"location":"lab_session/analysis_of_cnv/#parse-sequence-target-definition","title":"Parse sequence target definition","text":"<p>We use targeted sequencing of a prostate cancer sample in this exercise, as the resulting data files are of a more convenient size than whole genome or exome. This hybrid-capture panel covers a few hundred cancer genes and a few thousands of additional targets intended to improve copy number and structural variant detection.</p> <p>In RStudio, navigate to the folder containing the exercise files:</p> <pre><code>setwd('your/path/to/exercise_folder')\n</code></pre> <p>Create a new Quarto Document in the exercise folder. Keep your code as shown in the template, and write your comments and answers to questions between code chunks. Make sure you run the code, investigate the results and answer the questions. You can click Render to run all code and generate a html report. This way, when you are through this exercise, your report is done too.</p> <p>The BED (Browser Extensible Data) format is a text file format commonly used to store genomic regions as coordinates and associated annotations. The data are presented in the form of columns separated by spaces or tabs. This format was developed during the Human Genome Project and then adopted by other sequencing projects.</p> <p>We begin by parsing and investigating the target definition BED file.</p> <pre><code>library(data.table)\n\n# Input file\nbed_file &lt;- 'targets.bed'\n\n# Check that it exists\nfile.exists(bed_file)\n\n# Read it \ntargets &lt;- fread(bed_file)\n\n# Check content\ntargets\n\n# Let the first column be the target number\ntargets &lt;- cbind(1:nrow(targets),targets)\n\n# Set useful column names\ncolnames(targets) &lt;- c('target','chromosome','start','end')\n\n# Calculate target length\ntargets[,length:=end-start]\nsummary(targets)\n\n# Check target distribution over chromosomes\ntargets[,table(chromosome)]\n</code></pre> <p>The R code in this exercise features <code>data.table</code> syntax, for example using <code>:=</code> to modify the content of a table. If you are more familiar with base R or tidyverse syntax, a comparison between them can be found here.</p> <p>Feel free to complete the exercise using the coding style you prefer.</p>"},{"location":"lab_session/analysis_of_cnv/#annotate-with-gene-symbol","title":"Annotate with gene symbol","text":"<p>Before parsing the sequence data, let's assign gene symbols to targets that overlap a gene. We'll take advantage of databases accessible through Bioconductor packages. First, we need to create a GenomicRanges object representing our targets:</p> <pre><code>library(GenomicRanges)\n\n# Make a GRanges object\ntarget_ranges &lt;- makeGRangesFromDataFrame(targets)\ntarget_ranges\n\n# Check the sequence naming style\nseqlevelsStyle(target_ranges)\n</code></pre> <p>As you may recognize, our targets have the NCBI/EMBL sequence naming style (1 and not chr1, etc). This matches the reference genome used with our sequence data.</p>"},{"location":"lab_session/analysis_of_cnv/#download-gene-coordinates","title":"Download gene coordinates","text":"<p>Let's use the <code>biomaRt</code> package to download Ensembl gene coordinates for reference genome b37/Hg19.</p> <p>To learn more about a function, just type ?function_name to open its documentation in RStudio's bottom right window.</p> <pre><code>library(biomaRt)\n\n?useEnsembl\n\n# Open a connection\nbiomart_connection &lt;- useEnsembl(biomart=\"ensembl\", dataset=\"hsapiens_gene_ensembl\", GRCh=37)\n\n?getBM\n\n# Get gene coordinates\ngenes &lt;- getBM(\nattributes=c('ensembl_gene_id','hgnc_symbol','chromosome_name','start_position','end_position'),\nmart = biomart_connection)\n\n# Convert to data.table\ngenes &lt;- as.data.table(genes)\n\ngenes\n</code></pre> <p>We can now make a GenomicRanges object for the genes:</p> <pre><code>colnames(genes)[3:5] &lt;- c('chromosome','start','end')\n\ngene_ranges &lt;- makeGRangesFromDataFrame(genes)\n\nseqlevelsStyle(gene_ranges)\n</code></pre>"},{"location":"lab_session/analysis_of_cnv/#overlap-targets-and-genes","title":"Overlap targets and genes","text":"<p>The chromosome naming style of the genes is also NCBI/EMBL. We can now go ahead and match them up to targets using the findOverlaps function:</p> <pre><code># Overlap the two GRanges sets\noverlap &lt;- findOverlaps(target_ranges,gene_ranges)\n\noverlap\n</code></pre> <p>Overlaps are presented as pairs of query (first argument) and subject (second argument) rows for which there is any overlap. We can simply assign targets their corresponding gene name for each overlap, and check the result:</p> <pre><code>targets$gene &lt;- '' # default if no overlap\n\n# Put a gene symbol on each target\ntargets[queryHits(overlap),gene:=genes[subjectHits(overlap)]$hgnc_symbol]\n\n# Count targets per gene\nas.data.table(table(targets$gene))\n\nas.data.table(sort(table(targets$gene),decreasing = T))[N&gt;25]\n</code></pre> <p>There are many ways to achieve the same thing. You should focus mostly on what we do, and not worry too much about exactly how. Google, ChatGTP, etc, are there for you as long as you know what you want to achieve.</p>"},{"location":"lab_session/analysis_of_cnv/#parse-sequence-read-count","title":"Parse sequence read count","text":"<p>We are now ready to access the BAM file and count the number of reads (actually read pairs, corresponding to DNA fragments) that map to each target. For this we use the <code>bamsignals</code> package and the <code>GenomicRanges</code> object representing the targets. To count a read pair, we require its midpoint to fall within a target, and that it is not flagged as a duplicate.</p> <pre><code>library(bamsignals)\nlibrary(Rsamtools)\n\n# Imput bam file\nbamfile &lt;- 'sample1.bam'\nfile.exists(bamfile)\nfile.exists(paste0(bamfile,'.bai'))\n\n# Count reads in targets\ntargets$count &lt;- bamCount(bamfile, target_ranges, paired.end=\"midpoint\", filteredFlag=1024)\ntargets\n\nsummary(targets)\n</code></pre>"},{"location":"lab_session/analysis_of_cnv/#plot-sequence-read-count","title":"Plot sequence read count","text":"<p>Let's investigate the raw sequence read count across targets. You are probably already familiar with ggplot2.</p> <pre><code>library(ggplot2)\ntheme_set(theme_bw())\n\n# Simple ggplot\nggplot(data = targets) + geom_point(mapping = aes(x = target, y = count))\n\n# Some adjustments\nggplot(data = targets) + ylim(c(0,3500)) +\ngeom_point(mapping = aes(x = target, y = count),size=.5)\n</code></pre> <p>Targets are just shown in order. To plot genomic positions without mixing up chromosomes, we can separate the plot by chromosome:</p> <pre><code># Plot start pos by chromosome\nggplot(data = targets) + ylim(c(0,3500)) +\ngeom_point(mapping = aes(x = start, y = count),size=.5) + facet_wrap(facets = vars(chromosome),ncol = 3)\n</code></pre> <p>The figure may now not look a bit squished in RStudio's bottom-right plots tab. Click the Zoom button and enlarge the new window.</p> <p>Let's choose a few genes to highlight in the plot.</p> <pre><code># Genes of interest\nmy_genes &lt;- c('AR','ATM','BRCA1','BRCA2','PTEN','TMPRSS2','ERG')\ntargets[gene %in% my_genes,label:=gene]\n\nggplot() + ylim(c(0,3500)) +\ngeom_point(data = targets, mapping = aes(x = target, y = count, col = label), size=.5)\n\nggplot(data = targets) + ylim(c(0,3500)) +\ngeom_point(mapping = aes(x = start, y = count, col = label), size=.5) +\nfacet_wrap(facets = vars(chromosome),ncol = 3)\n</code></pre>"},{"location":"lab_session/analysis_of_cnv/#model-and-correct-for-gc-content-bias","title":"Model and correct for GC content bias","text":"<p>Sequence GC content is known to affect PCR amplification and sequence coverage. Let's retrieve the GC content for the targets and investigate if there is a bias in our data set. <code>BSgenome</code> is a Bioconductor package that provides a framework for working with genomic sequences in R. The package provides a set of objects, functions, and methods for working with different reference genomes and their annotations, such as gene locations, exons, and transcription start sites.</p> <p>First we check whether a matching reference genome is available:</p> <pre><code>library(BSgenome)\n\n# Check which genomes are available\navailable.genomes()\n</code></pre> <p>The best match is <code>BSgenome.Hsapiens.UCSC.hg19</code>. But that means our target <code>GenomicRanges</code> object needs to have UCSC-style sequence naming:</p> <pre><code>ucsc_ranges &lt;- target_ranges\n\n# Swap sequence naming style\nseqlevelsStyle(ucsc_ranges) &lt;- \"UCSC\"\nucsc_ranges\n\nseqlevelsStyle(ucsc_ranges)\n</code></pre> <p>We can now assign GC content to targets:</p> <pre><code>library(BSgenome.Hsapiens.UCSC.hg19)\nlibrary(stringr)\n\n# Get the target sequences\ndna &lt;- getSeq(Hsapiens, ucsc_ranges)\n\n# Calculate GC content for the target sequences\ntargets$gc &lt;- str_count(as.character(dna),pattern = '[GC]') / width(dna)\n\n# Plot the GC content and read count\nggplot(data = targets) + ylim(c(0,3500)) +\ngeom_point(mapping = aes(x = gc, y = count),alpha=.1)\n</code></pre> <p>There appears to be a modest effect of target GC content on read count. Let's build a loess model of the effect.</p> <pre><code># Make a loess model\nloess_model=loess(count ~ gc, data = targets)\n</code></pre> <p>To plot the model, we predict the read count from GC:</p> <pre><code># Add predicted count (model y value) to targets\ntargets[,gc_predicted_count:=predict(loess_model,gc)]\n\n# Plot with the model prediction\nggplot(data=targets) + ylim(c(0,3500)) +\ngeom_point(mapping = aes(x = gc, y = count),alpha=.1,size=1) +\ngeom_line(mapping = aes(x = gc,y = gc_predicted_count),col='blue')\n</code></pre> <p>To remove the GC content bias, we divide the observed count with the predicted. This new count ratio should be a slightly better measure of DNA abundance in our sample.</p> <pre><code># Correct for the effect of GC content\ntargets[,count_ratio:=count/gc_predicted_count]\ntargets\n</code></pre> <p>To compare the new metric with the previous, we can plot them side-by-side. With patchwork we can use arithmetic operators to combine and align multiple plots.</p> <pre><code>library(patchwork)\n\n# Plot and align\np1 &lt;- ggplot(data = targets) + geom_point(mapping = aes(x = gc, y = count),alpha=.1)\n\np2 &lt;- ggplot(data = targets) + geom_point(mapping = aes(x = gc, y = count_ratio),alpha=.1)\n\np1+p2\n</code></pre> <p>The GC content correction seems to have had a small but positive effect. It is common to also use either a matched normal or a pool of normal reference samples (sequenced similarly) to remove a little bit more systematic noise. We have omitted that step here.</p> <p>Question 1: An important reason to perform copy number analysis of cancer samples is to find homozygous deletions of tumor suppressor genes. Let's assume a patient has a small hemizygous germline deletion (loss of one copy) affecting one tumor suppressor gene only. In the tumor cell population there is also a somatic deletion of the other, homologous, chromosome, including the remaining copy of the gene. Would the resulting homozygous deletion still be visible if the read count ratio of the tumor sample were to be divided by that of the normal sample?</p> <p>Write answers to all questions as text in your Quarto document, so that they are stored in the report with your data analysis. When done, render the report (including all analysis steps you have included) by clicking the Render button. Email the result to the course teacher.</p>"},{"location":"lab_session/analysis_of_cnv/#segment-the-data","title":"Segment the data","text":"<p>Although we can already spot some apparent gains and losses, statistical tools can help us better define copy number segments, for which we can then calculate the most likely copy number given the observation. Circular binary segmentation (CBS) is probably the most commonly used segmentation method. Here we use the <code>PSCBS</code> (\"Parent specific\" CBS, as it can also use SNP allele data) R package as a wrapper to perform basic CBS on the GC content-adjusted count ratio.</p> <p>CBS requires a DNA abundance measure to be log-transformed, making its distribution more normal-like. If you have worked with copy number analysis before, you may recognize the term log ratio.</p> <pre><code>library(PSCBS)\n\n# Log transform the read count\ntargets[,log_ratio:=log2(count_ratio)]\nsummary(targets)\n\n# Segmentation requires values to be finite\ntargets[is.infinite(log_ratio),log_ratio:=NA]\nsummary(targets)\n\n# Segment the log ratio\nsegments &lt;- segmentByCBS(y=targets$log_ratio)\nsegments\n</code></pre> <p>After segmentation we can transform the segment mean values back and plot the segments with the targets.</p> <pre><code># Linear space segment means\nsegments &lt;- as.data.table(segments)[,count_ratio:=2^mean]\nsegments\n\nggplot() + ylim(c(0,2.5)) +\ngeom_point(data = targets, mapping = aes(x = target, y = count_ratio,fill=chromosome),shape=21) + geom_segment(data=segments,col='green',size=2,\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio))\n</code></pre> <p>Note that the segmentation is based only on the target log ratio and order, and that segment start and end refer to target number. To avoid segments spanning across chromosomes, and breakpoints just missing the chromosome boundary, we can add chromosomes to the segmentation call. The chromosome vector is required to be numeric.</p> <pre><code># Segment with chromosomes specified\nsegments &lt;- segmentByCBS(y=targets$log_ratio,\nchromosome=as.numeric(str_replace(targets$chromosome,'X','23')))\nsegments\n\n# Tidy up\nsegments &lt;- as.data.table(segments)[,count_ratio:=2^mean][!is.na(chromosome),-1]\nsegments\n\nggplot() + ylim(c(0,2.5)) +\ngeom_point(data = targets, mapping = aes(x = target, y = count_ratio,fill=chromosome),shape=21) + geom_segment(data=segments,col='green',size=2,\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio))\n</code></pre> <p>CBS can also take a vector of chromosomal positions as input, in which case the resulting segment start and end positions are based on them. We use the target midpoints:</p> <pre><code># Segment with position and chromosome positions\nsegments_pos &lt;- segmentByCBS(y=targets$log_ratio,\nchromosome=as.numeric(str_replace(targets$chromosome,'X','23')),\nx=targets$start+60)\n\nsegments_pos &lt;- as.data.table(segments_pos)[,count_ratio:=2^mean][!is.na(chromosome),-1]\nsegments_pos\n\n\n# convert chromosomes back to NCBI\nsegments_pos[,chromosome:=str_replace(as.character(chromosome),'23','X')]\n\nggplot(data = targets) + ylim(c(0,2.5)) +\ngeom_point(mapping = aes(x = start, y = count_ratio, fill=label),shape=21) +\ngeom_segment(data=segments_pos,col='green',\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio)) +\nfacet_wrap(facets = vars(chromosome),ncol = 2) +\ntheme(panel.spacing = unit(0, \"lines\"),strip.text.x = element_text(size = 6))\n</code></pre> <p>Let's now take another look at segmented targets plotted in order, as they are very unevenly distributed over the genome. We can spot some apparent copy number losses and gains of genes in our selection. Take a closer look at BRCA2 and part of PTEN.</p> <pre><code>p1 &lt;- ggplot() + ylim(c(0,2.5)) + geom_point(data = targets, mapping = aes(x = target, y = count_ratio,fill=chromosome),shape=21) + geom_segment(data=segments,col='green',size=2,\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio))\n\np2 &lt;- ggplot() + ylim(c(0,2.5)) + geom_point(data = targets, mapping = aes(x = target, y = count_ratio,fill=label),shape=21) + geom_segment(data=segments,col='green',size=2,\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio))\n\np1/p2\n</code></pre> <p>Question 2: Target density is only about one per megabase, with much higher density in some cancer genes. How many targets do you think a deletion would have to cover for us to be able to find it? What else might influence sensitivity?</p> <p>For BRCA2 and (part of) PTEN, count ratio (remember, it is the observed read count (DNA abundance) relative to the expected, given target GC content), is near 0.5, or 50%. For a largely diploid genome this is exactly what to expect if these genomic segments were hemizygously deleted, i.e., one out of the two homologous copies would have been lost. This would also fit reasonably well with gain of 1 copy of 8q (resulting in a count ratio of 1.5) and 3 extra copies of AR (being on the X chromosome, tumor cells would have quadrupled its single original AR copy, resulting in the count ratio going from about 0.5 to near 2.).</p> <p>But this solution would require the tumor cell content (\"purity\") in the sample to be near 100%, which is rare unless the sample comes from a cell line. Note also that several other segments have a mean count ratio that would not fit this solution well. There are actually multiple other combinations of copy number and purity that would fit the observation reasonably well.</p> <p>Our measure of relative DNA abundance - count ratio - is automatically centered around 1, regardless of the average copy number (\"ploidy\") of the sequenced genome.</p> <p>Let's assume the cancer cells sampled had an average of 4 instead of 2 copies. This would neither make us sequence to twice the amount, nor observe twice the sequence count relative to our GC content model for most of the genome.</p> <p>The average DNA abundance measured throughout the genome, whether it is raw sequence count, count ratio, or a logarithm of count ratio, does not increase in samples with a higher average copy number (more DNA per cell). Instead the average measured DNA abundance will be observed for the average copy number, whatever that is.</p> <p>We are also likely to have some normal DNA content in the sample. If both the tumor genome and normal genome are near diploid on average, and the tumor cell content is near 50%, homozygous deletion (loss of both homologous copies) would be observed at a coverge ratio near 0.5, as for every tumor cell that would contribute zero copies to the DNA extraction, one normal cell would contribute its normal two. As loss of one copy in the tumor cell fraction would then appear at about 0.75 and gains would appear near 1.25, 1.5, ..., this solution also appears to fit our data reasonably well.</p> <p>Question 3: If a cancer genome has an average of 4 copies, and the tumor cell content is near 100%, at what count ratio should we ovserve a copy number of 3?</p> <p>Fortunately there is another tool we can use to inform the copy number estimates. We can use SNPs from a small variant caller and investigate their variant allele ratios.</p>"},{"location":"lab_session/analysis_of_cnv/#parse-snp-allele-ratio","title":"Parse SNP allele ratio","text":"<p>Let's use the <code>VariantAnnotation</code> package to parse a VCF file containing SNPs.</p> <pre><code>library(VariantAnnotation)\n\n# Parse the vcf file\nvcf &lt;- readVcf('sample1.vcf')\nvcf\n\n# Genotype column\ng &lt;- geno(vcf)\ng\n\n# Allelic and total read depth \nas.data.table(g$AD)\nas.data.table(g$DP)\n</code></pre> <p>The AD column of the VCF file contains the number of reads supporting the reference and alternative allele. The DP column contains the total read depth. Let's make a table of SNPs containing the alt-allele ratio.</p> <pre><code># A table of SNPs\nsnp_table &lt;- data.table(id=names(vcf),\nAD=sapply(g$AD, \"[[\", 2),\nDP=unlist(g$DP[,1]))\nsnp_table\n\n# Compute allele ratio\nsnp_table[,allele_ratio:=round(AD/DP,3)]\nsnp_table\n</code></pre> <p>A GenomicRanges object defining the SNPs' positions on the reference genome can be accessed with the <code>rowRanges</code> function. We can now overlap that with the GenomicRanges representing our targets and use the resulting <code>Hits</code> object to assign allele ratio to targets.</p> <pre><code># Match targets to SNPs\noverlaps &lt;- findOverlaps(target_ranges,rowRanges(vcf))\noverlaps\n\n# Set SNP allele ratio for targets\ntargets[queryHits(overlaps),allele_ratio:=snp_table$allele_ratio[subjectHits(overlaps)]]\ntargets\n</code></pre> <p>Let's plot the SNP allele ratio with the count ratio and see if the copy number status becomes more clear. Note that many targets contain no SNP, and that many SNPs are homozygous. The allele ratio of a homozygous SNP is either 0 or 1, depending on whether the alleles match the reference genome or not. Only heterozygous SNP allele ratio is affected by copy number alteration.</p> <pre><code>p1 &lt;- ggplot() + ylim(c(0,2.5)) +\ngeom_point(data = targets, mapping = aes(x = target, y = count_ratio,fill=label),shape=21) + geom_segment(data=segments,col='green',size=2,\nmapping = aes(x=start,xend=end,y=count_ratio,yend=count_ratio))\n\np2 &lt;- ggplot() +\ngeom_point(data = targets, mapping = aes(x = target, y = allele_ratio,fill=label),shape=21)\n\np1 / p2 + plot_layout(guides = 'collect')\n</code></pre> <p>We can see that for most segments, particularly segments with a count ratio near 1, heterozygous SNPs have allele ratios near 0.5. This indicates that the average copy number may be 2.</p> <p>Question 4: What do you think is the tumor cell fraction, and what has most likely happened to PTEN and BRCA2?</p>"},{"location":"lab_session/analysis_of_cnv/#estimating-genome-wide-copy-number","title":"Estimating genome-wide copy number","text":"<p>Although the ploidy and purity of this example are relatively clear, that is not always the case. There are several methods available that fits your data to potential solutions of ploidy and purity, and computes the copy numbers corresponding to the best fit. Beware, this is somewhat prone to error, especially when the purity is below 50%, if there are few copy number alterations, and if there is some tumor cell heterogeneity. Also, reporting the total copy number of every segment does not make sense in most settings. You should always review the result and curate if necessary.</p>"},{"location":"lab_session/analysis_of_cnv/#final-questions","title":"Final questions","text":"<p>Some questions require a bit of exploration and perhaps more code and visualisation. Motivate your answers to the extent you think is reasonable, and feel free to collaborate with the other students.</p> <p>Question 5: Homozygous deletions are rarely larger than one, occationally a few, megabases. What appears to be the sizes of the deletions affecting PTEN and BRCA2?</p> <p>Question 6: Let's assume you suspect that this patient may have a TMPRSS2-ERG fusion. Is that supported at all by copy number data?</p> <p>Question 7: Is there a MYC amplification?</p> <p>Question 8: How many copies of AR would you say there are in each tumor cell?</p> <p>Question 9: Let's assume there is a genomic segment with somatic copy-neutral loss of heterozogosity, i.e.\u00a0one of the two homologous copies was lost and the other duplicated. What count ratio would we observe in this sample, and at what allele ratio(s) would the (germline-heterozygous) SNPs likely appear?</p> <p>Once you have completed the exercise, make sure the resulting report (generated by clicking Render) is a self-contained HTML file by specifying HTML format options at the top of your Quarto document:</p> <pre><code>author: \"Your Name\"\nformat:\n  html:\n    self-contained: true\n</code></pre> <p>You can add more options to make the result look better. Also check that the report is readable, for example that it does not include full print-outs of large data structures. When done, email the rendered report (or a link if it is too large) to the course teacher.</p>"},{"location":"lab_session/annotation_files/","title":"Annotation files","text":""},{"location":"lab_session/annotation_files/#files-for-annotation-of-variation-in-the-human-genome","title":"Files for annotation of variation in the human genome","text":"<pre><code>#These are available from the Broad institute in Boston, who also provides the GATK software suite.\ncd ~/workspace/inputs/references/gatk\n\n#Download the following files:\n# SNP calibration call sets - dbsnp, hapmap, omni, and 1000G\n# Runtime: &lt; 2min\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf .\n# Runtime: ~ 2min\nbgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz .\n\n# Have a loot what the files contain. \n# Dont worry, we will look at VCF files later.\ngunzip -c 1000G_phase1.snps.high_confidence.hg38.vcf.gz | less -SN\n\n# Indel calibration call sets - dbsnp, Mills\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz .\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz .\n\n# Interval lists that can be used to parallelize certain GATK tasks\ngsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list .\ngsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ .\n\n# list the files we just downloaded\nls -lh\n</code></pre>"},{"location":"lab_session/annotation_files/#index-the-variation-files","title":"Index the variation files","text":"<pre><code>cd ~/workspace/inputs/references/gatk/\n\n#SNP calibration call sets - dbsnp, hapmap, omni, and 1000G\n# Runtime: ~ 4min\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz\n# Runtime: ~ 3min\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz\n\n#Indel calibration call sets - dbsnp, Mills\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz\ngatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz\n</code></pre>"},{"location":"lab_session/annotation_files/#interval-files-and-coordinates-for-the-exome-sequencing-assay","title":"Interval files and coordinates for the exome sequencing assay","text":"<p>These files contain the location of the exons in the human genome that were \"targeted\", that is for which targeting oligos were created (primary_targets). </p> <p>The files also contain the location of the actual oligos (capture_targets).</p> <pre><code># change directories\ncd ~/workspace/inputs/references/exome\n\n# download the files\n#wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\n#unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\n\n# remove the zip\n#rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip\n\n# Lift-over of the Roche coordinates from hg19 to the hg38 assembly.\n# the software is availble from USCS, downloadble here:\n#wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver\n#chmod +x liftOver\n# the chain file\nwget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz\n\n# run liftover\nliftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed  hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed\n\nliftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed  hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed\n\n# create a version in standard bed format (chr, start, stop)\ncut -f 1-3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed\n\ncut -f 1-3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed\n\n# take a quick look at the format of these files\nhead SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed\nhead SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed\n</code></pre>"},{"location":"lab_session/annotation_files/#calculate-the-size-of-the-seqcap-v3-exome","title":"Calculate the size of the SeqCap v3 exome","text":"<pre><code>#This can be done in many ways - give it a try yourself before trying the code below and compare results\n\n# first sort the bed files and store the sorted versions\nbedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed\nbedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed\n\n# now merge the bed files to collapse any overlapping regions so they are not double counted.\nbedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed &gt; SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed\n\nbedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed &gt; SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed\n\n# finally use a Perl one liner to determine the size of the files in Mb\nFILES=(SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed)\necho ${FILES[0]}\n\nfor FILE in ${FILES[@]}\ndo\necho \"--------------------------------------------------------\"\necho $FILE\n#With merge\ncat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(&lt;&gt;){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";'\ndone\n# note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable?\n\n# now create a subset of these bed files\ngrep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed &gt; exome_regions.bed\n#When creating files, make a habit to investigate the output to avoid downstream confusion\nhead -n 10 exome_regions.bed\n\ngrep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed &gt; probe_regions.bed\nhead -n 10 probe_regions.bed\n\n# clean up intermediate files\n#rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed\n</code></pre>"},{"location":"lab_session/annotation_files/#create-an-inverval-list-for-the-exome-bed-files","title":"Create an inverval list for the exome bed files","text":"<pre><code># first for the complete exome and probe bed file\ncd ~/workspace/inputs/references/\n#Commands below already run\n#mkdir temp\ncd temp\n#Commands below already run\n#wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict\ncd ~/workspace/inputs/references/exome\njava -jar $PICARD BedToIntervalList I=SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O=SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD=~/workspace/inputs/references/temp/ref_genome.dict\njava -jar $PICARD BedToIntervalList I=SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O=SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD=~/workspace/inputs/references/temp/ref_genome.dict\n#rm -fr ~/workspace/inputs/references/temp/\n#Explore the interval-lists and what the files contain, scroll down until the coordinates are shown\nless -SN SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list\n\n# next for our subset exome and probe regions file\ncd ~/workspace/inputs/references/exome\njava -jar /usr/local/bin/picard.jar BedToIntervalList I=exome_regions.bed O=exome_regions.bed.interval_list SD=~/workspace/inputs/references/genome/ref_genome.dict\njava -jar /usr/local/bin/picard.jar BedToIntervalList I=probe_regions.bed O=probe_regions.bed.interval_list SD=~/workspace/inputs/references/genome/ref_genome.dict\n</code></pre>"},{"location":"lab_session/aws_env/","title":"The workspace on AWS and installations","text":"<p>For the practical sessions each student will work partly using his/her own laptop and partly using a virtual machine on Amazon Web Services (AWS). This machine is an \"instance\" of a linux computer with the correct configurations. </p> <p>To be able to connect to AWS on windows you need to be able to download the SSH client PuTTY. If you have a mac or a linux/unix computer nothing is needed to preinstall to work on the command line. </p> <p>To perform the exercises of the course you also need to install the following software on your local machine: IGV. Choose IGV that comes bundled with Java for your operating system.</p> <p>This workshop requires a large number of different bioinformatics tools. These have been pre-installed on AWS. For installation instructions of the differnet tools, see the list at the end of this document which links to the website of each software tool. </p>"},{"location":"lab_session/aws_env/#explore-the-workspace-on-aws","title":"Explore the workspace on AWS","text":"<pre><code>#Make sure the credentials are set for the \"pem\" (key) file.\n# If needed run:\n# chmod 400 course-setup-student-key.pem\n\n# Log onto AWS cloud (make sure in are in the same directory as the pem key).\n# You might get a question \"Are you sure you want to continue connecting (yes/no/[fingerprint])?\". Answer \"yes\".\nssh -i ./PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE\n\n# Check where you are\npwd\n\n# Note \"~\"\" is a shortcut for the user directory. \n# Pre-load shortcut and settings \nsource .bashrc\n\n# the workspace folder contains the course files and pre-installed bioinformatic tools\ncd workspace/\n\n# The workspace folder contains the course files and pre-installed bioinformatic tools. Navigate to the folders, list the files and reflecton what is there\ncd  ~/workspace/inputs\nls -alF\n# Look around in the folders\n\ncd ~/workspace/bin\nls -alF\n# Look around in the folders\n\n# Have a look at the raw exone sequencing data for the germline DNA\ncd ~/workspace/inputs/data/fastq/Exome_Norm\n\n# What are the files in this folder?\n\n# Explore the file format (gunzip unpacks zipped files)\ngunzip -c Exome_Norm_R1.fastq.gz | less -SN\n\n# The format is nicely described here:\n# https://en.wikipedia.org/wiki/FASTQ_format\n# Read through the fastq format to get an understanding of\n# base qualities from illumina DNA sequencing data.\n# The format will be discussed tomorrow during the lab intro.\n</code></pre>"},{"location":"lab_session/aws_env/#r","title":"R","text":"<p>R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. </p> <p>If you have not installed R yet:</p> <ol> <li>Download R to your local machine<ul> <li>R-project</li> </ul> </li> <li>Double-click on the icon and start the installation process. </li> </ol> <p>Install the following R libraries on your local machine: <pre><code>#Open R and run the following code:\npackages &lt;- c(\"devtools\", \"BiocManager\", \"tidyverse\", \"data.table\", \"patchwork\", \"jsonlite\", \"reshape2\",\"cowplot\")\ninstall.packages(packages, dependencies = TRUE)\n# hat-tip: The tidyverse is an opinionated collection of R packages designed for data science. \n# All packages share an underlying design philosophy, grammar, and data structures. \n# Installing Tidyverse will install several packages like tidyr, dplyr, stringr, ggplot2, lubridate, tibble, readr, purr and forcats.\n\nif (!require(\"BiocManager\", quietly = TRUE))\ninstall.packages(\"BiocManager\")\n\nBiocManager::install(pkgs=c(\"biomaRt\", \"Biostrings\", \"ensembldb\", \"IRanges\", \"EnsDb.Hsapiens.v86\", \"bamsignals\", \"BSgenome\", \"BSgenome.Hsapiens.NCBI.GRCh38\", \"csaw\", \"DNAcopy\", \"GenomicRanges\", \"org.Hs.eg.db\", \"Rsamtools\", \"Repitools\", \"TxDb.Hsapiens.UCSC.hg19.knownGene\", \"VariantAnnotation\", \"chimeraviz\", \"aroma.light\"), dependencies = TRUE)\n\ninstall.packages(\"PSCBS\", dependencies = TRUE)\n</code></pre> Note:    During the course you will be visualising data using R. If some of the libraries do not work on your local machine, you can run R and plot on AWS instead. However, then the plots will have to be dowloaded as pdf-files instead of being viewed interactively in R-studio.</p> <p>The above mentioned libraries are installed in R on AWS, available in the following path, <code>/home/ubuntu/miniconda3/envs/r-rnaseq/bin/R</code>, to access this R env follow the below instructions.</p> <ul> <li>Using the command line do: <pre><code># Start R terminal without GUI\n/home/ubuntu/miniconda3/envs/r-rnaseq/bin/R\n</code></pre></li> <li>Using R do: <pre><code># Load libraries. Note, not all are needed every time,\n# just check here that they load without error. \n# Load each library and wait for it to finish before continuing \nlibrary(BiocManager)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(IRanges)\nlibrary(data.table)\nlibrary(patchwork)\nlibrary(PSCBS)\nlibrary(stringr)\nlibrary(jsonlite)\nlibrary(reshape2)\nlibrary(cowplot)\nlibrary(biomaRt)\nlibrary(Biostrings)\nlibrary(ensembldb)\nlibrary(EnsDb.Hsapiens.v86)\nlibrary(bamsignals)\nlibrary(BSgenome)\nlibrary(BSgenome.Hsapiens.NCBI.GRCh38)\n#library(csaw)\nlibrary(DNAcopy)\nlibrary(GenomicRanges)\n#library(org.Hs.eg.db)\nlibrary(Rsamtools)\n#library(Repitools)\n#library(TxDb.Hsapiens.UCSC.hg19.knownGene)\nlibrary(VariantAnnotation)\nlibrary(chimeraviz)\n\n# Check your current directory\ngetwd()\n\n# Set to your home directory or some other directory\nsetwd(~)\n# here we can run the R scripts \n\n# Let us test to plot and save as pdf using pre-loaded data in R\n\n# Check the data\nmtcars\n\n# Assign a plot to p\np &lt;- ggplot(mtcars, aes(mpg, wt)) +\ngeom_point()\n\n# Save the p plot to the mtcars.pdf\nggsave(filename = \"mtcars.pdf\", plot = p)\n\n# How to download the pdf-file to your local machine,\n# see the section below, however first quit R. \"no\"\n# means that the R-workspace will not be saved.\nq(\"no\")\n</code></pre></li> </ul>"},{"location":"lab_session/aws_env/#how-to-use-the-terminal-download-files-to-your-local-machine","title":"How to use the terminal download files to your local machine","text":"<p>This can be used for both downloading files plotted in R on AWS and other files and file types, such as bam-files that will be visualised in IGV.</p> <pre><code>cd TO_PREFERRED_DIRECTORY\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/PATH_TO_FILE/FILENAME .\n\n# Before proceeding - test that you can get the \"mtcars.pdf\"\n# file available here: /home/ubuntu/workspace/\n# downloaded to your local computer. If not, ask for assistance.\n</code></pre>"},{"location":"lab_session/aws_env/#list-of-preinstalled-bioinformatic-tools","title":"List of preinstalled bioinformatic tools","text":""},{"location":"lab_session/aws_env/#bam-readcount","title":"bam-readcount","text":"<p>bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). </p>"},{"location":"lab_session/aws_env/#bcftools","title":"BCFtools","text":"<p>BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). </p>"},{"location":"lab_session/aws_env/#bwa","title":"BWA","text":"<p>BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license.</p>"},{"location":"lab_session/aws_env/#cnvkit","title":"CNVkit","text":"<p>CNVkit is a python based copy number caller designed for use with hybrid capture. It will not be applied during the course but is a frequently used tool for CNV-analysis of copy-number data.</p>"},{"location":"lab_session/aws_env/#delly","title":"Delly","text":"<p>Delly is a structural variant caller developed at EMBL. It uses paired-ends, split-reads and read-depth to sensitively and accurately delineate genomic rearrangements throughout the genome. </p>"},{"location":"lab_session/aws_env/#fastqc","title":"FastQC","text":"<p>FastQC is a quality control program for raw sequencing data. </p>"},{"location":"lab_session/aws_env/#gatk-4","title":"GATK 4","text":"<p>GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. </p>"},{"location":"lab_session/aws_env/#gffcompare","title":"Gffcompare","text":"<p>Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. </p>"},{"location":"lab_session/aws_env/#hisat2","title":"HISAT2","text":"<p>HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. </p>"},{"location":"lab_session/aws_env/#kallisto","title":"Kallisto","text":"<p>Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data.</p>"},{"location":"lab_session/aws_env/#mosdepth","title":"mosdepth","text":"<p>mosdepth is a program for determining depth in sequencing data. </p>"},{"location":"lab_session/aws_env/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.</p>"},{"location":"lab_session/aws_env/#picard","title":"PICARD","text":"<p>PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. </p> <ul> <li>Try to answer the following:<ul> <li>Who is Picard?<ul> <li>Clue: \"TNG\"</li> </ul> </li> </ul> </li> </ul>"},{"location":"lab_session/aws_env/#pizzly","title":"Pizzly","text":"<p>Pizzly is a fusion detection algorithm which uses output from Kallisto. </p>"},{"location":"lab_session/aws_env/#sambamba","title":"Sambamba","text":"<p>Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. </p>"},{"location":"lab_session/aws_env/#samtools","title":"Samtools","text":"<p>Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. </p>"},{"location":"lab_session/aws_env/#seqtk","title":"seqtk","text":"<p>Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module.</p>"},{"location":"lab_session/aws_env/#strelka","title":"Strelka","text":"<p>Strelka is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. </p>"},{"location":"lab_session/aws_env/#stringtie","title":"StringTie","text":"<p>StringTie is a software program to perform transcript assembly and quantification of RNAseq data. </p>"},{"location":"lab_session/aws_env/#varscan","title":"Varscan","text":"<p>Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted  on github. </p>"},{"location":"lab_session/aws_env/#vcf-annotation-tools","title":"vcf-annotation-tools","text":"<p>VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files.</p>"},{"location":"lab_session/aws_env/#vep","title":"VEP","text":"<p>VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. </p>"},{"location":"lab_session/aws_env/#vt","title":"vt","text":"<p>vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants.</p>"},{"location":"lab_session/basic_unix/","title":"Unix for Bioinformatics","text":""},{"location":"lab_session/basic_unix/#outline","title":"Outline:","text":"<ol> <li>What is the command line?</li> <li>Directory Structure</li> <li>Syntax of a Command</li> <li>Options of a Command</li> <li>Command Line Basics (ls, pwd, Ctrl + C, man, alias, ls -lthra)</li> <li>Getting Around (cd)</li> <li>Absolute and Relative Paths</li> <li>Tab Completion</li> <li>History Repeats Itself (history, head, tail, )</li> <li>Editing Yourself ( Ctrl + A, Ctrl +E, Ctrl +K, Ctrl +W )</li> <li>Create and Destroy (echo, cat, rm, rmdir)</li> <li>Transferring Files (scp)</li> <li>Piping and Redirection (|, &gt;, \u00bb, cut, sort, grep)</li> <li>Compressions and Archives (tar, gzip, gunzip)</li> <li>Forced Removal (rm -r)</li> <li>BASH Wildcard Characters (?, *, find, environment variables($), quotes/ticks)</li> <li>Manipulation of a FASTA file (cp, mv, wc -l/-c)</li> <li>Symbolic Links (ln -s)</li> <li>STDOUT and STDERR (&gt;1, &gt;2)</li> <li>Paste Command (paste, for loops)</li> <li>Shell Scripts and File Permissions (chmod, nano, ./)</li> </ol>"},{"location":"lab_session/basic_unix/#a-simple-unix-cheat-sheet","title":"A Simple Unix cheat sheet","text":"<p>Download Unix Cheat Sheet</p>"},{"location":"lab_session/basic_unix/#what-is-unix","title":"What is UNIX?","text":"<p>UNIX is an operating system which was first developed in the 1960s, and has been under constant development ever since. By operating system, we mean the suite of programs which make the computer work. It is a stable, multi-user, multi-tasking system for servers, desktops and laptops.</p> <p>UNIX systems also have a graphical user interface (GUI) similar to Microsoft Windows which provides an easy to use environment. However, knowledge of UNIX is required for operations which aren't covered by a graphical program, or for when there is no windows interface available, for example, in a telnet session.</p>"},{"location":"lab_session/basic_unix/#types-of-unix","title":"Types of UNIX","text":"<p>There are many different versions of UNIX, although they share common similarities. The most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X.</p> <p>Here in the School, we use Solaris on our servers and workstations, and Fedora Linux on the servers and desktop PCs.</p>"},{"location":"lab_session/basic_unix/#the-unix-operating-system","title":"The UNIX operating system","text":"<p>The UNIX operating system is made up of three parts; the kernel, the shell and the programs.</p>"},{"location":"lab_session/basic_unix/#the-kernel","title":"The kernel","text":"<p>The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore and communications in response to system calls.</p> <p>As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile (which has the effect of removing the file myfile). The shell searches the filestore for the file containing the program rm, and then requests the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished running, the shell then returns the UNIX prompt % to the user, indicating that it is waiting for further commands.</p>"},{"location":"lab_session/basic_unix/#the-shell","title":"The shell","text":"<p>The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt (% on our systems).</p> <p>The adept user can customise his/her own shell, and users can use different shells on the same machine. This shell is an all-text display (most of the time your mouse doesn\u2019t work) and is accessed using an application called the \"terminal\" which usually looks like a black window with white letters or a white window with black letters by default.</p> <p>The tcsh shell has certain features to help the user inputting commands.</p> <p>Filename Completion - By typing part of the name of a command, filename or directory and pressing the [Tab] key, the tcsh shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will beep, prompting you to type a few more letters before pressing the tab key again.</p> <p>History - The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or type history for a list of previous commands.</p>"},{"location":"lab_session/basic_unix/#files-and-processes","title":"Files and processes","text":"<p>Everything in UNIX is either a file or a process.</p> <p>A process is an executing program identified by a unique PID (process identifier).</p> <p>A file is a collection of data. They are created by users using text editors, running compilers etc.</p> <p>Examples of files:</p> <ul> <li>a document (report, essay etc.)</li> <li>the text of a program written in some high-level programming language</li> <li>instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file);</li> <li>a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files.</li> </ul>"},{"location":"lab_session/basic_unix/#the-directory-structure","title":"The Directory Structure","text":"<p>All the files are grouped together in the directory structure. The file-system is arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root (written as a slash / )</p> <p>Absolute path: always starts with \u201d/\u201d - the root folder</p> <p><pre><code>/home/ubuntu/workspace\n</code></pre> the folder (or file) \u201cworkspace\u201d in the folder \u201cubuntu\u201d or \"student#1\" in the folder \u201chome\u201d in the folder from the root.</p> <p>Relative path: always relative to our current location.</p> <p>a single dot (.) refers to the current directory two dots (..) refers to the directory one level up</p>"},{"location":"lab_session/basic_unix/#_1","title":"Basic Unix","text":"<p>Usually, /home is where the user accounts reside, ie. users\u2019 \u2018home\u2019 directories. For example, for a user that has a username of \u201cstudent#1\u201d: their home directory is /home/student1. It is the directory that a user starts in after starting a new shell or logging into a remote server.</p> <p>The tilde (~) is a short form of a user\u2019s home directory.</p>"},{"location":"lab_session/basic_unix/#starting-an-unix-terminal","title":"Starting an UNIX terminal","text":""},{"location":"lab_session/basic_unix/#_2","title":"Basic Unix","text":"<p>After opening or logging into a terminal, system messages are often displayed, followed by the \u201cprompt\u201d. A prompt is a short text message at the start of the command line and ends with a $ in bash shell, commands are typed after the prompt. The prompt typically follows the form username@server:current_directory $ . </p> <p>If your screen looks like the one below, i.e. your see your a bunch of messages and then your ubuntu student instance number followed by \u201c@172.31.84.105:~$\u201d at the beginning of the line, then you are successfully logged in.</p> <p></p>"},{"location":"lab_session/basic_unix/#unix-basics","title":"Unix Basics","text":"<p>First some basics - how to look at your surroundings</p>"},{"location":"lab_session/basic_unix/#present-working-directory-where-am-i","title":"present working directory \u2026 where am I?","text":"<pre><code>pwd\n</code></pre> <p>see the words separated by /. this is called the path. In unix, the location of each folder or file is shown like this.. this is more like the address or the way to find a folder or file. </p> <p>for example, my Desktop, Documents and Downloads folder usually are located in my home directory so in that case, it path to the folders will be </p> <pre><code>/home/ubuntu/Desktop\n/home/ubuntu/Documents\n/home/ubuntu/Downloads\n</code></pre>"},{"location":"lab_session/basic_unix/#syntax-of-a-command","title":"Syntax of a command","text":"<p>A command plus the required parameters/arguments The separator used in issuing a command is space, number of spaces does not matter.</p> <p></p>"},{"location":"lab_session/basic_unix/#now-lets-try-some-basic-and-simple-commands","title":"Now let's try some basic and simple commands","text":"<p>list files here\u2026 you should see just the folders that we have created for you here and nothing else. <pre><code>ls\n</code></pre> list files somewhere else, like /tmp/</p> <pre><code>ls /tmp\n</code></pre>"},{"location":"lab_session/basic_unix/#tip","title":"TIP!","text":"<ul> <li>In unix one of the first things that\u2019s good to know is how to escape once you\u2019ve started something you don\u2019t want. Use Ctrl + c (shows as \u2018^C\u2019 in the terminal) to exit (kill) a command. In some cases, a different key sequence is required ( Ctrl + d ). Note that anything including and after a \u201c#\u201d symbol is ignored, i.e. a comment. So in all the commands below, you do not have to type anything including and past a \u201c#\u201d.</li> </ul>"},{"location":"lab_session/basic_unix/#options","title":"Options","text":"<p>Each command can act as a basic tool, or you can add \u2018options\u2019 or \u2018flags\u2019 that modify the default behavior of the tool. These flags come in the form of \u2018-v\u2019 \u2026 or, when it\u2019s a more descriptive word, two dashes: \u2018--verbose\u2019 \u2026 that\u2019s a common (but not universal) one that tells a tool that you want it to give you output with more detail. Sometimes, options require specifying amounts or strings, like \u2018-o results.txt\u2019 or \u2018--output results.txt\u2019 \u2026 or \u2018-n 4\u2019 or \u2018--numCPUs 4\u2019. Let\u2019s try some, and see what the man page for the \u2018list files\u2019 command \u2018ls\u2019 is like.</p> <pre><code>ls -R\n</code></pre> <p>Lists directories and files recursively. This will be a very long output, so use Ctrl + C to break out of it. Sometimes you have to press Ctrl + C many times to get the terminal to recognize it. In order to know which options do what, you can use the manual pages. To look up a command in the manual pages type \u201cman\u201d and then the command name. So to look up the options for \u201cls\u201d, type:</p> <p><pre><code>man ls\n</code></pre> Navigate this page using the up and down arrow keys, PageUp and PageDown, and then use q to quit out of the manual. In this manual page, find the following options, quit the page, and then try those commands. You could even open another terminal, log in again, and run manual commands in that terminal.</p> <p><pre><code>ls -l /usr/bin/ #long format, gives permission values, owner, group, size, modification time, and name\n</code></pre> </p>"},{"location":"lab_session/basic_unix/#exercise","title":"Exercise:","text":"<p>Feel free to see manual pages for these basic commands</p> <p>Commands and their  Meanings <pre><code>man ls  \nls -a   \nman mkdir   </code></pre></p> <p>Now see the difference between these three commands <pre><code>cd cd ~ cd ..   change to parent directory\n</code></pre></p> <p>Also these commands</p> <p><pre><code>ls -l\nls -a\nls -l -a \nls -la\nls -ltrha\nls -ltrha --color\n</code></pre> </p> <p>Quick aside: what if I want to use same options repeatedly? and be lazy? You can create a shortcut to another command using \u2018alias\u2019.</p> <pre><code>alias ll='ls -lah'\nll\n</code></pre>"},{"location":"lab_session/basic_unix/#getting-around","title":"Getting Around","text":"<p>The filesystem you\u2019re working on is like the branching root system of a tree. The top level, right at the root of the tree, is called the \u2018root\u2019 directory, specified by \u2018/\u2019 \u2026 which is the divider for directory addresses, or \u2018paths\u2019. </p> <p>Now let's see a little about the commands you checked in the exercise above. </p> <p>We move around using the \u2018change directory\u2019 command, \u2018cd\u2019. The command pwd return the present working directory.</p> <pre><code>cd  # no effect? that's because by itself it sends you home (to ~)\ncd /  # go to root of tree's root system\ncd home  # go to where everyone's homes are\npwd\ncd username  # use your actual home, not \"username\"\npwd\ncd /\npwd\ncd ~  # a shortcut to home, from anywhere\npwd\ncd .  # '.' always means *this* directory\npwd\ncd ..  # '..' always means *one directory up*\npwd\n</code></pre>"},{"location":"lab_session/basic_unix/#absolute-and-relative-paths","title":"Absolute and Relative Paths","text":"<p>You can think of paths like addresses. You can tell your friend how to go to a particular store from where they are currently (a \u2018relative\u2019 path), or from the main Interstate Highway that everyone uses (in this case, the root of the filesystem, \u2018/\u2019 \u2026 this is an \u2018absolute\u2019 path). Both are valid. But absolute paths can\u2019t be confused, because they always start off from the same place, and are unique. Relative paths, on the other hand, could be totally wrong for your friend if you assume they\u2019re somewhere they\u2019re not. With this in mind, let\u2019s try a few more:</p> <pre><code>cd /usr/bin  # let's start in /usr/bin\n#relative (start here, take one step up, then down through lib and gcc)\n\ncd ../lib/init/\npwd\n#absolute (start at root, take steps)\n\ncd /usr/lib/init/\npwd\n</code></pre> <p>Now, because it can be a real pain to type out, or remember these long paths, we need to discuss \u2026</p>"},{"location":"lab_session/basic_unix/#tab-completion","title":"Tab Completion","text":"<p>Using tab-completion is a must on the command line. A single <code>&lt;tab&gt;</code> auto-completes file or directory names when there\u2019s only one name that could be completed correctly. If multiple files could satisfy the tab-completion, then nothing will happen after the first <code>&lt;tab&gt;</code>. In this case, press <code>&lt;tab&gt;</code> a second time to list all the possible completing names. Note that if you\u2019ve already made a mistake that means that no files will ever be completed correctly from its current state, then <code>&lt;tab&gt;</code>\u2019s will do nothing.</p> <p>touch updates the timestamp on a file, here we use it to create three empty files.</p> <p><pre><code>cd # go to your home directory\nmkdir ~/tmp\ncd ~/tmp\ntouch one seven september\nls o\n</code></pre> tab with no enter should complete to \u2018one\u2019, then enter</p> <pre><code>ls s\n</code></pre> <p>tab with no enter completes up to \u2018se\u2019 since that\u2019s in common between seven and september. tab again and no enter, this second tab should cause listing of seven and september. type \u2018v\u2019 then tab and no enter now it\u2019s unique to seven, and should complete to seven. enter runs \u2018ls seven\u2019 command.</p> <p>It cannot be overstated how useful tab completion is. You should get used to using it constantly. Watch experienced users type and they maniacally hit tab once or twice in between almost every character. You don\u2019t have to go that far, of course, but get used to constantly getting feedback from hitting tab and you will save yourself a huge amount of typing and trying to remember weird directory and filenames.</p>"},{"location":"lab_session/basic_unix/#time-to-shift-gears-and-pick-up-some-speed-now","title":"TIME TO SHIFT GEARS AND PICK UP SOME SPEED NOW!","text":""},{"location":"lab_session/basic_unix/#history-repeats-itself","title":"History Repeats Itself","text":"<p>Linux remembers everything you\u2019ve done (at least in the current shell session), which allows you to pull steps from your history, potentially modify them, and redo them. This can obviously save a lot of time and typing.</p> <p>The \u2018head\u2019 command views the first 10 (by default) lines of a file. The \u2018tail\u2019 commands views the last 10 (by default) lines of a file. Type \u2018man head\u2019 or \u2018man tail\u2019 to consult their manuals.</p> <pre><code>&lt;up arrow&gt;  # last command\n&lt;up&gt;  # next-to-last command\n&lt;down&gt;  # last command, again\n&lt;down&gt;  # current command, empty or otherwise\nhistory  # usually too much for one screen, so ...\nhistory | head # we discuss pipes (the vertical bar) below\nhistory | tail\nhistory | less # use 'q' to exit less\nls -l\npwd\nhistory | tail\n!560  # re-executes 560th command (yours will have different numbers; choose the one that recreates your really important result!)\n</code></pre>"},{"location":"lab_session/basic_unix/#editing-yourself","title":"Editing Yourself","text":"<p>Here are some more ways to make editing previous commands, or novel commands that you\u2019re building up, easier:</p> <pre><code>&lt;up&gt;&lt;up&gt;  # go to some previous command, just to have something to work on\n&lt;ctrl-a&gt;  # go to the beginning of the line\n&lt;ctrl-e&gt;  # go to the end of the line\n#now use left and right to move to a single word (surrounded by whitespace: spaces or tabs)\n&lt;ctrl-k&gt;  # delete from here to end of line\n&lt;ctrl-w&gt;  # delete from here to beginning of preceeding word\nblah blah blah&lt;ctrl-w&gt;&lt;ctrl-w&gt;  # leaves you with only one 'blah'\n</code></pre> <p>You can also search your history from the command line:</p> <pre><code>&lt;ctrl-r&gt;fir  # should find most recent command containing 'fir' string: echo 'first' &gt; test.txt\n&lt;enter&gt;  # to run command\n&lt;ctrl-c&gt;  # get out of recursive search\n&lt;ctr-r&gt;  # repeat &lt;ctrl-r&gt; to find successively older string matches\n</code></pre>"},{"location":"lab_session/basic_unix/#create-and-destroy","title":"Create and Destroy","text":"<p>We already learned one command that will create a file, touch. Now let\u2019s look at create and removing files and directories.</p> <pre><code>cd  # home again\nmkdir ~/tmp2\ncd ~/tmp2\necho 'Hello, world!' &gt; first.txt\necho text then redirect (\u2018&gt;\u2019) to a file.\n\ncat first.txt  # 'cat' means 'concatenate', or just spit the contents of the file to the screen\n</code></pre>"},{"location":"lab_session/basic_unix/#why-concatenate-try-this","title":"why \u2018concatenate\u2019? try this:","text":"<pre><code>cat first.txt first.txt first.txt &gt; second.txt\ncat second.txt\n</code></pre> <p>OK, let\u2019s destroy what we just created:</p> <pre><code>cd ../\nrmdir tmp2  # 'rmdir' meands 'remove directory', but this shouldn't work!\nrm tmp2/first.txt\nrm tmp2/second.txt  # clear directory first\nrmdir tmp2  # should succeed now\n</code></pre> <p>So, \u2018mkdir\u2019 and \u2018rmdir\u2019 are used to create and destroy (empty) directories. \u2018rm\u2019 to remove files. To create a file can be as simple as using \u2018echo\u2019 and the \u2018&gt;\u2019 (redirection) character to put text into a file. Even simpler is the \u2018touch\u2019 command.</p> <p><pre><code>mkdir ~/cli\ncd ~/cli\ntouch newFile\nls -ltra  # look at the time listed for the file you just created\ncat newFile  # it's empty!\nsleep 60  # go grab some coffee\ntouch newFile\nls -ltra  # same time?\n</code></pre> So \u2018touch\u2019 creates empty files, or updates the \u2018last modified\u2019 time. Note that the options on the \u2018ls\u2019 command you used here give you a Long listing, of All files, in Reverse Time order (l, a, r, t).</p>"},{"location":"lab_session/basic_unix/#forced-removal-caution","title":"Forced Removal (CAUTION!!!)","text":"<p>When you\u2019re on the command line, there\u2019s no \u2018Recycle Bin\u2019. Since we\u2019ve expanded a whole directory tree, we need to be able to quickly remove a directory without clearing each subdirectory and using \u2018rmdir\u2019. <pre><code>cd\nmkdir -p rmtest/dir1/dir2 # the -p option creates all the directories at once\nrmdir rmtest # gives an error since rmdir can only remove directories that are empty\nrm -rf rmtest # will remove the directory and EVERYTHING in it\n</code></pre></p> <p>Here -r = recursively remove sub-directories, -f means force. Obviously, be careful with \u2018rm -rf\u2019, there is no going back, if you delete something with rm, rmdir its gone! There is no Recycle Bin on the Command-Line!</p>"},{"location":"lab_session/basic_unix/#piping-and-redirection","title":"Piping and Redirection","text":"<p>Pipes (\u2018|\u2019) allow commands to hand output to other commands, and redirection characters (\u2018&gt;\u2019 and \u2018\u00bb\u2019) allow you to put output into files.</p> <pre><code>echo 'first' &gt; test.txt\ncat test.txt # outputs the contents of the file to the terminal\necho 'second' &gt; test.txt\ncat test.txt\necho 'third' &gt;&gt; test.txt\ncat test.txt\n</code></pre> <p>The \u2018&gt;\u2019 character redirects output of a command that would normally go to the screen instead into a specified file. \u2018&gt;\u2019 overwrites the file, \u2018\u00bb\u2019 appends to the file.</p> <p>The \u2018cut\u2019 command pieces of lines from a file line by line. This command cuts characters 1 to 3, from every line, from file \u2018test.txt\u2019</p> <p><pre><code>cut -c 1-3 test.txt\n</code></pre> same thing, piping output of one command into input of another</p> <p><pre><code>cat test.txt | cut -c 1-3  </code></pre> This pipes (i.e., sends the output of) cat to cut to sort (-r means reverse order sort), and then grep searches for pattern (\u2018s\u2019) matches (i.e. for any line where an \u2018s\u2019 appears anywhere on the line.)</p> <pre><code>cat test.txt | cut -c 1-3 | sort -r\ncat test.txt | cut -c 1-3 | sort -r | grep s\n</code></pre> <p>This is a great way to build up a set of operations while inspecting the output of each step in turn. We\u2019ll do more of this in a bit.</p>"},{"location":"lab_session/basic_unix/#compression-and-archives","title":"Compression and Archives","text":"<p>As file sizes get large, you\u2019ll often see compressed files, or whole compressed folders. Note that any good bioinformatics software should be able to work with compressed file formats.</p> <p><pre><code>gzip test.txt\ncat test.txt.gz\n</code></pre> To uncompress a file <pre><code>gunzip -c test.txt.gz\n</code></pre></p> <p>The \u2018-c\u2019 leaves the original file alone, but dumps expanded output to screen</p> <pre><code>gunzip test.txt.gz  # now the file should change back to uncompressed test.txt\n</code></pre> <p>Tape archives, or .tar files, are one way to compress entire folders and all contained folders into one file. When they\u2019re further compressed they\u2019re called \u2018tarballs\u2019. We can use wget (web get).</p> <pre><code>wget -L -O PhiX_Illumina_RTA.tar.gz http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/PhiX/Illumina/RTA/PhiX_Illumina_RTA.tar.gz\n</code></pre> <p>The .tar.gz and .tgz are commonly used extensions for compressed tar files, when gzip compression is used. The application tar is used to uncompress .tar files</p> <p><pre><code>tar -xzvf PhiX_Illumina_RTA.tar.gz\n</code></pre> Here -x = extract, -z = use gzip/gunzip, -v = verbose (show each file in archive), -f filename</p> <p>Note that, unlike Windows, linux does not depend on file extensions to determine file behavior. So you could name a tarball \u2018fish.puppy\u2019 and the extract command above should work just fine. The only thing that should be different is that tab-completion doesn\u2019t work within the \u2018tar\u2019 command if it doesn\u2019t see the \u2018correct\u2019 file extension.</p>"},{"location":"lab_session/basic_unix/#bash-wildcard-characters","title":"BASH Wildcard Characters","text":"<p>We can use \u2018wildcard characters\u2019 when we want to specify or operate on sets of files all at once.</p> <p><pre><code>ls ?hiX/Illumina\n</code></pre> list files in Illumina sub-directory of any directory ending in \u2018hiX\u2019</p> <p><pre><code>ls PhiX/Illumina/RTA/Sequence/*/*.fa\n</code></pre> list all files ending in \u2018.fa\u2019 a few directories down. So, \u2018?\u2019 fills in for zero or one character, \u2018*\u2019 fills in for zero or more characters. The \u2018find\u2019 command can be used to locate files using a similar form.</p> <p><pre><code>find . -name \"*.f*\"\nfind . -name \"*.f?\"\n</code></pre> how is this different from the previous ls commands?</p>"},{"location":"lab_session/basic_unix/#quick-note-about-the-quotes","title":"Quick Note About the Quote(s)","text":"<p>The quote characters \u201c and \u2018 are different. In general, single quotes preserve the literal meaning of all characters between them. On the other hand, double quotes allow the shell to see what\u2019s between them and make substitutions when appropriate. For example:</p> <p><pre><code>VRBL=someText\necho '$VRBL'\necho \"$VRBL\"\n</code></pre> However, some commands try to be \u2018smarter\u2019 about this behavior, so it\u2019s a little hard to predict what will happen in all cases. It\u2019s safest to experiment first when planning a command that depends on quoting \u2026 list filenames first, instead of changing them, etc. Finally, the \u2018backtick\u2019 characters ` (same key - unSHIFTED - as the tilde ~) causes the shell to interpret what\u2019s between them as a command, and return the result.</p>"},{"location":"lab_session/basic_unix/#counts-the-number-of-lines-in-file-and-stores-result-in-the-lines-variable","title":"counts the number of lines in file and stores result in the LINES variable","text":"<pre><code>LINES=`cat PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome.1.bt2 | wc -l` echo $LINES\n</code></pre>"},{"location":"lab_session/basic_unix/#symbolic-links","title":"Symbolic Links","text":"<p>Since copying or even moving large files (like sequence data) around your filesystem may be impractical, we can use links to reference \u2018distant\u2019 files without duplicating the data in the files. Symbolic links are disposable pointers that refer to other files, but behave like the referenced files in commands. I.e., they are essentially \u2018Shortcuts\u2019 (to use a Windows term) to a file or directory.</p> <p>The \u2018ln\u2019 command creates a link. You should, by default, always create a symbolic link using the -s option.</p> <pre><code>ln -s PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa .\nls -ltrhaF  # notice the symbolic link pointing at its target\ngrep -c \"&gt;\" genome.fa\n</code></pre>"},{"location":"lab_session/basic_unix/#stdout-stderr","title":"STDOUT &amp; STDERR","text":"<p>Programs can write to two separate output streams, \u2018standard out\u2019 (STDOUT), and \u2018standard error\u2019 (STDERR). The former is generally for direct output of a program, while the latter is supposed to be used for reporting problems. I\u2019ve seen some bioinformatics tools use STDERR to report summary statistics about the output, but this is probably bad practice. Default behavior in a lot of cases is to dump both STDOUT and STDERR to the screen, unless you specify otherwise. In order to nail down what goes where, and record it for posterity:</p> <p><pre><code>wc -c genome.fa 1&gt; chars.txt 2&gt; any.err\n</code></pre> the 1st output, STDOUT, goes to \u2018chars.txt\u2019 the 2nd output, STDERR, goes to \u2018any.err\u2019</p> <p><pre><code>cat chars.txt\n</code></pre> Contains the character count of the file genome.fa</p> <p><pre><code>cat any.err\n</code></pre> Empty since no errors occured.</p> <p>Saving STDOUT is pretty routine (you want your results, yes?), but remember that explicitly saving STDERR is important on a remote server, since you may not directly see the \u2018screen\u2019 when you\u2019re running jobs.</p>"},{"location":"lab_session/basic_unix/#the-sed-command","title":"The sed command","text":"<p>Let\u2019s take a look at the \u2018sed\u2019 command. NOTE: On Macs use \u2018gsed\u2019. sed (short for stream editor) is a command that allows you to manipulate character data in various ways. One useful thing it can do is substitution. Let\u2019s download a simple file to work on:</p> <pre><code>wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/region.bed -O region.bed\n</code></pre> <p>Take a look at the file:</p> <p><pre><code>cat region.bed\n</code></pre> Now, let\u2019s make all the uppercase \u201cCHR\u201ds into lowercase:</p> <p><pre><code>cat region.bed | sed 's/CHR/chr/'\n</code></pre> What happened? Only the first CHR changed. That is because we need to add the \u201cg\u201d option:</p> <p><pre><code>cat region.bed | sed 's/CHR/chr/g'\n</code></pre> We can also do the the substitution without regards to case:</p> <p><pre><code>cat region.bed | sed 's/chr/chr/gi'\n</code></pre> Let\u2019s break down the argument to sed (within the single quotes)\u2026 The \u201cs\u201d means \u201csubstitute\u201d, the word between the 1st and 2nd forward slashes (i.e. /) is the word the substitute for, the word between the 2nd and 3rd slashes is the word to substitute with, and finally the \u201cgi\u201d at the end are flags for global substitution (i.e. substituting along an entire line instead of just the first occurence on a line), and for case insenstivity (i.e. it will ignore the case of the letters when doing the substitution).</p> <p>Note that this doesn\u2019t change the file itself, it is simply piping the output of the cat command to sed and outputting to the screen. If you wanted to change the file itself, you could use the \u201c-i\u201d option to sed:</p> <p><pre><code>cat region.bed\nsed -i 's/chr/chr/gi' region.bed\n</code></pre> Now if you look at the file, the lines have changed.</p> <p><pre><code>cat region.bed\n</code></pre> Another useful use of sed is for capturing certain lines from a file. You can select certain lines from a file:</p> <p><pre><code>sed '4q;d' region.bed\n</code></pre> This will just select the 4th line from the file.</p> <p>You can also extract a range of lines from a file:</p> <p><pre><code>sed -n '10,20p' region.bed\n</code></pre> This gets the 10th through 20th lines from the file.</p> <p>CHALLENGE: See if you can find a way to use sed to remove all the \u201cCHR\u201ds from the file.</p>"},{"location":"lab_session/basic_unix/#more-pipes","title":"More pipes","text":"<p>Now, let\u2019s delve into pipes a little more. Pipes are a very powerful way to look at and manipulate complex data using a series of simple programs. Let\u2019s look at some fastq files. Get a few small fastq files:</p> <p><pre><code>wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/C61.subset.fq.gz -O C61.subset.fq.gz\nwget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I561.subset.fq.gz -O I561.subset.fq.gz\nwget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I894.subset.fq.gz -O I894.subset.fq.gz\n</code></pre> Since the files are gzipped files we need to use \u201czcat\u201d to look at them. zcat is just like cat except for gzipped files:</p> <p><pre><code>zcat C61.subset.fq.gz | head\n</code></pre> Fastq records are 4 lines per sequence, a header line, the sequence, a plus sign (which is historical), and then the quality encoding for the sequence. Notice that each header line has the barcode for that read at the end of the line. Let\u2019s count the number of each barcode. In order to do that we need to just capture the header lines from this file. We can use \u201csed\u201d to do that:</p> <p><pre><code>zcat C61.subset.fq.gz | sed -n '1~4p' | head\n</code></pre> By default sed prints every line. In this case we are giving the \u201c-n\u201d option to sed which will not print every line. Instead, we are giving it the argument \u201c1~4p\u201d, which means to print the first line, then skip 4 lines and print again, and then continue to do that.</p> <p>Now that we have a way to get just the headers, we need to isolate the part of the header that is the barcode. There are multiple ways to do this\u2026 we will use the cut command:</p> <pre><code>zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | head\n</code></pre> <p>So we are using the \u201c-d\u201d option to cut with \u201c:\u201d as the argument to that option, meaning that we will be using the delimiter \u201c:\u201d to split the input. Then we use the \u201c-f\u201d option with argument \u201c10\u201d, meaning that we want the 10th field after the split. In this case, that is the barcode.</p> <p>Finally, as before, we need to sort the data and then use \u201cuniq -c\u201d to count. Then put it all together and run it on the entire dataset (This will take about a minute to run):</p> <pre><code>zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | sort | uniq -c\n</code></pre> <p>Now you have a list of how many reads were categorized into each barcode. Here is a sed tutorial for more exercises.</p> <p>CHALLENGE: Find the distribution of the first 5 bases of all the reads in C61_S67_L006_R1_001.fq.gz. I.e., count the number of times the first 5 bases of every read occurs across all reads.</p>"},{"location":"lab_session/basic_unix/#loops","title":"Loops","text":"<p>Loops are useful for quickly telling the shell to perform one operation after another, in series. For example:</p> <pre><code>for i in {1..21}; do echo $i &gt;&gt; a; done  # put multiple lines of code on one line, each line terminated by ';'\ncat a\n</code></pre>"},{"location":"lab_session/basic_unix/#1-through-21-on-separate-lines","title":"&lt;1 through 21 on separate lines&gt;","text":"<p>The general form is:</p> <p><pre><code>for name in {list}; do\ncommands\ndone\n</code></pre> The list can be a sequence of numbers or letters, or a group of files specified with wildcard characters:</p> <pre><code>for i in {3,2,1,liftoff}; do echo $i; done  # needs more excitement!\nfor i in {3,2,1,\"liftoff!\"}; do echo $i; done  # exclamation point will confuse the shell unless quoted\n</code></pre> <p>A \u201cwhile\u201d loop is more convenient than a \u201cfor\u201d loop \u2026 if you don\u2019t readily know how many iterations of the loop you want:</p> <pre><code>while {condition}; do\ncommands\ndone\n</code></pre> <p>Now, let\u2019s do some bioinformatics-y things with loops and pipes. First, let\u2019s write a command to get the nucleotide count of the first 10,000 reads in a file. Use zcat and sed to get only the read lines of a file, and then only take the first 10,000:</p> <pre><code>zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | less\n</code></pre> <p>Use grep\u2019s \u201c-o\u201d option to get each nucleotide on a separate line (take a look at the man page for grep to understand how this works):</p> <pre><code>zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | less\n</code></pre> <p>Finally, use sort and uniq to get the counts:</p> <p><pre><code>zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c\n 264012 A\n 243434 C\n 215045 G\n    278 N\n 277231 T\n</code></pre> And, voila, we have the per nucleotide count for these reads!</p> <p>We just did this for one file, but what if we wanted to do it for all of our files? We certainly don\u2019t want to type the command by hand dozens of times. So we\u2019ll use a while loop. You can pipe a command into a while loop and it will iterate through each line of the input. First, get a listing of all your files:</p> <pre><code>ls -1 *.fq.gz\n</code></pre> <p>Pipe that into a while loop and read in the lines into a variable called \u201cx\u201d. We use \u201c$x\u201d to get the value of the variable in that iteration of the loop:</p> <p><pre><code>ls -1 *.fq.gz | while read x; do echo $x is being processed...; done\n</code></pre> Add the command we created above into the loop, placing $x where the filename would be and semi-colons inbetween commands:</p> <p><pre><code>ls -1 *.fq.gz | while read x; do echo $x is being processed...; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c; done\n</code></pre> When this runs it will print the name of every single file being processed and the nucleotide count for the reads from those files.</p> <p>Now, let\u2019s say you wanted to write the output of each command to a separate file. We would redirect the output to a filename, but we need to create a different file name for each command and we want the file name to reflect its contents, i.e. the output file name should be based on the input file name. So we use \u201cparameter expansion\u201d, which is fancy way of saying substitution:</p> <pre><code>ls -1 *.fq.gz | while read x; do echo $x is being processed...; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c &gt; ${x%.fq.gz}.nucl_count.txt; done\n</code></pre> <p>This will put the output of the counting command into a file whose name is the prefix of the input file plus \u201c.nucl_count.txt\u201d. It will do this for every input file.</p>"},{"location":"lab_session/basic_unix/#manipulation-of-a-fasta-file","title":"Manipulation of a FASTA File","text":"<p>Let\u2019s copy the phiX-174 genome (using the \u2018cp\u2019 command) to our current directory so we can play with it:</p> <pre><code>cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa phix.fa    </code></pre> <p>Similarly we can also use the move command here, but then ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa will no longer be there:</p> <pre><code>cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa  ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa\nls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/\nmv ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa phix.fa\nls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/\n</code></pre> <p>This functionality of mv is why it is used to rename files.</p> <p>Note how we copied the \u2018genome.fa\u2019 file to a different name: \u2018phix.fa\u2019</p> <pre><code>wc -l phix.fa\n</code></pre> <p>count the number of lines in the file using \u2018wc\u2019 (word count) and parameter \u2018-l\u2019 (lines).</p> <p>We can use the \u2018grep\u2019 command to search for matches to patterns. \u2018grep\u2019 comes from \u2018globally search for a regular expression and print\u2019.</p> <p><pre><code>grep -c '&gt;' phix.fa\n\n\n#Only one FASTA sequence entry, since only one header line (\u2018&gt;gi|somethingsomething\u2026\u2019)\n\n\ncat phix.fa\n</code></pre> This may not be useful for anything larger than a virus! Let\u2019s look at the start codon and the two following codons:</p> <pre><code>grep --color \"ATG......\" phix.fa\n#\u2019.\u2019 characters are the single-character wildcards for grep. So \u201cATG\u2026\u2026\u201d matches any set of 9 characters that starts with ATG.\n\n#Use the \u2013color \u2018-o\u2019 option to only print the pattern matches, one per line\n\ngrep -o \"ATG......\" phix.fa\n\n#Use the \u2018cut\u2019 command with \u2018-c\u2019 to select characters 4-6, the second codon\n\ngrep --color  -o \"ATG......\" phix.fa | cut -c4-6\n\u2018sort\u2019 the second codon sequences (default order is same as ASCII table; see \u2018man ascii\u2019)\n\ngrep --color  -o \"ATG......\" phix.fa | cut -c4-6 | sort\n\n#Combine successive identical sequences, but count them using the \u2018uniq\u2019 command with the \u2018-c\u2019 option\n\ngrep --color  -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c\n\n#Finally sort using reverse numeric order (\u2018-rn\u2019)\n\ngrep --color  -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c | sort -rn\n\u2026 which gives us the most common codons first\n</code></pre> <p>This may not be a particularly useful thing to do with a genomic FASTA file, but it illustrates the process by which one can build up a string of operations, using pipes, in order to ask quantitative questions about sequence content. More generally than that, this process allows one to ask questions about files and file contents and the operating system, and verify at each step that the process so far is working as expected. The command line is, in this sense, really a modular workflow management system.</p>"},{"location":"lab_session/basic_unix/#shell-scripts-file-permissions","title":"Shell Scripts, File Permissions","text":"<p>Often it\u2019s useful to define a whole string of commands to run on some input, so that (1) you can be sure you\u2019re running the same commands on all data, and (2) so you don\u2019t have to type the same commands in over and over! Let\u2019s use the \u2018nano\u2019 text editor program that\u2019s pretty reliably installed on most linux systems.</p> <pre><code>nano test.sh\n</code></pre>"},{"location":"lab_session/basic_unix/#insert-cli_figure7","title":"insert cli_figure7","text":"<p>nano now occupies the whole screen; see commands at the bottom. Let\u2019s type in a few commands. First we need to put the following line at the top of the file:</p> <pre><code>#!/bin/bash\n</code></pre> <p>The \u201c#!\u201d at the beginning of a script tells the shell what language to use to interpret the rest of the script. In our case, we will be writing \u201cbash\u201d commands, so we specify the full path of the bash executable after the \u201c#!\u201d. Then, add some commands:</p> <pre><code>#!/bin/bash\n\necho \"Start script...\"\npwd\nls -l\nsleep 10\necho \"End script.\"\n</code></pre> <p>Hit Cntl-O and then enter to save the file, and then Cntl-X to exit nano.</p> <p>Though there are ways to run the commands in test.sh right now, it\u2019s generally useful to give yourself (and others) \u2018execute\u2019 permissions for test.sh, really making it a shell script. Note the characters in the first (left-most) field of the file listing:</p> <pre><code>ls -lh test.sh\n-rw-rw-r-- 1 ubuntu workspace 79 Dec 19 15:05 test.sh\n</code></pre> <p>The first \u2018-\u2018 becomes a \u2018d\u2019 if the \u2018file\u2019 is actually a directory. The next three characters represent read, write, and execute permissions for the file owner (you), followed by three characters for users in the owner\u2019s group, followed by three characters for all other users. Run the \u2018chmod\u2019 command to change permissions for the \u2018test.sh\u2019 file, adding execute permissions (\u2018+x\u2019) for the user (you) and your group (\u2018ug\u2019):</p> <p><pre><code>chmod ug+x test.sh\nls -lh test.sh\n\n-rwxr-xr-- 1 ubuntu workspace 79 Dec 19 15:05 test.sh\n</code></pre> The first 10 characters of the output represent the file and permissions. The first character is the file type, the next three sets of three represent the file permissions for the user, group, and everyone respectively.</p> <p>r = read w = write x = execute So let\u2019s run this script. We have to provide a relative reference to the script \u2018./\u2019 because its not our our \u201cPATH\u201d.:</p> <p><pre><code>#you can do either \n./test.sh\n\n#or you can run it like this\n\nbash test.sh\n</code></pre> And you should see all the commands in the file run in sequential order in the terminal.</p>"},{"location":"lab_session/basic_unix/#command-line-arguments-for-shell-scripts","title":"Command Line Arguments for Shell Scripts","text":"<p>Now let\u2019s modify our script to use command line arguments, which are arguments that can come after the script name (when executing) to be part of the input inside the script. This allows us to use the same script with different inputs. In order to do so, we add variables $1, $2, $3, etc\u2026. in the script where we want our input to be. So, for example, use nano to modify your test.sh script to look like this:</p> <p><pre><code>#!/bin/bash\n\necho \"Start script...\"\nPWD=`pwd`\necho \"The present working directory is $PWD\"\nls -l $1\nsleep $2\nwc -l $3\necho \"End script.\"\n</code></pre> Now, rerun the script using command line arguments like this:</p> <pre><code>./test.sh genome.fa 15 PhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\n</code></pre> <p>Note that each argument is separated by a space, so $1 becomes \u201cgenome.fa\u201d, $2 becomes \u201c15\u201d, and $3 becomes \u201cPhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\u201d. Then the commands are run using those values. Now rerun the script with some other values:</p> <p><pre><code>./test.sh .. 5 genome.fa\n</code></pre> Now, $1 becomes \u201c..\u201d, $2 is \u201c5\u201d, and $3 is \u201cgenome.fa\u201d.</p>"},{"location":"lab_session/basic_unix/#pipes-and-loops-inside-scripts","title":"Pipes and Loops inside scripts","text":"<p>Open a new text file using the text editor \u201cnano\u201d:</p> <p><pre><code>nano get_nucl_counts.sh\n</code></pre> Copy and Paste the following into the file:</p> <pre><code>#!/bin/bash\n\nzcat $1 | sed -n '2~4p' | head -$2 | grep -o . | sort | uniq -c\n</code></pre> <p>Save the file and exit. Change the permissions on the file to make it executable:</p> <pre><code>chmod a+x get_nucl_counts.sh\n</code></pre> <p>Now, we can run this script giving it different arguments every time. The first argument (i.e. the first text after the script name when it is run) will get put into the variable \u201c\\(1\u201d. The second argument (delimited by spaces) will get put into \u201c\\)2\u201d. In this case, \u201c\\(1\u201d is the file name, and \u201c\\)2\u201d is the number of reads we want to count. So, then we can run the script over and over again using different values and the command will run based on those values:</p> <pre><code>./get_nucl_counts.sh I561.subset.fq.gz 1000\n./get_nucl_counts.sh I561.subset.fq.gz 10000\n./get_nucl_counts.sh C61.subset.fq.gz 555\n</code></pre> <p>We can also put loops into a script. We\u2019ll take the loop we created earlier and put it into a file, breaking it up for readability and using backslashes for line continuation:</p> <p><pre><code>nano get_nucl_counts_loop.sh\n</code></pre> Put this in the file and save it: <pre><code>#!/bin/bash\n\nls -1 *.fq.gz | \\\nwhile read x; do \\\necho $x is being processed...; \\\nzcat $x | sed -n '2~4p' | head -$1 | \\\ngrep -o . | sort | uniq -c &gt; ${x%.fq.gz}.nucl_count.txt; \\\ndone\n</code></pre></p> <p>Make it executable: <pre><code>chmod a+x get_nucl_counts_loop.sh\n</code></pre> And now we can execute the entire loop using the script. Note that there is only one argument now, the number of reads to use:</p> <pre><code>./get_nucl_counts_loop.sh 100\n</code></pre>"},{"location":"lab_session/basic_unix/#finally-a-good-summary-for-rounding-up-the-session","title":"FINALLY - a good summary for rounding up the session","text":"<ul> <li>TEN SIMPLE RULES FOR GETTING STARTED WITH COMMAND-LINE BIOINFORMATICS</li> </ul>"},{"location":"lab_session/basics_awk/","title":"AWK crash course for Bioinformatics","text":"<p>Note: An introduction on how to analyze table data without MS Excel</p>"},{"location":"lab_session/basics_awk/#what-is-awk","title":"What is AWK?","text":"<p>AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool.</p> <p>The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions.</p> <p>AWK has a limited intended application domain, and was especially designed to support one-liner programs.</p> <p>It is a standard feature of most Unix-like operating systems.</p> <p>source: Wikipedia</p>"},{"location":"lab_session/basics_awk/#why-awk","title":"Why awk?","text":"<p>You can replace a pipeline of 'stuff | grep | sed | cut...' with a single call to awk. For a simple script, most of the timelag is in loading these apps into memory, and it's much faster to do it all with one. This is ideal for something like an openbox pipe menu where you want to generate something on the fly. You can use awk to make a neat one-liner for some quick job in the terminal, or build an awk section into a shell script. </p>"},{"location":"lab_session/basics_awk/#simple-awk-commands","title":"Simple AWK commands","text":"<p>An AWK program consists of a sequence of pattern-action statements and optional function definitions. It processes text files. AWK is a line oriented language. It divides a file into lines called records. Each line is broken up into a sequence of fields. The fields are accessed by special variables: $1 reads the first field, $2 the second and so on. The $0 variable refers to the whole record.</p> <p>The structure of an AWK program has the following form: <pre><code>pattern { action }\n</code></pre> The pattern is a test that is performed on each of the records. If the condition is met then the action is performed. Either pattern or action can be omitted, but not both. The default pattern matches each line and the default action is to print the record.</p> <pre><code>awk -f program-file [file-list]\nawk program [file-list]\n</code></pre> <p>An AWK program can be run in two basic ways: a) the program is read from a separate file; the name of the program follows the -f option, b) the program is specified on the command line enclosed by quote characters.</p>"},{"location":"lab_session/basics_awk/#print-all-the-lines-from-a-file","title":"Print all the lines from a file","text":"<p>By default, awk prints all lines of a file , so to print every line of above created file use below command </p> <p><pre><code>awk '{print}' file\n</code></pre> Note: In awk command \u2018{print}\u2019 is used print all fields along with their values.</p>"},{"location":"lab_session/basics_awk/#print-only-specific-field-like-2nd-3rd","title":"Print only specific field like 2nd &amp; 3rd","text":"<p>In awk command, we use $ (dollar) symbol followed by field number to prints field values. </p> <p><pre><code>awk -F \",\" '{print $2, $3}' file\n</code></pre> In the above command we have used the option  -F \u201c,\u201d  which specifies that comma (,) is the field separator in the file. This is usually good practice when dealing with tables where the separators between each column could be single or multiple white-space (\" \") or tab (\"\\t\") or a colon (\":\") or a semicolon (\";\").</p>"},{"location":"lab_session/basics_awk/#awk-one-liners","title":"AWK one-liners","text":"<p>AWK one-liners are simple one-shot programs run from the command line. Let us have the following text file: words.txt</p> <p>We want to print all words included in the words.txt file that are longer than five characters.</p> <p><pre><code>wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/words.txt -O words.txt\nawk 'length($1) &gt; 5 {print $0}' words.txt\n\nstoreroom\nexistence\nministerial\nfalcon\nbookworm\nbookcase\n</code></pre> The AWK program is placed between two single quote characters. The first is the pattern; we specify that the length of the record is greater that five. The length function returns the length of the string. The $1 variable refers to the first field of the record; in our case there is only one field per record. The action is placed between curly brackets.</p> <p><pre><code>awk 'length($1) &gt; 5' words.txt\nstoreroom\nexistence\nministerial\nfalcon\nbookworm\nbookcase\n</code></pre> As we have specified earlier, the action can be omitted. In such a case a default action is performed \u2014 printing of the whole record.</p> <p><pre><code>awk 'length($1) == 3' words.txt\ncup\nsky\ntop\nwar\n</code></pre> We print all words that have three characters.</p> <p><pre><code>awk '!(length($1) == 3)' words.txt\nstoreroom\ntree\nstore\nbook\ncloud\nexistence\nministerial\nfalcon\ntown\nbookworm\nbookcase\n</code></pre> With the ! operator, we can negate the condition; we print all lines that do not have three characters.</p> <p><pre><code>awk '(length($1) == 3) || (length($1) == 4)' words.txt\ntree\ncup\nbook\ntown\nsky\ntop\nwar\n</code></pre> Next we apply conditions on numbers.</p> <p>We have a file with scores of students - scores.txt. <pre><code>wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/scores.txt -O scores.txt\nawk '$2 &gt;= 90 { print $0 }' scores.txt\nLucia 95\nJoe 92\nSophia 90\n</code></pre> We print all students with scores 90+.</p> <p><pre><code>awk '$2 &gt;= 90 { print }' scores.txt\nLucia 95\nJoe 92\nSophia 90\n</code></pre> If we omit an argument for the print function, the $0 is assumed.</p> <p><pre><code>awk '$2 &gt;= 90' scores.txt\nLucia 95\nJoe 92\nSophia 90\n</code></pre> A missing { action } means print the matching line.</p> <p><pre><code>awk '{ if ($2 &gt;= 90) print }' scores.txt\nLucia 95\nJoe 92\nSophia 90\n</code></pre> Instead of a pattern, we can also use an if condition in the action.</p> <p><pre><code>awk '{sum += $2} END { printf(\"The average score is %.2f\\n\", sum/NR) }' scores.txt\n</code></pre> The average score is 77.56</p> <p>This command calculates the average score. In the action block, we calculate the sum of scores. In the END block, we print the average score. We format the output with the built-in printf function. The %.2f is a format specifier; each specifier begins with the % character. The .2 is the precision -- the number of digits after the decimal point. The f expects a floating point value. The \\n is not a part of the specifier; it is a newline character. It prints a newline after the string is shown on the terminal.</p>"},{"location":"lab_session/basics_awk/#awk-working-with-pipes","title":"AWK working with pipes","text":"<p>AWK can receive input and send output to other commands via the pipe.</p> <pre><code>echo -e \"1 2 3 5\\n2 2 3 8\" | awk '{print $(NF)}'\n5\n8\n</code></pre> <p>In this case, AWK receives output from the echo command. It prints the values of last column.</p> <pre><code>awk -F: '$7 ~ /bash/ {print $1}' /etc/passwd | wc -l\n3\n</code></pre> <p>Here, the AWK program sends data to the wc command via the pipe. In the AWK program, we find out those users who use bash. Their names are passed to the wc command which counts them. In our case, there are three users using bash.</p>"},{"location":"lab_session/basics_awk/#awk-for-bioinformatics-looking-at-transcriptome-data","title":"AWK for Bioinformatics - looking at transcriptome data","text":"<p>You can find a lot of online tutorials, but here we will try out a few steps which show how a bioinformatician analyses a GTF file using awk.</p> <p>GTF is a special file format that contains information about different regions of the genome and their associated annotations. More on that here - Ensembl File Formats-GTF.</p> <pre><code>wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/transcriptome.gtf -O transcriptome.gtf\n\nhead transcriptome.gtf | less -S\n\n##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73)\n##provider: GENCODE\n##contact: gencode@sanger.ac.uk\n##format: gtf\n##date: 2013-09-02\nchr1    HAVANA  exon    173753  173862  .   -   .   gene_id \"ENSG00000241860.2\"; transcript_id \"ENST00000466557.2\"; gene_type \"processed_transcript\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.13\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"RP11-34P13.13-001\"; exon_number 1;  exon_id \"ENSE00001947154.2\";  level 2; tag \"not_best_in_genome_evidence\"; havana_gene \"OTTHUMG00000002480.3\"; havana_transcript \"OTTHUMT00000007037.2\";\nchr1    HAVANA  transcript  1246986 1250550 .   -   .   gene_id \"ENSG00000127054.14\"; transcript_id \"ENST00000478641.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"CPSF3L\"; transcript_type \"retained_intron\"; transcript_status \"KNOWN\"; transcript_name \"CPSF3L-006\"; level 2; havana_gene \"OTTHUMG00000003330.11\"; havana_transcript \"OTTHUMT00000009365.1\";\nchr1    HAVANA  CDS 1461841 1461911 .   +   0   gene_id \"ENSG00000197785.9\"; transcript_id \"ENST00000378755.5\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"ATAD3A\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"ATAD3A-003\"; exon_number 13;  exon_id \"ENSE00001664426.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS31.1\"; havana_gene \"OTTHUMG00000000575.6\"; havana_transcript \"OTTHUMT00000001365.1\";\nchr1    HAVANA  exon    1693391 1693474 .   -   .   gene_id \"ENSG00000008130.11\"; transcript_id \"ENST00000341991.3\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"NADK\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"NADK-002\"; exon_number 3;  exon_id \"ENSE00003487616.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS30565.1\"; havana_gene \"OTTHUMG00000000942.5\"; havana_transcript \"OTTHUMT00000002768.1\";\nchr1    HAVANA  CDS 1688280 1688321 .   -   0   gene_id \"ENSG00000008130.11\"; transcript_id \"ENST00000497186.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"NADK\"; transcript_type \"nonsense_mediated_decay\"; transcript_status \"KNOWN\"; transcript_name \"NADK-008\"; exon_number 2;  exon_id \"ENSE00001856899.1\";  level 2; tag \"mRNA_start_NF\"; tag \"cds_start_NF\"; havana_gene \"OTTHUMG00000000942.5\"; havana_transcript \"OTTHUMT00000002774.3\";\n</code></pre> <p>The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. (text copied from the source)</p> <p>let's get only the lines that have <code>gene</code> in the 3th column.</p> <pre><code>$ awk -F \"\\t\" '$3 == \"gene\"' transcriptome.gtf | head | less -S\n\nchr1    HAVANA  gene    11869   14412   .   +   .   gene_id \"ENSG00000223972.4\"; transcript_id \"ENSG00000223972.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\nchr1    HAVANA  gene    14363   29806   .   -   .   gene_id \"ENSG00000227232.4\"; transcript_id \"ENSG00000227232.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"WASH7P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"WASH7P\"; level 2; havana_gene \"OTTHUMG00000000958.1\";\nchr1    HAVANA  gene    29554   31109   .   +   .   gene_id \"ENSG00000243485.2\"; transcript_id \"ENSG00000243485.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"MIR1302-11\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"MIR1302-11\"; level 2; havana_gene \"OTTHUMG00000000959.2\";\nchr1    HAVANA  gene    34554   36081   .   -   .   gene_id \"ENSG00000237613.2\"; transcript_id \"ENSG00000237613.2\"; gene_type \"lincRNA\"; gene_status \"KNOWN\"; gene_name \"FAM138A\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"FAM138A\"; level 2; havana_gene \"OTTHUMG00000000960.1\";\nchr1    HAVANA  gene    52473   54936   .   +   .   gene_id \"ENSG00000268020.2\"; transcript_id \"ENSG00000268020.2\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G4P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G4P\"; level 2; havana_gene \"OTTHUMG00000185779.1\";\nchr1    HAVANA  gene    62948   63887   .   +   .   gene_id \"ENSG00000240361.1\"; transcript_id \"ENSG00000240361.1\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G11P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G11P\"; level 2; havana_gene \"OTTHUMG00000001095.2\";\nchr1    HAVANA  gene    69091   70008   .   +   .   gene_id \"ENSG00000186092.4\"; transcript_id \"ENSG00000186092.4\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5\"; level 2; havana_gene \"OTTHUMG00000001094.1\";\nchr1    HAVANA  gene    89295   133566  .   -   .   gene_id \"ENSG00000238009.2\"; transcript_id \"ENSG00000238009.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.7\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.7\"; level 2; havana_gene \"OTTHUMG00000001096.2\";\nchr1    HAVANA  gene    89551   91105   .   -   .   gene_id \"ENSG00000239945.1\"; transcript_id \"ENSG00000239945.1\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.8\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.8\"; level 2; havana_gene \"OTTHUMG00000001097.2\";\nchr1    HAVANA  gene    131025  134836  .   +   .   gene_id \"ENSG00000233750.3\"; transcript_id \"ENSG00000233750.3\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"CICP27\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"CICP27\"; level 1; tag \"pseudo_consens\"; havana_gene \"OTTHUMG00000001257.3\";\n</code></pre> <p>Perhaps filter a bit more and print the content of the 9th column in the file.</p> <pre><code>$ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S\n\ngene_id \"ENSG00000223972.4\"; transcript_id \"ENSG00000223972.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"DDX11L1\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"DDX11L1\"; level 2; havana_gene \"OTTHUMG00000000961.2\";\ngene_id \"ENSG00000227232.4\"; transcript_id \"ENSG00000227232.4\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"WASH7P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"WASH7P\"; level 2; havana_gene \"OTTHUMG00000000958.1\";\ngene_id \"ENSG00000243485.2\"; transcript_id \"ENSG00000243485.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"MIR1302-11\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"MIR1302-11\"; level 2; havana_gene \"OTTHUMG00000000959.2\";\ngene_id \"ENSG00000237613.2\"; transcript_id \"ENSG00000237613.2\"; gene_type \"lincRNA\"; gene_status \"KNOWN\"; gene_name \"FAM138A\"; transcript_type \"lincRNA\"; transcript_status \"KNOWN\"; transcript_name \"FAM138A\"; level 2; havana_gene \"OTTHUMG00000000960.1\";\ngene_id \"ENSG00000268020.2\"; transcript_id \"ENSG00000268020.2\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G4P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G4P\"; level 2; havana_gene \"OTTHUMG00000185779.1\";\ngene_id \"ENSG00000240361.1\"; transcript_id \"ENSG00000240361.1\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"OR4G11P\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"OR4G11P\"; level 2; havana_gene \"OTTHUMG00000001095.2\";\ngene_id \"ENSG00000186092.4\"; transcript_id \"ENSG00000186092.4\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5\"; level 2; havana_gene \"OTTHUMG00000001094.1\";\ngene_id \"ENSG00000238009.2\"; transcript_id \"ENSG00000238009.2\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.7\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.7\"; level 2; havana_gene \"OTTHUMG00000001096.2\";\ngene_id \"ENSG00000239945.1\"; transcript_id \"ENSG00000239945.1\"; gene_type \"lincRNA\"; gene_status \"NOVEL\"; gene_name \"RP11-34P13.8\"; transcript_type \"lincRNA\"; transcript_status \"NOVEL\"; transcript_name \"RP11-34P13.8\"; level 2; havana_gene \"OTTHUMG00000001097.2\";\ngene_id \"ENSG00000233750.3\"; transcript_id \"ENSG00000233750.3\"; gene_type \"pseudogene\"; gene_status \"KNOWN\"; gene_name \"CICP27\"; transcript_type \"pseudogene\"; transcript_status \"KNOWN\"; transcript_name \"CICP27\"; level 1; tag \"pseudo_consens\"; havana_gene \"OTTHUMG00000001257.3\";\n</code></pre> <p>What about if we want just a specific piece from this information? We can <code>|</code> the output from the first awk script in to a second one. Note that we will use different field separator <code>\"; \"</code>.</p> <pre><code>$ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head\n\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"protein_coding\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\n</code></pre>"},{"location":"lab_session/basics_awk/#chaining-awk-calls","title":"Chaining AWK calls","text":"<p>We will start with the AWK call that we were using before, and we will append a pipe | so it can be used as input for the next AWK call, this time using a space and a semicolon as the delimiter to define what a column is:</p> <p><pre><code>awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head | less -S\n\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\ngene_type \"pseudogene\"\ngene_type \"protein_coding\"\ngene_type \"lincRNA\"\ngene_type \"lincRNA\"\ngene_type \"pseudogene\"\n</code></pre> Now that we see what the third column looks like, we can filter for protein-coding genes</p> <p><pre><code>awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\\nawk -F \"; \" '$3 == \"gene_type \\\"protein_coding\\\"\"' | \\\nhead | less -S\n\ngene_id \"ENSG00000186092.4\"; transcript_id \"ENSG00000186092.4\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5\"; level 2; havana_gene \"OTTHUMG00000001094.1\";\ngene_id \"ENSG00000237683.5\"; transcript_id \"ENSG00000237683.5\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"AL627309.1\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"AL627309.1\"; level 3;\ngene_id \"ENSG00000235249.1\"; transcript_id \"ENSG00000235249.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F29\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F29\"; level 2; havana_gene \"OTTHUMG00000002860.1\";\ngene_id \"ENSG00000185097.2\"; transcript_id \"ENSG00000185097.2\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F16\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F16\"; level 2; havana_gene \"OTTHUMG00000002581.1\";\ngene_id \"ENSG00000269831.1\"; transcript_id \"ENSG00000269831.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL669831.1\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL669831.1\"; level 3;\ngene_id \"ENSG00000269308.1\"; transcript_id \"ENSG00000269308.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL645608.2\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL645608.2\"; level 3;\ngene_id \"ENSG00000187634.6\"; transcript_id \"ENSG00000187634.6\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"SAMD11\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"SAMD11\"; level 2; havana_gene \"OTTHUMG00000040719.8\";\ngene_id \"ENSG00000268179.1\"; transcript_id \"ENSG00000268179.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL645608.1\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL645608.1\"; level 3;\ngene_id \"ENSG00000188976.6\"; transcript_id \"ENSG00000188976.6\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"NOC2L\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"NOC2L\"; level 2; havana_gene \"OTTHUMG00000040720.1\";\ngene_id \"ENSG00000187961.9\"; transcript_id \"ENSG00000187961.9\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"KLHL17\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"KLHL17\"; level 2; havana_gene \"OTTHUMG00000040721.6\";\n</code></pre> I added a space and a backslash  (not to be confused with the regular slash /) after the first and second pipes to split the code into two lines; this makes it easier to read and it highlights that we are taking two separate steps.</p> <p>The double quotes around protein_coding are escaped (also with a backslash \\\") because they are already contained inside double quotes. To avoid the backslashing drama we can use the partial matching operator ~ instead of the total equality operator ==.</p> <p><pre><code>awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\\nawk -F \"; \" '$3 ~ \"protein_coding\"' | \\\nhead | less -S\n</code></pre> The output is the same as before: those lines that contain a protein_coding somewhere in their third column make the partial matching rule true, and they get printed (which is the default behavior when there are no curly braces).</p> <p>Now we have all the protein-coding genes, but how do we get to the genes that only have one exon? Well, we have to revisit our initial AWK call: we selected lines that corresponded to genes, but we actually want lines that correspond to exons. That\u2019s an easy fix, we just change the word \u201cgene\u201d for the word \u201cexon\u201d. Everything else stays the same.</p> <pre><code>awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\\nawk -F \"; \" '$3 ~ \"protein_coding\"' | \\\nhead | less -S\n\ngene_id \"ENSG00000186092.4\"; transcript_id \"ENST00000335137.3\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F5\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F5-001\"; exon_number 1;  exon_id \"ENSE00002319515.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS30547.1\"; havana_gene \"OTTHUMG00000001094.1\"; havana_transcript \"OTTHUMT00000003223.1\";\ngene_id \"ENSG00000237683.5\"; transcript_id \"ENST00000423372.3\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"AL627309.1\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"AL627309.1-201\"; exon_number 1;  exon_id \"ENSE00002221580.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000237683.5\"; transcript_id \"ENST00000423372.3\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"AL627309.1\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"AL627309.1-201\"; exon_number 2;  exon_id \"ENSE00002314092.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000235249.1\"; transcript_id \"ENST00000426406.1\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F29\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F29-001\"; exon_number 1;  exon_id \"ENSE00002316283.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS41220.1\"; havana_gene \"OTTHUMG00000002860.1\"; havana_transcript \"OTTHUMT00000007999.1\";\ngene_id \"ENSG00000185097.2\"; transcript_id \"ENST00000332831.2\"; gene_type \"protein_coding\"; gene_status \"KNOWN\"; gene_name \"OR4F16\"; transcript_type \"protein_coding\"; transcript_status \"KNOWN\"; transcript_name \"OR4F16-001\"; exon_number 1;  exon_id \"ENSE00002324228.1\";  level 2; tag \"basic\"; tag \"CCDS\"; ccdsid \"CCDS41221.1\"; havana_gene \"OTTHUMG00000002581.1\"; havana_transcript \"OTTHUMT00000007334.1\";\ngene_id \"ENSG00000269831.1\"; transcript_id \"ENST00000599533.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL669831.1\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL669831.1-201\"; exon_number 1;  exon_id \"ENSE00003063549.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000269831.1\"; transcript_id \"ENST00000599533.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL669831.1\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL669831.1-201\"; exon_number 2;  exon_id \"ENSE00003084653.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000269831.1\"; transcript_id \"ENST00000599533.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL669831.1\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL669831.1-201\"; exon_number 3;  exon_id \"ENSE00003138540.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000269308.1\"; transcript_id \"ENST00000594233.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL645608.2\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL645608.2-201\"; exon_number 1;  exon_id \"ENSE00003079649.1\";  level 3; tag \"basic\";\ngene_id \"ENSG00000269308.1\"; transcript_id \"ENST00000594233.1\"; gene_type \"protein_coding\"; gene_status \"NOVEL\"; gene_name \"AL645608.2\"; transcript_type \"protein_coding\"; transcript_status \"NOVEL\"; transcript_name \"AL645608.2-201\"; exon_number 2;  exon_id \"ENSE00003048391.1\";  level 3; tag \"basic\";\n</code></pre>"},{"location":"lab_session/basics_awk/#cleaning-up-the-output","title":"Cleaning up the output","text":"<p>Before we try to count how many exons belong to the same protein-coding gene, let\u2019s simplify the output so we only get the gene names (which are in column 5).</p> <p><pre><code>awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\\nawk -F \"; \" '$3 ~ \"protein_coding\" {print $5}' | \\\nhead\n\ngene_name \"OR4F5\"\ngene_name \"AL627309.1\"\ngene_name \"AL627309.1\"\ngene_name \"OR4F29\"\ngene_name \"OR4F16\"\ngene_name \"AL669831.1\"\ngene_name \"AL669831.1\"\ngene_name \"AL669831.1\"\ngene_name \"AL645608.2\"\ngene_name \"AL645608.2\"\n</code></pre> This is sort of what we want. We could chain another AWK call using -F \" \", and pick the second column (which would get rid of the gene_name). Feel free to try that approach if you are curious.</p> <p>We can also take a shortcut by using the tr -d command, which deletes whatever characters appear in double quotes. For example, to remove every vowel from a sentence: <pre><code>echo \"This unix thing is cool\" | tr -d \"aeiou\" # Ths nx thng s cl\n</code></pre> Let\u2019s try deleting all the semicolons and quotes before the second AWK call:</p> <p><pre><code>awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\\ntr -d \";\\\"\" | \\\nawk -F \" \" '$6 == \"protein_coding\" {print $10}' | \\\nhead\n\nOR4F5\nAL627309.1\nAL627309.1\nOR4F29\nOR4F16\nAL669831.1\nAL669831.1\nAL669831.1\nAL645608.2\nAL645608.2\n</code></pre> Run <code>bash awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | tr -d \";\\\"\" | head</code> to understand what the input to the second AWK call looks like. It\u2019s just words separated by spaces; the sixth word corresponds to the gene type, and the tenth word to the gene name.</p>"},{"location":"lab_session/basics_awk/#counting-genes","title":"Counting genes","text":"<p>There is one more concept we need to introduce before we start counting. AWK uses a special rule called END, which is only true once the input is over. See an example:</p> <p><pre><code>echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\\nawk '\n    { print $1 }\n\nEND { print \"Done with letters!\" }\n'\n\na\na\nb\nb\nb\nc\n</code></pre> Done with letters! The -e option tells echo to convert each \\n into a new line, which is a convenient way of printing multiple lines from a single character string.</p> <p>In AWK, any amount of whitespace is allowed between the initial and the final quote '. I separated the first rule from the END rule to make them easier to read.</p> <p>Now we are ready for counting.</p> <p><pre><code>echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\\nawk '\n    { counter[$1] += 1 }\n\nEND {\n    for (letter in counter){\n        print letter, counter[letter]\n    }\n}\n'\n\na 2\nb 3\nc 1\n</code></pre> Wow, what is all that madness?</p> <p>Instead of printing each letter, we manipulate a variable that we called counter. This variable is special because it is followed by brackets [ ], which makes it an associative array, a fancy way of calling a variable that stores key-value pairs.</p> <p>In this case we chose the values of the first column \\(1 to be the keys of the counter variable, which means there are 3 keys (\u201ca\u201d, \u201cb\u201d and \u201cc\u201d). The values are initialized to 0. For every line in the input, we add a 1 to the value in the array whose key is equal to \\(1. We use the addition operator +=, a shortcut for counter[\\)1] = counter[\\)1] + 1.</p> <p>When all the lines are read, the END rule becomes true, and the code between the curly braces { } is executed. The structure for (key in associate_array) { some_code } is called a for loop, and it executes some_code as many times as there are keys in the array. letter is the name that we chose for the variable that cycles through all the keys in counter, and counter[letter] gives the value stored in counter for each letter (which we we calculated in the previous curly brace chunk).</p> <p>Now we can apply this to the real example:</p> <p><pre><code>awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\\ntr -d \";\\\"\" | \\\nawk -F \" \" '\n$6 == \"protein_coding\" {\n    gene_counter[$10] += 1\n}\n\nEND {\n    for (gene_name in gene_counter){\n        print gene_name, gene_counter[gene_name]\n    }\n}' &gt; number_of_exons_by_gene.txt\n\nhead number_of_exons_by_gene.txt\nCAPN6 24\nARL14EPL 3\nDACH1 38\nIFNA13 1\nHSP90AB1 36\nCAPN7 52\nDACH2 84\nIFNA14 1\nLARS 188\nCAPN8 78\n</code></pre> If you are using the real transcriptome, it takes less than a minute to count up one million exons. Pretty impressive.</p> <p>We saved the output to a file, so now we can use AWK to see how many genes are made up of a single exon.</p> <pre><code>awk '$2 == 1' number_of_exons_by_gene.txt | wc -l # 1362\n</code></pre> <p>I will suggest to follow the original tutorial if you need to refer to these steps later on for your own data: AWK GTF! How to Analyze a Transcriptome Like a Pro</p>"},{"location":"lab_session/genomic_alteration_interpretation_exe/","title":"GENOMIC ALTERATIONS INTERPRETATION -- EXERCISES","text":"<p>By David Tamborero.</p>"},{"location":"lab_session/genomic_alteration_interpretation_exe/#1-exercise-with-genomic-variants-knowledgebases-clinvar","title":"(1) Exercise with genomic variants knowledgebases: ClinVar","text":"<p>There are several international projects devoted to gather knowledge associated to gene variants according to the results that are continuously generated by clinical and preclinical studies. These efforts are consolidated in different databases (aka knowledgebases). Each knowledgebase may have a different scope and a distinct process to gather, annotate and access the data. Understanding these details is important for an accurate use of the resources.</p> <p>Some of these knowledgebases are more focused on germline variants and their relevance to cause (or predispose) to diseases (including cancer, i.e. variants that increase hereditary risk of cancer). One of these databases is ClinVar, a project under the umbrella of the (American) National Center for Biotechnology.</p> <p>The ClinVar website allow to query information associated with a particular variant* of your interest by using different formats, including variant identifiers that are provided by third-party associations (e.g. dbSNP identifiers). The more common method to query for a variant is by introducing the exact genomic coordinates. ClinVar classifies variants by their pathogenicity, in five levels (pathogenic, likely pathogenic \u2013 benign, likely benign \u2013 variant of unknown significance) following the recommendations of the American College of Medical Genetics and Genomics and the Association for Molecular Pathology. These guidelines include criteria that are specific to the germline context, such as the assessment of the variant inheritance model in families of the individual presenting the phenotype of interest.</p> <ul> <li>the search box also allows to look for all the genomic variants in a given gene or for a given disease.</li> </ul> <p>Exercise: search for the gene CFTR, which encodes a protein that functions as a channel across the cell membrane that produce mucus, sweat, saliva, tears, and digestive enzymes. Germline pathogenic variants in this gene are demonstrated to lead to cytstic fibrosis. As a result of querying this gene in the ClinVar database, you will see a new page with a table listing all the variants observed this gene and having phenotypes annotated in the resource.</p> <p>Select only those variants that have been classified as \u2018Pathogenic\u2019 by using the corresponding filter in the \u2018Clinical significance\u2019 section located at the left side of the page (see image below).</p> <p></p> <p>Importantly, ClinVar acts as a repository of assertions, but there is no in-house team devoted to manually review these assertions (as opposite to other databases in which \u2018editors\u2019 review and \u2018approve\u2019 the content before releasing it). Instead, ClinVar provides a score (from zero to four stars) that can be used as a proxy on the \u2018quality consensus\u2019 of the entry. For instance, one star is automatically given by the system when several curators have submitted an assertion for the same variant with contradictory interpretations. Two stars are given if the interpretation of the variant coincides across the curators that have submitted information for that variant. And three stars means that the assertion has been submitted by a certified committee of experts, and thus is considered a consensus expert recommended interpretation. Maximum score (four stars) is given for variant effects that are currently used in the standard clinical practice (which can be only submitted by the corresponding authorities in the ClinVar repo).</p> <p>In the CFTR results page provided by ClinVar, select now the filter \u2018Practice guidelines\u2019 in the \u2018Review status\u2019 section located at the left side of the page. This should limit the number of displayed variants to ~20-25. Note in the \u2018Condition(s)\u2019 column of he results table that most of these variants are indeed described as pathogenic for Cystic Fibrosis. If you click in any of them, you will open a new page with the details of this variant and associated information (see image pasted below). Note also how this ClinVar page includes links to other resources with more information of the variant. The information provided by ClinVar includes (you need to scroll down in the page) who has submitted the variant assertion and the published references supporting the variant effect.</p> <p></p> <p>ClinVar is a repository widely employed to look for information of variants associated with Mendelian diseases. It also contains information of germline variants associated to cancer, e.g. cancer predisposing variants. You can repeat the search for BRCA1 or BRCA2 (genes whose pathogenic germline variants predispose to ovarian and breast cancer), or MLH1, MSH2 or MSH6 (genes whose germline pathogenic variants predispose to colorectal cancer), as examples of the latter.</p> <p>(NOTE about the page interface: be aware that when you search for a new gene in ClinVar, the results are likely going to be filtered by the last applied filter conditions (so it s not a new search from scratch)-- which can be a bit unnoticed as the active left bar options are not that obvious to notice).</p> <p>Please note that this example is focused on querying variants annotated in the ClinVar database for a given gene, but it is also possible to query for information (if any) of a specific variant of your interest. For that, you will need to use standarized variant nomenclature, as HGVS (e.g. NM_000314.4:c.395G&gt;T); for more details, visit this link</p>"},{"location":"lab_session/genomic_alteration_interpretation_exe/#2-exercise-with-genomic-variants-knowledgebases-oncokb","title":"(2) Exercise with genomic variants knowledgebases: OncoKB","text":"<p>OncoKB is a knowledgebase curated by Memorial Sloan Kettering members and focused on cancer mutations (https://www.oncokb.org/). This resource provides classification of the oncogenic effects of variants in cancer genes and their value (if any) as biomarkers of prognosis, diagnosis and drug response. This database is curated manually and thus all the entries have been approved by their Scientific Content Management Team. Therefore, OncoKB does not provide a measure of the quality of the assertions, as all the entries passed their quality criteria. This can not confused with the level of evidence supporting the described effect; for instance, one biomarker can be supported by clinical studies or by pre-clinical data, which represents two distinct levels of clinical evidence; but in both cases, the quality of the supporting data is sound (according to their criteria). In detail, the classification that OncoKB follows to classify the actionability of the biomarkers can be found here: https://www.oncokb.org/levels</p> <p>The searches can be done at the level of a particular variant, or you can query for all the variants with associated information available for a given gene (see image below).</p> <p></p> <p>You can look for your gene of interest, e.g. FGFR3; you will see the assertions divided in annotated alterations, therapeutic, prognostic and FDA approved (in this case, there is no \u201cdiagnostic\u201d entry associated to this gene).</p> <p></p> <p>Annotated alterations summarizes the variants in this gene with known effect, classified as oncogenic/likely oncogenic or Likely neutral. Note that for oncogenic variant, the annotation includes the particular effect (gain-of-function in this case, as FGFR3 is an oncogene). Therapeutic entries are the subset of oncogenic variants that are also known to be biomarkers of drug response, following the aforementioned classification rank. Note that a biomarker must include the cancer type(s) and the drug(s) in which that effect has been described. Note that the FDA-recognized content is a recent (October 2021), incorporation in oncoKB, in which the entries labelled as so are recognized as FDA approved information. Click in an entry to see more details, including the supporting literature employed to curate the information.</p> <p></p>"},{"location":"lab_session/genomic_alteration_interpretation_exe/#3-use-of-a-lightweight-clinical-decision-support-system","title":"(3) Use of a (lightweight) clinical decision support system","text":"<p>Molecular profiles of tumors can inform treatment decisions; most of the therapeutic interventions are currently guided by genomic profiling (specially mutations) and -to a lesser extent- expression signatures. Performance of NGS assays is common in the oncology setting, both in investigational as well as routine care. The variants observed by the assay must be then interpreted to first identify those that are functionally relevant for the tumor individual, and then which of them are also clinically relevant according to current evidence. To do this process manually is time consuming and prone to inaccuracies or even errors. Clinical decision support systems (CDSS) implement efficient processes to annotate and classify the variants using a variety of computational tools and databases. As the complexity of molecular-guided therapies continues to grow, the use of CDSS is increasingly important.</p> <p>There are several commercial CDSS available at present, but larger academic centers also develop their own in-house solutions, which may better adapt to their specific needs and investigational initiatives. One of this academic solutions, the Molecular Tumor Board Portal (MTBP), released an open version available for the community (http://mtbp.org) This public portal is a lightweight* version of the \u2018production\u2019 instances used in real clinical project. Among others, the public portal provides a limited analysis of the data (accepts only mutations, with no information about mutation signatures, and do not incorporate information about origin/clonality of the mutations neither generate clinical flags such as allocation to clinical trials which requires additional clinical/pathology information). This public resource is limited to only provide a generic functional and predictive interpretation of a list of variants uploaded by the user for research purposes only.</p> <p>note the disclaimer in the website: as the public portal is considered a research tool and must be used in that context!</p> <p>Open the MTBP public version website http://mtbp.org; remind that this is a free tool, and you do not even need to login to be able to make an analysis (however, if you want to keep the results of your analysis in a repository, you need to register with a valid email \u2013 so the system can associate the</p> <p>repository to that account). With or without being logged in, you can click the \u2018See examples\u2019 or the \u2018Analyse your variants\u2019 button (see next image):</p> <p></p> <p>\u2018See examples\u2019, as the name indicates, will show the reports that are obtained by analysing some variants observed in three different tumor types used as examples here. If you click \u2018Analyse your varaints\u2019, you will be able to perform an analysis for your own variants of interest. This will open a new window; you will need to introduce a name to identify your analysis in the system (step 1), select the tumor type of the sequenced sample (step 2), and then upload the variants (step 3).</p> <p>Note that there is a box in step 3 to include an OncoKB token \u2013 this is an optional step, which allows the system to query more comprehensively that database if the user is registered to that resource (and thus obtained a token; see the question mark for more details). Keep this field empty if you do not have that token. In addition, the variants can be passed by either uploading a VCF file or by typing the variants in a free text box. The VCF file must follow standards (including the use of the \u2018PASS\u2019 label in the filter column to indicate the variants that are to be included in the analysis) that can be annotated in hg19/GRCh37 or hg38/GRCh38. Variants typed in the free text box must follow HGVS standarized syntax. Please use the question marks to open popups with more information.</p> <p>When ready, click the \u2018Run\u2019 button:</p> <p></p> <p>If everything goes well (including the validation of the variants\u2019 format), the information will be sent to the system and processed during a while; the results of the analysis will be available in the system when finished. Note that the resulting report contains: - some descriptives of the gene and the variant (such as the protein coordinates and the consequence type) - the evidence (if any) supporting the functional relevance classification - the evidence (if any) supporting the role of the variant as cancer biomarker</p> <p>Note that the report is interactive, and you can access to furtehr details, including the original sources of evidence, by clicking around in the document. Remember also that the evidence supporting the variant\u2019s functional relevance is classified in three families:</p> <ul> <li>evidence of type A: the effect is supported by manually curated data (i.e. results of clinical or experimental studies)</li> <li>evidence of type B: the effect can be assumed by using well sedimented biological knowledge (i.e. to assume that a premature stop codon before the last exon-exon junction will trigger nonsense-mediated decay)</li> <li>evidence of type C: the effect is based on a bioinformatics prediction, which is the lowest level of evidence.</li> </ul> <p>Also, the evidence supporting the biomarker is tiered by several factors, including the match of the patient and biomarker cancer type and the clinical status of the biomarker effect (e.g. it is accepted in clinical guidelines versus is supported by preliminary results of clinical trials) following ESMO recommendations.</p> <p></p> <p>Use the VCF files distributed for the course (or use variants of your own interest) to test the system and discuss the results. Again, remember that the open MTBP is a lightweight version of the system, and thus the analyses are limited to the generic interpretation provided here.</p> <p>VCFs can be downloaded here:</p> <p>BREAST_sample_muts.hg19.vcf</p> <p>LUAD_sample_muts.hg19.vcf</p>"},{"location":"lab_session/ggplot2/","title":"ggplot2","text":"<p>ggplot2 is an R package for making visualizations and is based on the \u2018Grammar of Graphics\u2019. It is often referred as one of the best packages for visualizations. </p> <p>Actually, there are other tools for making graphs in R such as Lattice and base R graphics. However, ggplot2 is straightforwardly easy to make any kind of graphs even complex ones. </p> <p>It is easy to add complexity or take it away which makes ggplot2 superior among these three. Since you can\u2019t go back and forth once you created a plot in base R or in lattice. That\u2019s enough for comparison , let\u2019s start learning it now.</p> <p>Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer.</p>"},{"location":"lab_session/ggplot2/#grammar-of-graphics","title":"Grammar of Graphics","text":"<p>Each language has a grammar and this grammar dictates the structure of sentences and the position of each element in a given sentence. For example in English grammar, we could take the sentence \"The little monkey hangs confidently by a branch\"</p> <p></p> <p>Notice the need for these elements (words) in specific positions to make a meaningful sentence. This is governed by the grammar. Just as the grammar of language helps us construct meaningful sentences out of words, the Grammar of Graphics helps us to construct graphical figures out of different visual elements. This grammar gives us a way to talk about parts of a plot: all the circles, lines, arrows, and words that are combined into a diagram for visualizing data. </p> <p>Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham to describe the components of a plot, including</p> <ul> <li>the data being plotted</li> <li>the geometric objects (circles, lines, etc.) that appear on the plot</li> <li>a set of mappings from variables in the data to the aesthetics (appearance) of the geometric objects</li> <li>a statistical transformation used to calculate the data values used in the plot</li> <li>a position adjustment for locating each geometric object on the plot</li> <li>a scale (e.g., range of values) for each aesthetic mapping used</li> <li>a coordinate system used to organize the geometric objects</li> <li>the facets or groups of data shown in different plots</li> </ul> <p>Similar to how tidyr and dplyr provide efficient data transformation and manipulation, ggplot2 provides more efficient ways to create specific visual images.</p>"},{"location":"lab_session/ggplot2/#working-with-ggplot2-is-like-playing-with-lego-bricks-and-tetris","title":"Working with ggplot2 is like playing with LEGO bricks and Tetris!","text":"<p>Imagine you have a big pile of LEGO bricks and you want to build a castle. You start by selecting the bricks you want to use and arranging them in a certain way to create the foundation of the castle. This is similar to how you start building a plot in ggplot2 by selecting your data and specifying the basic plot components.</p> <p>Next, you begin to add layers to your LEGO castle, such as towers, walls, and a drawbridge. Each layer adds a different aspect to the castle and helps to make it more complete and visually interesting. In ggplot2, you can add layers to your plot to display different elements of your data, such as points, lines, and labels. Each layer can be customized separately to create a unique and informative plot.</p> <p>Now imagine you are playing a game of Tetris. Each piece that falls from the top of the screen is like a layer in ggplot2. You have to rotate and position each piece carefully to create a complete row and prevent gaps. Similarly, in ggplot2, you have to add and position each layer carefully to create a complete and informative plot.</p> <p>Just like building a LEGO castle or playing Tetris, creating a plot in ggplot2 requires careful selection and arrangement of the different components to create a complete and visually appealing result. </p> <p>So with that inspiration, we can summarise that ggplot objects can be highly complex. The basic order of layers will usually look like this:</p> <ol> <li>Begin with the baseline ggplot() command - this \u201copens\u201d the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command</li> <li>Add \u201cgeom\u201d layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix.</li> <li>Add design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation.</li> </ol> <p>Read about all the building blocks available to you from the ggplot2 Function reference documentation -- ggplot2 function reference. </p> <p>A simple example of skeleton code is as follows. We will explain each component in the sections below.</p> <pre><code># plot data from my_data columns as red points\nggplot(data = my_data)+                   # use the dataset \"my_data\"\ngeom_point(                             # add a layer of points (dots)\nmapping = aes(x = col1, y = col2),    # \"map\" data column to axes\ncolor = \"red\")+                       # other specification for the geom\nlabs()+                                 # here you add titles, axes labels, etc.\ntheme()                                 # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) \n</code></pre>"},{"location":"lab_session/ggplot2/#starting-up-with-ggplot","title":"Starting up with ggplot","text":"<p>Firstly , we need to install ggplot2 package <pre><code>install.packages(\"ggplot2\")\nlibrary(\"ggplot2\")\n</code></pre></p>"},{"location":"lab_session/ggplot2/#the-basics-of-plotting-with-ggplot","title":"The Basics of plotting with ggplot","text":"<p>In order to create a plot, you:</p> <ol> <li>Call the ggplot() function which creates a blank canvas</li> <li>Specify aesthetic mappings, which specifies how you want to map variables to visual aspects. In this case we are simply mapping the displ and hwy variables to the x- and y-axes.</li> <li>You then add new layers that are geometric objects which will show up on the plot. In this case we add geom_point to add a layer with points (dot) elements as the geometric shapes to represent the data.</li> </ol> <p><pre><code># create canvas\nggplot(data)\n\n# variables of interest mapped\nggplot(data, aes(x = displ, y = hwy))\n\n# data plotted\nggplot(data, aes(x = displ, y = hwy)) +\ngeom_point()\n</code></pre> </p>"},{"location":"lab_session/ggplot2/#ggplot2-hands-on-exercise","title":"ggplot2 hands-on exercise","text":"<p>WE WILL DO THIS ON YOUR LAPTOP!</p> <p>For our hands-on exercise, we will use a simple, easy to understand step-by-step tutorial from Library Carpentry project. (Library Carpentry is a global community teaching software and data skills to people working in library- and information-related roles. Read more about the project here).</p> <p>In the exercise, we will gradually build up on plotting data using ggplot2 in R.</p> <p>Before We Start lesson set up your directories and data</p> <ol> <li>Under the File menu, click on New project, choose New directory, then New project</li> <li>Enter the name ki_cg_course for this new folder (or \u201cdirectory\u201d). This will be your working directory for the rest of the day.</li> <li>Click on Create project</li> <li>Create a new file where we will type our scripts. Go to File &gt; New File &gt; R script. Click the save icon on your toolbar and save your script as \u201cscript.R\u201d.</li> </ol> <p>Copy and paste the below lines of code to create three new subdirectories and download the original and the reformatted books data:</p> <pre><code>install.packages(\"fs\")\nlibrary(fs)   # https://fs.r-lib.org/.  \n# fs is a cross-platform, uniform interface to file system operations via R. \n\n# let's create data directories\ndir_create(\"data\")\ndir_create(\"data_output\")\ndir_create(\"fig_output\")\n\n# now lets download the files we will be using in our exercise\n\ndownload.file(\"https://ndownloader.figshare.com/files/22031487\",\n\"data/books.csv\", mode = \"wb\")\n\ndownload.file(\"https://ndownloader.figshare.com/files/22051506\",\n\"data_output/books_reformatted.csv\", mode = \"wb\")\n</code></pre>"},{"location":"lab_session/ggplot2/#click-here-data-visualisation-with-ggplot2","title":"CLICK HERE  ---&gt; Data Visualisation with ggplot2","text":"<p>(clicking on the link above will take you to the Data Visualisation with ggplot2 chapter from Introduction to R series in Library Carpentry's website, please feel free to bookmark the page for later reference.)</p>"},{"location":"lab_session/ggplot2/#extra-tips","title":"Extra tips","text":"<p>If you have completed the above tutorial and would like to explore more on using ggplot in epidemiological context, please feel free to use the EpiRHandbook resources. </p> <p>To try ggplot hands-on, you could follow the original ggplot basics chapter from The Epidemiologist R Handbook (Batra, Neale, et al. The Epidemiologist R Handbook. 2021).</p>"},{"location":"lab_session/human_genome/","title":"Human genome reference files","text":"<p>The human genome is work in progress. Different versions exist (assemblies) and is often a source of confusion in genomics. Quick use of published data is a bit more challenging if a differnt genome build was applied than what is used in your lab. </p> <p>Also, there may even be differences in the latest versions, e.g. the GRCh37 and the hg19 assemblies from NCBI and USCS, respectively, had different mitochondrial genome. </p> <p>In genomics, the reference genome offers a scaffold upon which new data can be mapped, which is a much more efficient way rather than building a genome from scratch.</p> <p>Avery nice overview of the human genome versions can be found here.</p> <p>During this course version GRCh38, from the Genome Reference Consortium, with modifications fromt he 1000 genomes consortium will be used. It includes extra decoy and HLA sequences. The files are already available on AWS but can be downloaded from here.</p> <p>As several of the analysis steps wold take many hours we will use a smaller version of the reference genome which contains chr6 and chr17. These were choosen because the datat that will be analysed contain interesting varaiants on these chromosomes.  - Connect to your AWS instance</p> <pre><code># Remember, source the settings, paths and variables file\nsource .bashrc\n\n# Check the CHRS variable.\necho $CHRS\n\n# This variable should return \"chr6_and_chr17\", if not, reach of for assistance.\n\n# Go to the directory for the reference genome files\ncd ~/workspace/inputs/references/genome\n\n# List the files\nls -halt\n\n# Check what the reference genome looks like. When you have the reference genome open just type \"1000\" and press enter. You will then jump 1000 lines.\nless -SN ref_genome.fa\n\n#less -SN will be used during the course. Check the man page. Give it some time to understand how it it structured, it is very helpful to be able to read the help-pages fast when working on the command line.\nman less\n\n# Check the chromosome headers in the fasta file.\ncat ref_genome.fa | grep -P \"^&gt;\"\n\n# View the first 10 lines of this file. Note the header line starting with `&gt;`. \n# Why does the sequence look like this?\ncd ~/workspace/inputs/references/genome\nhead -n 10 ref_genome.fa\n\n# How many lines and characters are in this file?\nwc ref_genome.fa\n\n# How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome.\ngrep -v \"&gt;\" ref_genome.fa | wc\n\n# How long does that command take to run?\ntime grep -v \"&gt;\" ref_genome.fa | wc\n\n# View 10 lines from the end of the filefile\ntail -n 10 ref_genome.fa\n\n# What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)?\n# Runtime: ~30s\ncat ref_genome.fa | grep -v \"&gt;\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}'\n# What does each of these bases refer to? What are the \"unexpected bases\"?\n# Google \"IUPAC nucleic acid codes\"\n</code></pre>"},{"location":"lab_session/human_genome/#split-the-long-fasta-by-chromosome","title":"Split the long fasta by chromosome","text":"<pre><code>cd ~/workspace/inputs/references/genome\n\n# Split.\nfaSplit byname ref_genome.fa ./ref_genome_split/\n\n# View contents.\ntree\n</code></pre>"},{"location":"lab_session/human_genome/#index-the-fasta-files","title":"Index the fasta files","text":"<p>Indexing is widely used in bioinformatics workflows to improve performance. Typically it is applied to large data files with many records to improve the ability of tools to rapidly access specific locations of the file. For example, if we have alignments against the entire genome and we want to visualize those alignments for a single gene on chr17 in a genome viewer such as IGV, we don\u2019t want the viewer to have to scan through the entire file. Indexing allows us to jump right to the correct place in the file and pull out just the information we need without reading much of the file.</p> <pre><code># first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though!\ncd ~/workspace/inputs/references/genome\nls -halt\nrm -f ref_genome.fa.fai ref_genome.dict\n\n# Use samtools to create a fasta index file.\nsamtools faidx ref_genome.fa\n\n# View the contents of the index file.\nhead ref_genome.fa.fai\n# What information does the index fail store?\n# google \"fasta index fai format\" or something similar\n\n# Use picard to create a dictionary file.\njava -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict\n\n# View the content of the dictionary file.\nhead ref_genome.dict\n# less can also be applied\n\n#Also index the split chromosomes.\nsamtools faidx ./ref_genome_split/chr6.fa\nsamtools faidx ./ref_genome_split/chr17.fa\n\n# Create reference index for the genome to use BWA\nbwa index ref_genome.fa\n</code></pre>"},{"location":"lab_session/human_genome/#human-genome-transcriptome-reference-files","title":"Human genome transcriptome reference files","text":"<pre><code># Make sure CHRS environment variable is set.\necho $CHRS\ncd ~/workspace/inputs/references/transcriptome\n\n# List files in the directory\nls -halt\n\n# Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. \n# When the file is displayed usin \"less\" type \"/\" and then a something you want to highlight e.g. \"chr6\". This is very useful when searching for specific infomration\nless -SN ref_transcriptome.gtf\n</code></pre>"},{"location":"lab_session/human_genome/#explore-the-contents-of-the-transcriptome-reference-files","title":"Explore the contents of the transcriptome reference files","text":"<pre><code>#How many chromsomes are represented?\ncut -f1 ref_transcriptome.gtf | sort | uniq -c\n\n# How many unique gene IDs are in the .gtf file?\n# We can use a perl command-line command to find out:\nperl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l\n\n# what are all the feature types listed in the third column of the GTF?\n# how does the following command (3 commands piped together) answer that question?\ncut -f 3 ref_transcriptome.gtf | sort | uniq -c\n</code></pre> <p>Below we will prepare files for the bioinformatic tools (HISAT/Kallisto) used to analyse RNA-sequencing data.</p>"},{"location":"lab_session/human_genome/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome","text":"<pre><code>cd ~/workspace/inputs/references/transcriptome\n\n# Create a database of observed splice sites represented in our reference transcriptome GTF\n~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf &gt; splicesites.tsv\nhead splicesites.tsv\n\n# Create a database of exon regions in our reference transcriptome GTF\n~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf &gt; exons.tsv\nhead exons.tsv\n\n# build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps\n# specify to use 8 threads with the `-p 8` option\n# run time for this index is ~5 minutes\n~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome\n</code></pre>"},{"location":"lab_session/human_genome/#create-a-reference-transcriptome-index-for-use-with-kallisto","title":"Create a reference transcriptome index for use with Kallisto","text":"<pre><code>cd ~/workspace/inputs/references/transcriptome\nmkdir kallisto\ncd kallisto\n\n# tidy up the headers to just include the ensembl transcript ids\ncat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \"&gt;$1\\n\"}else{print $_}' &gt; ref_transcriptome_clean.fa\n\n# run time for this index is ~30 seconds\nkallisto index --index=ref_transcriptome_kallisto_index ref_transcriptome_clean.fa\n</code></pre>"},{"location":"lab_session/intro_igv/","title":"Introduction to IGV","text":"<p>For various resons there is often a need to manually inspect sequencing data. For this purpose, we will use IGV, the Integrative Genomics Viewer. IGV is in a nutshell an easy-to-use and interactive tool for the visual exploration of genomic data (not just sequening data). In IGV it is possible to integrate multiple data types in a straight forward manner. </p> <p>Download the IGV version that includes java.</p> <pre><code>#First download IGV to your local computer. \n#Using a mac, execute the igv.sh bash script (obs - change to the correct path)\n/PATH_GOES_HERE/IGV_DIRECTORY/igv.sh\n\n#Go to your favourite directory and download a bam file for the IGV intro:\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/igv_intro_bams/* .\n</code></pre>"},{"location":"lab_session/intro_igv/#hcc1143-cell-line","title":"HCC1143 cell line","text":"<p>The cell line sequencincg data that will be used to explore the features of IGV comes from a 52 year old caucasian woman with breast cancer. Additional information can be found at the ATCC (American Type Culture Collection) website. The bam-file contains only reads on chromosome 21:19,000,000-20,000,000 in order to reduce file sizes.</p>"},{"location":"lab_session/intro_igv/#getting-familiar-with-igv","title":"Getting familiar with IGV","text":"<ul> <li>IGV comes pre-loaded with the genome build HG19. During this introduction we will use hg19.</li> <li>Remote data files (tracks) can be loaded from the IGV server. <ul> <li>Select \"File\" &amp; \"Load from Server...\":<ul> <li>Ensembl Genes </li> <li>GC Percentage</li> <li>dbSNP (latest version)</li> <li>Repeat masker</li> </ul> </li> </ul> </li> </ul> <ul> <li>Load the bam file<ul> <li>Select \"File\" &amp; \"Load from File...\":<ul> <li>Locate \"HCC1143.normal.21.19M-20M.bam\" on the hard drive and press \"Open\".</li> </ul> </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#familiarize-the-different-sections-of-igv","title":"Familiarize the different sections of IGV","text":"<ul> <li>The top genome ruler.</li> <li>The top panel data tracks.</li> <li>The mid panel sequencing data tracks.</li> <li> <p>The bottom panel data- and annotation tracks.</p> </li> <li> <p>Note the gene model.</p> <ul> <li>line with arrows: the direction/strand of the gene</li> <li>the thin boxes: untranslated regions</li> <li>the thick boxes: coding exons</li> </ul> </li> </ul> <p></p>"},{"location":"lab_session/intro_igv/#investigate-read-alignments","title":"Investigate read alignments","text":"<ul> <li>Go to chr21:19,479,237-19,479,814.</li> <li>Right click on the aligned reads in the mid panel sequencing data tracks.</li> <li> <p>Test to:</p> <ul> <li>Right click on a read and sort alignments by start location.</li> <li>group alignments by pair orientation. </li> </ul> </li> <li> <p>Experiment with the settings.</p> </li> <li>Click on an individual read and assess the information. </li> <li>Enable the ruler, it simplifies investigating variants. Push the button to the upper right, see figure below: </li> </ul>"},{"location":"lab_session/intro_igv/#investigate-a-snp","title":"Investigate a SNP","text":"<ul> <li>Go to chr21:19479237-19479814</li> <li>Note a heterozygous variant at chr21:19479321.</li> <li>Zoom in to observe the individual base changes.</li> <li>Sort aglinment according to base.</li> <li>Color alignments according to strand.</li> <li>Try to answer the following:<ul> <li>Reflect on mapping and base qualities, is this a high quality SNP?</li> <li>How does the shaded non-reference bases help?</li> <li>How does read strand information help? </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#investigate-a-homopolymer-region-with-variability","title":"Investigate a homopolymer region with variability","text":"<ul> <li>Go to chr21:19518412-19518497.<ul> <li>Group alignments by read strand.</li> <li>Sort alignments by base.</li> </ul> </li> <li>Try to answer the following:<ul> <li>Is this a region with variability that can be trusted, motivate?</li> </ul> </li> <li>Go to chr21:19518470<ul> <li>Sort alignments by base.</li> </ul> </li> <li>Try to answer the following:<ul> <li>Is this a region with variability that can be trusted, motivate? </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#gc-precentage-and-coverge","title":"GC precentage and coverge","text":"<ul> <li>Go to chr21:19611925-19631555.<ul> <li>In the menue bar, go to View &gt; Preferences &gt; Alignments tab &gt; set the visibility range threshold to 20 kb.</li> <li>Use Collapsed view for the read-pairs.</li> <li>Group alignments by \"none\".</li> <li>Sort alignments by start position.</li> <li>Color alignments by -&gt; insert size and pair orientation.</li> </ul> </li> <li>Is GC precentage and coverage correlated?</li> </ul>"},{"location":"lab_session/intro_igv/#snps-on-different-alleles","title":"SNPs on different alleles","text":"<ul> <li>Go to chr21:19666833-19667007.</li> <li>Sort by base at position chr21:19666901</li> <li>Try to answer the following:<ul> <li>Are these valid SNPs?</li> <li>Explan what you are observing. </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#problematic-region","title":"Problematic region","text":"<ul> <li>Load a the repeat masker remote data track.<ul> <li>Select \"File\" &amp; \"Load from Server...\":<ul> <li>Repeat masker</li> </ul> </li> </ul> </li> <li>Go to chr21:19800320-19818162.</li> <li>Try to answer the following:<ul> <li>Why are some reads white?</li> <li>Explan what you are observing. </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#deleted-region","title":"Deleted region","text":"<ul> <li>Go to chr21:19324469-19331468.</li> <li>Select expanded view.</li> <li>View reads as pairs.</li> <li>Color alignments by insert size and pair orientation.</li> <li>Sort reads by insert size.</li> <li>Try to answer the following:<ul> <li>Click on a red read pair and a grey read pair, can you identify information supporting the deletion in the red pairs? </li> <li>Is the deletion hetero- or homozygous? </li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#mis-alignment","title":"Mis-alignment","text":"<ul> <li>Go to chr21:19102154-19103108.</li> <li>Group alignment by \"Chromosome of mate\".</li> <li>Zoom out in a stepwise manner and compare the region of interest with surrounding regions.</li> <li>Try to answer the following:<ul> <li>Click on one of the brown pairs, where does the mate map?</li> <li>What is the reason for an increase in poorly mapped reads?</li> </ul> </li> </ul>"},{"location":"lab_session/intro_igv/#download-the-tumor-and-normal-bam-files-processed-today","title":"Download the tumor and normal bam files processed today","text":"<p>Go to your favourite directory <pre><code># Tumor bam and bai files\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.ba* ./\n\n# Normal bam and bai files\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.ba* .\n\n\n\n\n# The in solution hybridization based capture definition files\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE::~/workspace/inputs/references/exome/probe_regions.bed ./\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/references/exome/exome_regions.bed ./\n</code></pre></p>"},{"location":"lab_session/intro_igv/#open-the-files-in-igv","title":"Open the files in IGV","text":"<ul> <li>Start a new IGV session<ul> <li>Select File -&gt; New Session</li> <li>Select hg38</li> <li>Load the bam files<ul> <li>Exome_Norm_sorted_mrkdup_bqsr.bam</li> <li>Exome_Tumor_sorted_mrkdup_bqsr.bam</li> </ul> </li> <li>Load the cature definion files<ul> <li>exome_regions.bed</li> <li>probe_regions.bed</li> </ul> </li> </ul> </li> <li>Change viewing preferences<ul> <li>In the menue bar, go to View &gt; Preferences &gt; Alignments tab &gt; set the visibility range threshold to 25 kb.</li> <li>Use Expanded view for the read-pairs.</li> <li>Group alignments by \u201cnone\u201d.</li> <li>Sort alignments by start position.</li> <li>Color alignments by -&gt; no color.</li> <li>Type TP53 in the search box and press enter.</li> </ul> </li> <li>Try to answer the following:<ul> <li>Do you observe any variants?<ul> <li>If yes, what kinds?</li> <li>Are they coding/non-coding?</li> <li>Any somatic variants?</li> <li>Try to assess the consequence of a selected variant.</li> </ul> </li> <li>Why do you think there is a difference between the probe- and exome regions?<ul> <li>Assess the coverge pattern relatively the probe regions. Try to explain the pattern.</li> </ul> </li> </ul> </li> </ul>"},{"location":"lab_session/joint_curation/","title":"Joint curation session","text":"<p>During this session we will curate a couple of variants in a joint session. </p> <p>Each of you will get to look at the variant and decide if you think it is a true or false variant. </p> <p>Then we will have a poll to see what the consensus is in the group. </p>"},{"location":"lab_session/joint_curation/#some-simple-curation-considerations","title":"Some simple curation considerations:","text":"<pre><code>- Variant allele fraction (VAF)?\n- Total coverage?\n- Variant coverage?\n- Variant detected in the normal DNA?\n- Repetitive DNA?\n- Mappability and mapping qualities?\n- Base quality?\n- Support from reads in both directions?\n- Is it a lot of variation in the region?\n- Sign of misalignment?\n- ....\n</code></pre> <ul> <li>In IGV, open up the following bam files<ul> <li>Exome_Tumor_sorted_mrkdup_bqsr.bam</li> <li>Exome_Norm_sorted_mrkdup_bqsr.bam</li> </ul> </li> </ul>"},{"location":"lab_session/joint_curation/#variant-1","title":"Variant 1","text":"<ul> <li>chr17 7675088</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/joint_curation/#variant-2","title":"Variant 2","text":"<ul> <li>chr6:619600</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/joint_curation/#variant-2_1","title":"Variant 2","text":""},{"location":"lab_session/joint_curation/#-chr6619600","title":"- chr6:619600","text":""},{"location":"lab_session/joint_curation/#variant-3","title":"Variant 3","text":"<ul> <li>chr6:335424</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/joint_curation/#variant-4","title":"Variant 4","text":"<ul> <li>chr6:325777</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/joint_curation/#variant-5","title":"Variant 5","text":"<ul> <li>chr6:499430</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/joint_curation/#variant-6","title":"Variant 6","text":"<ul> <li>chr6:1742760</li> </ul> <p>Lets do a poll: - True? or - False?</p>"},{"location":"lab_session/processing_of_dna/","title":"Bioinformatic processing of DNA data","text":"<pre><code>cd ~/workspace/inputs/data/fastq\n\n# list all files\ntree\n\n# view the exome normal sample data files\ntree Exome_Norm\n\n# view the RNA-seq tumor sample data files.\ntree RNAseq_Tumor\n</code></pre>"},{"location":"lab_session/processing_of_dna/#investigate-the-raw-data-files-fastq-files","title":"Investigate the raw data files (fastq-files)","text":"<pre><code>cd ~/workspace/inputs/data/fastq/Exome_Tumor\n\n# show the first ten lines of the Exome Tumor fastq files\n# note this is how paird-end data is stored. \n# Read 1\nzcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head\n# Read 2\nzcat Exome_Tumor/Exome_Tumor_R2.fastq.gz | head\n\n# This wikipedia file gives a very nice overview of fastq-files and Illumina base qualities\n# https://en.wikipedia.org/wiki/FASTQ_format\nzcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 8\n\n# what do R1 and R2 refer to? What is the length of each read?\nzcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 2 | tail -n 1 | wc\n\n# how many lines are there in the Exome_Tumor file\nzcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | wc -l # There are: 33,326,620\n\n# how many paired reads or fragments are there then?\nexpr 33326620 / 4 # There are: 8,331,655 paired end reads\n\n# how many total bases of data are in the Exome Tumor data set?\necho \"8331655 * (101 * 2)\" | bc # There are: 1,682,994,310 bases of data\n\n# how many total bases when expressed as \"gigabases\" (specify 2 decimal points using `scale`)\necho \"scale=2; (8331655 * (101 * 2))/1000000000\" | bc # There are: 1.68 Gbp of data\n\n# what is the average coverage we expect to achieve with this much data for the exome region targeted?\n\n# first determine the size of our exome regions (answer = 6683920). \ncat /workspace/inputs/references/exome/exome_regions.bed | perl -ne 'chomp; @l=split(\"\\t\", $_); $size += $l[2]-$l[1]; if (eof){print \"size = $size\\n\"}' # now determine the average coverage of these positions by our bases of data\necho \"scale=2; (8331655 * (101 * 2))/6683920\" | bc # Average covered expected = 251.79x\n\n# what is the fundamental assumption of this calculation that is at least partially not true? What effect will this have on the observed coverage?\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-fastqc-to-check-the-quality-of-the-fastq-files","title":"Run fastqc to check the quality of the fastq-files","text":"<p>Have a look here for a short tutorial on the fastqc output. <pre><code>cd ~/workspace/inputs/data/fastq\n\nfastqc -t 8 Exome_Norm/Exome_Norm*.fastq.gz\nfastqc -t 8 Exome_Tumor/Exome_Tumor*.fastq.gz\ntree\n\nfastqc -t 8 RNAseq_Norm/RNAseq_Norm*.fastq.gz\nfastqc -t 8 RNAseq_Tumor/RNAseq_Tumor*.fastq.gz\ntree\n</code></pre></p>"},{"location":"lab_session/processing_of_dna/#run-multiqc","title":"Run MultiQC","text":"<p>We will run mutliQC to provide a combined report of the fastQC data. </p> <p>More info on MultiQC is available here.</p> <pre><code>cd ~/workspace/inputs\n#Dont do if directory exist\n#mkdir qc\ncd qc\nmultiqc ~/workspace/inputs/data/fastq/\ntree\n# Download MultiQC output to the local computer, open the .html in you favourite browser.\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/qc/multiqc* .\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-bwa","title":"Run BWA","text":"<pre><code># Run bwa mem using the above information\n\ncd ~/workspace/align\n# Runtime: ~ 4min\nbwa mem -t 2 -Y -R \"@RG\\tID:Exome_Norm\\tPL:ILLUMINA\\tPU:C1TD1ACXX-CGATGT.7\\tLB:exome_norm_lib1\\tSM:HCC1395BL_DNA\" -o ~/workspace/align/Exome_Norm.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R2.fastq.gz\n# Runtime: ~ 4min\nbwa mem -t 2 -Y -R \"@RG\\tID:Exome_Tumor\\tPL:ILLUMINA\\tPU:C1TD1ACXX-ATCACG.7\\tLB:exome_tumor_lib1\\tSM:HCC1395_DNA\" -o ~/workspace/align/Exome_Tumor.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R2.fastq.gz\n</code></pre>"},{"location":"lab_session/processing_of_dna/#convert-sam-to-bam-format","title":"Convert sam to bam format","text":"<p><pre><code>cd ~/workspace/align\n# Runtime: ~30s\nsamtools view -@ 2 -h -b -o Exome_Norm.bam Exome_Norm.sam\n# Runtime: ~45s\nsamtools view -@ 2 -h -b -o Exome_Tumor.bam Exome_Tumor.sam\n\n# Have a look at the bam header and try to figure out what type of information that is in the bam file:\nsamtools view -H Exome_Tumor.bam | less -SN # Have a look at the bam file data and try to figure out what type of information that is in the bam file:\nsamtools view Exome_Tumor.bam | less -SN </code></pre> Detailed information on the sam format can be found HERE .</p>"},{"location":"lab_session/processing_of_dna/#sort-bam-files","title":"Sort bam files","text":"<pre><code>cd ~/workspace/align\n# Runtime: ~ 4min\njava -Xmx12g -jar $PICARD SortSam I=Exome_Norm.bam O=Exome_Norm_namesorted.bam SO=queryname\njava -Xmx12g -jar $PICARD SortSam I=Exome_Tumor.bam O=Exome_Tumor_namesorted.bam SO=queryname\n</code></pre>"},{"location":"lab_session/processing_of_dna/#mark-duplicates","title":"Mark duplicates","text":"<p>During library preparation, PCR is performed. Removing the PCR duplicates is necessary for substantial removal of PCR artefacts. Picard MarkDuplicates use the read names and alignment positions to identify PCR duplicates. </p> <pre><code>cd ~/workspace/align\njava -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Norm_namesorted.bam -O Exome_Norm_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Norm_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT\njava -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Tumor_namesorted.bam -O Exome_Tumor_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Tumor_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT\n</code></pre>"},{"location":"lab_session/processing_of_dna/#position-sort-bam-file","title":"Position sort bam file","text":"<p>For indexing and subsequent steps a position-sorted bam is required. Therefore, we will sort bam file by coordinate.</p> <pre><code> cd ~/workspace/align\njava -Xmx12g -jar $PICARD SortSam -I Exome_Norm_namesorted_mrkdup.bam -O Exome_Norm_sorted_mrkdup.bam -SO coordinate\njava -Xmx12g -jar $PICARD SortSam -I Exome_Tumor_namesorted_mrkdup.bam -O Exome_Tumor_sorted_mrkdup.bam -SO coordinate\n</code></pre>"},{"location":"lab_session/processing_of_dna/#create-bam-index-for-use-with-gatk-igv-etc","title":"Create bam index for use with GATK, IGV, etc","text":"<p>In order to efficiently load and search a bam file, downstream applications typically require an index</p> <pre><code>cd ~/workspace/align\njava -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Norm_sorted_mrkdup.bam\njava -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Tumor_sorted_mrkdup.bam\n</code></pre>"},{"location":"lab_session/processing_of_dna/#indel-realignment","title":"Indel Realignment","text":"<p>Depending on the downstream appoarch something called Indel realignment may be needed. During alignment of the DNA data, each individual read is aligned to the reference sequence individually. This may lead to misaligned reads in e.g. repetitive regions or due to somatic/germline variation. If needed, add the realignment-step here, see docs here. In this course the HaplotypeCaller/MuTect2 will be applied to identify germline/somatic variantion. These two variant callers do realignment internally and therefore it is not needed. See the following blog for a detailed discussion of this issue. See here for latest versions of all GATK tutorials.</p>"},{"location":"lab_session/processing_of_dna/#perform-base-quality-score-recalibration","title":"Perform Base Quality Score Recalibration","text":"<p>BQSR stands for Base Quality Score Recalibration. In a nutshell, it is a data pre-processing step that detects systematic errors made by the sequencing machine when it estimates the accuracy of each base call.</p>"},{"location":"lab_session/processing_of_dna/#calculate-bqsr-table","title":"Calculate BQSR Table","text":"<p>First, calculate the BQSR table.</p> <p>NOTE: For exome data, we should modify the below to use the <code>--intervals</code> (<code>-L</code>) option. This excludes off-target sequences and sequences that may be poorly mapped, which have a higher error rate. Including them could lead to a skewed model and bad recalibration.</p> <pre><code>cd ~/workspace/align\ngatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS\n\ngatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS\n</code></pre>"},{"location":"lab_session/processing_of_dna/#apply-bqsr","title":"Apply BQSR","text":"<p>Now, apply the BQSR table to bam files. <pre><code>cd ~/workspace/align\ngatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30\n\ngatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30\n</code></pre></p> <p>Create an index for these new bams</p> <pre><code>cd ~/workspace/align\njava -Xmx12g -jar $PICARD BuildBamIndex I=Exome_Norm_sorted_mrkdup_bqsr.bam\njava -Xmx12g -jar $PICARD BuildBamIndex I=Exome_Tumor_sorted_mrkdup_bqsr.bam\n</code></pre>"},{"location":"lab_session/processing_of_dna/#clean-up-un-needed-sambam-files","title":"Clean up un-needed sam/bam files","text":"<p>Keep final sorted, duplicated marked, bqsr bam/bai/table files and mrkdup.txt files. Delete everything else.</p> <pre><code>cd ~/workspace/align\nmkdir final\nmv *_sorted_mrkdup_bqsr.* final/\nmv *.txt final/\n\nrm *.sam\nrm *.bam\nrm *.bai\n\nmv final/* .\nrmdir final/\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-samtools-flagstat","title":"Run Samtools flagstat","text":"<p>Samtools flagstat provides QC-metrics after aligning the reads, e.g. the fraction of read-pairs that map to different chromosomes (which should be limited).</p> <pre><code>cd ~/workspace/align/\nsamtools flagstat ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam &gt; ~/workspace/align/Exome_Norm_flagstat.txt\nsamtools flagstat ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam &gt; ~/workspace/align/Exome_Tumor_flagstat.txt\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-various-picard-collectmetrics-tools","title":"Run various picard CollectMetrics tools","text":"<p>Picard is a widely used QC tool in genomics and it is named after a famous Star Trek Captain.</p> <p>Description of the output of picard can be found here. </p> <pre><code>cd ~/workspace/align/\njava -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_insert_size_metrics.txt -H ~/workspace/align/Exome_Norm_insert_size_metrics.pdf\njava -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_insert_size_metrics.txt -H ~/workspace/align/Exome_Tumor_insert_size_metrics.pdf\n\njava -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa\njava -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_exome_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa\n\n#Picard CollectHsMetrics\n#Exome Only\njava -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list\njava -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-fastqc","title":"Run FastQC","text":"<pre><code>cd ~/workspace/align\nfastqc -t 8 Exome_Norm_sorted_mrkdup_bqsr.bam\nfastqc -t 8 Exome_Tumor_sorted_mrkdup_bqsr.bam\ntree\n</code></pre>"},{"location":"lab_session/processing_of_dna/#run-multiqc-to-produce-a-final-report","title":"Run MultiQC to produce a final report","text":"<pre><code>cd ~/workspace/align\n#Dont do if directory exist\n#mkdir post_align_qc\ncd post_align_qc\nmultiqc ~/workspace/align/\ntree\n\n# Download MultiQC output to the local computer, open the .html in you favourite browser.\n\n# Discuss the MultiQC output with a fellow student\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/post_align_qc/multiqc* .\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/","title":"BIOINFORMATIC PROCESSING OF RNA DATA","text":""},{"location":"lab_session/processing_of_rnaseq/#adapter-trimming-of-the-fastq-files","title":"Adapter Trimming of the FASTQ files","text":"<p>The purpose of adapter trimming is to remove sequences in our data that correspond to the Illumina sequence adapters.  The most common adapter trimming scenario is the removal of adapter sequences that occur at the end of read sequences. This happens when a DNA (or cDNA) fragment is shorter than the read length.  For example if we sequence our RNA-seq fragments to 150 base length and a fragment is only 140 bases long the read will end with 10 bases of adapter sequence. Since this adapter sequence does not correspond to the genome, it will not align. Too much adapter sequence can actually prevent reads from aligning at all. Adapter trimming may therefore sometime improve the overall alignment success rate for an RNA-seq data set.  Adapter trimming involves a simplistic alignment itself and therefore can be computationally expensive.</p> <p><pre><code>cd ~/workspace/inputs/references\n\n#Command already run\n#wget -c http://genomedata.org/rnaseq-tutorial/illumina_multiplex.fa\n\n#Have a look at the adapter sequences that we want to remove from the RNA data\nless -SN ~/workspace/inputs/references/illumina_multiplex.fa\n\n#Start with tumor RNAseq data\ncd ~/workspace/inputs/data/fastq/RNAseq_Tumor\n\n#Have a look to see what is in the folder\nls -halt\n\n#pPerform trimming using flexbar\n\n#Before using a new tool, make a habit of looking the tool up,\n# google \"flexbar trimming adapters\" or something similar\n\n#As this step takes approxmately 25 min, we will use nohup. \n#Google \"nohup\" ...\n#If running with nohup the command will continue even if the connection breaks to the aws server\n\n#Also, notice that we are using the flexbar argument --threads 7 which means that we are using 7 cpu threads. To check how many you have at your disposal use the command:\nlscpu\n#We have 2 thread left which is good, not to hog all resources\n#To check the available RAM use the command:\ncat /proc/meminfo\n\n#When trimming the adapters run two simultaneous jobs using 7 cores/job and send both to the background if connection breaks. Redirect the standard output to a custom log file\n#nohup cmd &gt; custom-out.log &amp;\n\nnohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane1_R1.fastq.gz --reads2 RNAseq_Tumor_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1 &gt; RNAseq_Tumor_Lane1.log &amp;\n\n#Start the second process\nnohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane2_R1.fastq.gz --reads2 RNAseq_Tumor_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2 &gt; RNAseq_Tumor_Lane2.log &amp;\n\n#Use htop to monitor the processor load\nhtop\n#quit htop by pressing \"q\"\n\n#have a look at one of the log files\nless -SN RNAseq_Tumor_Lane1.log\n\n####### Wait for the processes to stop before continuing #######\n\n#Start the \n#Normal RNAseq data\ncd ~/workspace/inputs/data/fastq/RNAseq_Norm\n\nnohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane1_R1.fastq.gz --reads2 RNAseq_Norm_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1 &gt; RNAseq_Normal_Lane1.log &amp;\n\nflexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane2_R1.fastq.gz --reads2 RNAseq_Norm_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2 &gt; RNAseq_Normal_Lane2.log &amp;\n\n####### Wait for the processes to stop #######\n</code></pre> As you can see, nohup is very useful for sending processes to the backgroud that will continue if connection breaks or if you need to log out. For repetition how to send stdout and stderr to a file while running nohup, have a look here.</p>"},{"location":"lab_session/processing_of_rnaseq/#run-fastqc-to-check-the-quality-of-the-fastq-files-as-for-dna-data","title":"Run fastqc to check the quality of the fastq-files (as for DNA data)","text":"<p><pre><code>#Run fastqc and compre the trimmed and non-trimmed file\ncd ~/workspace/inputs/data/fastq/RNAseq_Tumor\nfastqc RNAseq_Tumor_Lane1_R1.fastq.gz\nfastqc RNAseq_Tumor_Lane1_1.fastq.gz\n\n# Download the output to your computer\nscp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1_fastqc.html .\nscp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1_fastqc.html .\n</code></pre> Compare the two html files side by side: - Any obivous difference?</p> <pre><code>#Move the trimmed data\nmkdir -p ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed\nmkdir -p ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed\nmv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz\n\nmv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz\nmv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz\n\n#make a habit of always checking the result of any command\nls -halt ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed\nls -halt ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#alignment","title":"Alignment","text":"<p>We will use the aligner HISAT2 to perform spliced alignments to our reference genome. For efficiency, the output of HISAT (SAM format) will be piped directly to another program called sambamba to first convert to BAM format and then sort that BAM file. Before each command we will also create and assign a path for temporary directories.</p> <pre><code>mkdir -p ~/workspace/rnaseq/alignments\ncd ~/workspace/rnaseq/alignments\n\n# Align tumor data\n# Runtime: ~15 min each run\nTUMOR_DATA_1_TEMP=`mktemp -d ~/workspace/rnaseq/alignments/2895626107.XXXXXXXXXXXX`\nnohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626107 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.4 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz -2  ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam /dev/stdin &amp;\n\nTUMOR_DATA_2_TEMP=`mktemp -d ~/workspace/rnaseq/alignments/2895626112.XXXXXXXXXXXX`\nnohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626112 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.5 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz -2  ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam /dev/stdin &amp;\n\n#Wait for the processes to finish, monitor using htop\nrmdir $TUMOR_DATA_2_TEMP $TUMOR_DATA_1_TEMP\n\n# Align normal data\n\nNORMAL_DATA_1_TEMP=`mktemp -d ~/workspace/rnaseq/alignments/2895625992.XXXXXXXXXXXX`\nnohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895625992 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.4 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz -2  ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam /dev/stdin &amp;\n\nNORMAL_DATA_2_TEMP=`mktemp -d ~/workspace/rnaseq/alignments/2895626097.XXXXXXXXXXXX`\n\nnohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626097 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.5 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz -2  ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam /dev/stdin &amp;\n\n#Wait for the processes to finish, monitor using htop\nrmdir $NORMAL_DATA_2_TEMP $NORMAL_DATA_1_TEMP\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#merging-bams","title":"Merging BAMs","text":"<p>Since we have multiple BAMs of each sample that just represent additional data for the same sequence library, we should combine them into a single BAM for convenience before proceeding.</p> <pre><code>cd ~/workspace/rnaseq/alignments\n#Runtime: ~ 8m each merging command\nsambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam\n\nsambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#gtf-general-transfer-format-and-creating-files-for-downstream-processing","title":"GTF (General Transfer Format) and creating files for downstream processing","text":"<p>We will encounter the GTF file format during the exercises below, described in detail here.</p> <p>The GTF format is use to describe genes and other features of DNA, RNA and proteins.</p> <p>In the section below we will prepare files needed for RNAseq QC</p> <p>NOTE: for the sake of time we are only investigating genes on chr6 and chr17</p> <pre><code>#Go to this directory:\ncd ~/workspace/inputs/references/transcriptome\n\n#Have a peak into the transcriptome gtf file and try to make sense of it using the ensembl description above\nless -SN ref_transcriptome.gtf\n\n#check on which chromsomes from which we have transcripts\ncut -f1 ref_transcriptome.gtf | sort | uniq -c\n\n# Generating the necessary input files for picard CollectRnaSeqMetrics\ncd ~/workspace/inputs/references/transcriptome\n\ngrep -i rrna ref_transcriptome.gtf &gt; ref_ribosome.gtf\n\n#A bed file with Ribosomal RNA\ngff2bed &lt; ~/workspace/inputs/references/transcriptome/ref_ribosome.gtf &gt; ref_ribosome.bed\n\n#Check the ribosome bed file\nless -SN ref_ribosome.bed\n\njava -jar $PICARD BedToIntervalList I=~/workspace/inputs/references/transcriptome/ref_ribosome.bed O=~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list SD=~/workspace/inputs/references/genome/ref_genome.dict\n\ngtfToGenePred -genePredExt ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf ~/workspace/inputs/references/transcriptome/ref_flat.txt\n\ncat ref_flat.txt | awk '{print $12\"\\t\"$0}' | cut -d$'\\t' -f1-11 &gt; ref_flat_final.txt\n\nmv ref_flat_final.txt ref_flat.txt\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#post-alignment-qc","title":"Post-alignment QC","text":"<p><pre><code>cd ~/workspace/rnaseq/alignments\n#Run samtools flagstat\n#Runtime: ~2min, start and send to background\nnohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Norm.bam &gt; ~/workspace/rnaseq/alignments/RNAseq_Norm_flagstat.txt 2&gt; flagstat_RNA_N_bam.out &amp;\nnohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam &gt; ~/workspace/rnaseq/alignments/RNAseq_Tumor_flagstat.txt 2&gt; flagstat_RNA_T_bam.out &amp;\n\n#Runtime: ~12 min, start and send to background\nnohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam &gt; fastqc_RNA_T_bam.out 2&gt;&amp;1 &amp;\nnohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam &gt; fastqc_RNA_N_bam.out 2&gt;&amp;1 &amp;\n\n# Runtime: 26min\nnohup java -jar $PICARD CollectRnaSeqMetrics I=~/workspace/rnaseq/alignments/RNAseq_Norm.bam O=~/workspace/rnaseq/alignments/RNAseq_Norm.RNA_Metrics REF_FLAT=~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND=SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS=~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list &gt; collectRnaSeqMetrics_N.out 2&gt;&amp;1 &amp;\n#Follow the progress of the program by looking in the nohup output file\nless -SN collectRnaSeqMetrics_N.out\n\nnohup java -jar $PICARD CollectRnaSeqMetrics I=~/workspace/rnaseq/alignments/RNAseq_Tumor.bam O=~/workspace/rnaseq/alignments/RNAseq_Tumor.RNA_Metrics REF_FLAT=~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND=SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS=~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list &gt; collectRnaSeqMetrics_T.out 2&gt;&amp;1 &amp;\n\ncd ~/workspace/rnaseq\n#mkdir post_align_qc\ncd post_align_qc\nmultiqc ~/workspace/rnaseq/alignments/\n\n#Finally, download multiqc files to your local computer\n# Download MultiQC output to the local computer, open the .html in you favourite browser.\nscp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/rnaseq/post_align_qc/multiqc* .\n</code></pre> Familiarize yourself with the RNAseq MultiQC data</p>"},{"location":"lab_session/processing_of_rnaseq/#indexing-bams","title":"Indexing BAMs","text":"<p>In order to be able to view our BAM files in IGV, as usual we need to index them <pre><code>cd  ~/workspace/rnaseq/alignments/\nsamtools index RNAseq_Norm.bam\nsamtools index RNAseq_Tumor.bam\n</code></pre></p>"},{"location":"lab_session/processing_of_rnaseq/#run-a-simplified-reference-only-stringtie-expression-approach","title":"Run a simplified \"reference only\" StringTie expression approach","text":"<ul> <li>The StringTie developer's recommend to: <ul> <li>perform reference guided transcript compilation (aka transcript assembly) on each individual sample.</li> <li>merge transcript predictions from all samples into a single model of the transcriptome.</li> <li>annotate this predicted transcriptome with known transcriptome information.</li> <li>estimate abundance for each of the transcripts in this final transcriptome model in each sample.</li> </ul> </li> </ul> <p>In the final result we would have abundance for all the same transcripts across all samples. This includes a combination of predicted and known transcripts. </p> <p>It is sometimes convenient to have a more simplified workflow where we only have values for known transcripts. This is particularly true in species where we already have comprehensive high quality transcriptome annotations and there is less of a focus on de novo transcript discovery.</p> <p>The following workflow produces a \"reference-only\" transcriptome result in which we will perform abundance calculations on each lane of data individually.</p> <p><pre><code>cd ~/workspace/rnaseq\n#mkdir ref-only-expression\ncd ref-only-expression\n\nnohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam &gt; knowntranscripts_T1_L1.out 2&gt;&amp;1 &amp;\nnohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam &gt; knowntranscripts_T1_L2.out 2&gt;&amp;1 &amp;\nnohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam &gt; knowntranscripts_N1_L1.out 2&gt;&amp;1 &amp;\nnohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam &gt; knowntranscripts_T2_L2.out 2&gt;&amp;1 &amp;\n\n#Wait to continue submitting jobs until the scripts have finished\n</code></pre> Create a tidy expression matrix files for the StringTie results. This will be done at both the gene and transcript level and also will take into account the various expression measures produced: coverage, FPKM, and TPM.</p> <pre><code>cd ~/workspace/rnaseq/ref-only-expression\n\n#Already run\n#wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/stringtie_expression_matrix.pl\n#chmod +x stringtie_expression_matrix.pl\n\n./stringtie_expression_matrix.pl --expression_metric=TPM --result_dirs='RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file=transcript_tpm_all_samples.tsv --gene_matrix_file=gene_tpm_all_samples.tsv\n\n./stringtie_expression_matrix.pl --expression_metric=FPKM --result_dirs='RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file=transcript_fpkm_all_samples.tsv --gene_matrix_file=gene_fpkm_all_samples.tsv\n\n./stringtie_expression_matrix.pl --expression_metric=Coverage --result_dirs='RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file=transcript_coverage_all_samples.tsv --gene_matrix_file=gene_coverage_all_samples.tsv\n\n#Have a look at the output files\nhead gene_coverage_all_samples.tsv transcript_coverage_all_samples.tsv gene_fpkm_all_samples.tsv transcript_fpkm_all_samples.tsv gene_tpm_all_samples.tsv transcript_tpm_all_samples.tsv\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#reference-free-expression-analysis-with-kallisto","title":"Reference Free Expression Analysis with Kallisto","text":"<p>Remember that in previous sections we have been using reference genome fasta sequences for the reference for alignment and subsequent steps. However, Kallisto works directly on target cDNA/transcript sequences. </p> <p>We have for stringtie used transcript annotations for genes on our subset of chromosomes (i.e. chr6 and chr17). The transcript models were downloaded from Ensembl in GTF format. This GTF contains a description of the coordinates of exons that make up each transcript but it does not contain the transcript sequences themselves which Kallisto is using. There are many places we could obtain such transcript sequences. For example, we could have download them directly in Fasta format from the Ensembl FTP site (or from UCSC or NCBI).</p> <pre><code>cd ~/workspace/rnaseq/\n#mkdir kallisto\ncd kallisto\n\n# first check that the GTF and Fasta file are present for Kallisto\nhead ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf\nhead ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa\n\n#Each transcript and its sequence\nhead ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa\n\n# now check for the kallisto index is there\nls -halt ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index\n\n# create a list of all transcript IDs for later use:\ncd ~/workspace/rnaseq/kallisto/\ncat ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa | grep \"&gt;\" | perl -ne '$_ =~ s/\\&gt;//; print $_' | sort | uniq &gt; transcript_id_list.txt\n\nhead -n 10 transcript_id_list.txt\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#generate-abundance-estimates-for-all-samples-using-kallisto","title":"Generate abundance estimates for all samples using Kallisto","text":"<p>As we did with StringTie we will generate transcript abundances for each of our demonstration samples using Kallisto. Here we are treating the two lanes for each sample as if they were independent samples.</p> <pre><code>cd ~/workspace/rnaseq/kallisto/\n#mkdir quants\ncd quants\n\nnohup kallisto quant --index=/home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir=RNAseq_Norm_Lane1 --threads=8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz &gt; kallisto_quant_N_L1.out 2&gt;&amp;1 &amp;\n\nnohup kallisto quant --index=/home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir=RNAseq_Norm_Lane2 --threads=8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz &gt; kallisto_quant_N_L2.out 2&gt;&amp;1 &amp;\n\nnohup kallisto quant --index=/home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir=RNAseq_Tumor_Lane1 --threads=8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz &gt; kallisto_quant_T_L1.out 2&gt;&amp;1 &amp;\n\nnohup kallisto quant --index=/home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir=RNAseq_Tumor_Lane2 --threads=8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz &gt; kallisto_quant_T_L2.out 2&gt;&amp;1 &amp;\n\n#check nohup output\nless -SN kallisto_quant_T_L2.out\n\n#Create a single TSV file that has the TPM abundance estimates for all six samples.\n#First check the contets of an abudance output file from Kallisto\nless -SN ./RNAseq_Norm_Lane1/abundance.tsv\n\n#Merge all files\npaste */abundance.tsv | cut -f 1,2,5,10,15,20 &gt; transcript_tpms_all_samples.tsv\nls -1 */abundance.tsv | perl -ne 'chomp $_; if ($_ =~ /(\\S+)\\/abundance\\.tsv/){print \"\\t$1\"}' | perl -ne 'print \"target_id\\tlength$_\\n\"' &gt; header.tsv\n\ncat header.tsv transcript_tpms_all_samples.tsv | grep -v \"tpm\" &gt; transcript_tpms_all_samples.tsv2\n\nmv transcript_tpms_all_samples.tsv2 transcript_tpms_all_samples.tsv\n\nrm -f header.tsv\n\nhead transcript_tpms_all_samples.tsv\n\n#First create a gene version of the Kallisto TPM matrix, we will simply sum the TPM values for transcripts of the same gene.\n\n# Command already run\n#wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/kallisto_gene_matrix.pl\n#chmod +x kallisto_gene_matrix.pl\n\n./kallisto_gene_matrix.pl --gtf_file=/home/ubuntu/workspace/inputs/references/transcriptome/ref_transcriptome.gtf --kallisto_transcript_matrix_in=transcript_tpms_all_samples.tsv --kallisto_transcript_matrix_out=gene_tpms_all_samples.tsv\n\nless -SN gene_tpms_all_samples.tsv\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#compare-expression-values-between-kallisto-and-stringtie","title":"Compare expression values between Kallisto and StringTie","text":"<p>To compare the two approaches we can use the expression value for each Ensembl transcript.</p> <p>To do this comparison, we need to gather the expression estimates for each of our replicates from each approach. </p> <p>First - download the files to your local machine</p> <pre><code>#Run these commands on your local machine\n\nscp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/kallisto/transcript_tpms_all_samples.tsv .\n\nmv transcript_tpms_all_samples.tsv kallisto_transcript_tpms_all_samples.tsv\n\nscp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/ref-only-expression/transcript_tpm_all_samples.tsv .\nmv transcript_tpm_all_samples.tsv stringtie_transcript_tpms_all_samples.tsv\n\n#Remember, one gene can have many transcripts. The ensembl gene ID for TP53 is ENSG00000141510. Check which ensembl transcripts that exist in the reference gtf.\n\n#RUN ON THE AWS SERVER\ngrep ENST ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf | grep ENSG00000141510 | perl -nle '@a = split /\\t/; $_ =~ /transcript_id\\s\"(\\S*)\";/g; print $1;' | sort | uniq\n\n# start R\n\n# set the working directory to where the files were downloaded\nsetwd(\"DIR GOES HERE\")\n\n# load libraries\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# read in data\nkallisto_transcript_tpm &lt;- read.delim(\"kallisto_transcript_tpms_all_samples.tsv\")\nhead(kallisto_transcript_tpm)\n\nstringtie_transcript_tpm &lt;- read.delim(\"stringtie_transcript_tpms_all_samples.tsv\")\nhead(stringtie_transcript_tpm)\n\n# minor reformatting\nkallisto_transcript_tpm &lt;- kallisto_transcript_tpm[,-2]\nkallisto_transcript_tpm &lt;- melt(kallisto_transcript_tpm, id.vars=c(\"target_id\"))\nhead(kallisto_transcript_tpm)\n\nstringtie_transcript_tpm &lt;- melt(stringtie_transcript_tpm, id.vars=c(\"Transcript_ID\"))\nhead(stringtie_transcript_tpm)\n\n# merge the data\nkallisto_stringtie_tpm &lt;- merge(kallisto_transcript_tpm, stringtie_transcript_tpm, by.x=c(\"target_id\", \"variable\"), by.y=c(\"Transcript_ID\", \"variable\"), suffixes=c(\".kallisto\", \".stringtie\"))\n\n#The numeric vector is read as a character vector, correcting ..\nkallisto_stringtie_tpm$value.stringtie = as.numeric(kallisto_stringtie_tpm$value.stringtie)\nstr(kallisto_stringtie_tpm)\n\n#TP53 transcript ID vector from the transcript GTF\nTP53 = c(\"ENST00000269305\", \"ENST00000359597\", \"ENST00000413465\", \"ENST00000420246\", \"ENST00000445888\", \"ENST00000455263\", \"ENST00000503591\", \"ENST00000504290\", \"ENST00000504937\", \"ENST00000505014\", \"ENST00000508793\", \"ENST00000509690\", \"ENST00000510385\", \"ENST00000514944\", \"ENST00000574684\", \"ENST00000576024\", \"ENST00000604348\", \"ENST00000610292\", \"ENST00000610538\", \"ENST00000610623\", \"ENST00000615910\", \"ENST00000617185\", \"ENST00000618944\", \"ENST00000619186\", \"ENST00000619485\", \"ENST00000620739\", \"ENST00000622645\", \"ENST00000635293\")\n\nidx = match(kallisto_stringtie_tpm$target_id, TP53)\nidx.1 = which(is.na(idx) == FALSE)\nidx.2 = idx[idx.1]\nTP53[idx.2] == kallisto_stringtie_tpm$target_id[idx.1]\n#Check TP53 expression values\nkallisto_stringtie_tpm[idx.2,]\n#To plot TP53, make a new column\nkallisto_stringtie_tpm$TP53 = \"Other transcript\"\nkallisto_stringtie_tpm$TP53[idx.2] = \"TP53\"\n\nkallisto_stringtie_tpm$TP53 = factor(kallisto_stringtie_tpm$TP53, levels = c(\"Other transcript\", \"TP53\"))\nstr(kallisto_stringtie_tpm)\n\n#To remove 0/NA values\nkallisto_stringtie_tpm$value.stringtie[which(is.na(kallisto_stringtie_tpm$value.stringtie)==TRUE)] = 0.001\n#kallisto_stringtie_tpm$value.kallisto[which(is.na(kallisto_stringtie_tpm$value.kallisto)==TRUE)] = 0.001\nkallisto_stringtie_tpm$value.stringtie = kallisto_stringtie_tpm$value.stringtie + 0.1\nkallisto_stringtie_tpm$value.kallisto = kallisto_stringtie_tpm$value.kallisto + 0.1\n########### plot the result ######################\n\n#pdf(file=\"transcript_stringtie_v_kallisto.pdf\", height=8, width=8)\nggplot(data=kallisto_stringtie_tpm, aes(x=log2(value.kallisto), y=log2(value.stringtie), colour=TP53)) +\ngeom_point(alpha=1) +\nscale_colour_manual(values = c(\"forestgreen\", \"firebrick\", \"dodgerblue\"), name = \"Transcripts\", drop=TRUE) +\ngeom_point(data = subset(kallisto_stringtie_tpm, TP53 == \"TP53\"), aes(x=log2(value.kallisto), y=log2(value.stringtie), colour=TP53), inherit.aes=FALSE)+\nfacet_wrap(~variable)+\ntheme_bw()\n\n#To plot density disbributions\ndf = rbind(data.frame(variable = kallisto_stringtie_tpm$variable, value =  kallisto_stringtie_tpm$value.kallisto, type = \"kallisto\"), data.frame(variable = kallisto_stringtie_tpm$variable, value =  kallisto_stringtie_tpm$value.stringtie, type = \"stringtie\"))\ndf$type = factor(df$type, levels =c(\"kallisto\",\"stringtie\"))\nhead(df)\n\nggplot(data=df, aes(x=log2(value))) +\ngeom_density() +\nfacet_wrap(type~variable, ncol = 4)+\nggtitle(\"Density distributions\")+\nxlab(\"Log2 expression value\")+\ntheme(legend.position = \"none\")\n#dev.off()\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#gene-fusion-detection","title":"Gene Fusion detection","text":""},{"location":"lab_session/processing_of_rnaseq/#introduction","title":"Introduction","text":"<p>In addition to providing information about gene expression, RNA-seq data can be used to discover transcripts which result from chromosomal translocations. Translocations and their resultant chimeric (AKA fusion) transcripts are important driver mutations in many cancers. A variety of specialized alignment and filtering strategies have been developed to identify fusion transcripts from RNA, but these programs suffer from low specificity (i.e. many false-positives) and poor correlation across methods.</p> <p>This tutorial uses the kallisto and pizzly tools for fusion detection from RNA-seq data. Kallisto quantifies transcript abundance through pseudoalignment. Pizzly aligns reads which kallisto has flagged as potentially spanning fusion junctions. Running the tutorial requires RNA fastq files, a reference transcriptome, and a gene annotation file- see below.</p> <p>Some of the tools used to detection gene fusions from RNAseq are Arriba, EricScript, FusionCatcher, Fusion-Inspector, fusion-report, Pizzly, Squid, Star-Fusion.</p> <p>All these tools have their own merits and demerits. They are often used in combinations or all together. Hence, we need to assess which one is suitable for our data.</p> <p>We will use Pizzly in this exercise.</p>"},{"location":"lab_session/processing_of_rnaseq/#setup","title":"Setup","text":"<p>Prerequisites- This module assumes you have completed the initial setup process for the course including: - installed Conda package manager, R, pizzly, and kallisto.     - have fastq sequence files of normal and tumor RNA at ~/workspace/inputs/data/fastq/chr6_and_chr17/. </p> <p>Additional setup:</p> <p>Important: pizzly will not work with the most recent Ensembl human GTF annotation file. Download the version 87 GTF as shown in the below code block. We will subset the reference transcriptome fasta file. Fasta splitting programs which do not preserve the full Ensembl header such as gffread or gtf_to_fasta will not work with pizzly.</p> <ul> <li>Download Ensembl GTF and fasta and parse to include only chromosomes 6 and 17 (15 min): </li> </ul> <p><pre><code># Get files from source\n#Already run\n#mkdir -p ~/workspace/inputs/reference/fusion\n#cd ~/workspace/inputs/reference/fusion/\n#wget ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz\n#gunzip Homo_sapiens.GRCh38.cdna.all.fa.gz       \n#wget ftp://ftp.ensembl.org/pub/release-87/gtf/homo_sapiens/Homo_sapiens.GRCh38.87.gtf.gz\n#gunzip -k Homo_sapiens.GRCh38.87.gtf.gz\n\ncd ~/workspace/inputs/reference/fusion/\nls -halt\n\n# Get annotations for only chromosomes 6 and 17\ncat Homo_sapiens.GRCh38.87.gtf | grep --color=never -w \"^6\" &gt; chr617.gtf\ncat Homo_sapiens.GRCh38.87.gtf | grep --color=never -w \"^17\"&gt;&gt; chr617.gtf \nhead chr617.gtf\ntail chr617.gtf\n\n#### CODE BELOW ALREDY RUN - TAKES &gt;2h\n# Parse transcriptome fasta, preserving full ensembl headers\n#  Setup directory\n#mkdir -p ~/workspace/inputs/reference/fusion/per-feature\n#cd ~/workspace/inputs/reference/fusion/per-feature\n#  Split fasta at each instance of a sequence header and write to new file\n#csplit -s -z ../Homo_sapiens.GRCh38.cdna.all.fa '/&gt;/' '{*}'\n#  If from chromosomes 6 or 17, rename files using the columns of the original ensemble header\n#####  (This step takes about 120 minutes. You can proceed with the next section in ~/workspace/inputs/data/fastq/chr6_and_chr17)\n\n#Run with Nohup in case connection is lost\n#for f in xx*; do awk -F \":\" 'NR==1 &amp;&amp; $3==\"6\" || NR==1 &amp;&amp; $3==\"17\"{print $2 \".\" $3 \".\" $4 \".\" $5}' $f | xargs -I{} mv $f {}.fa; done\n#  Concatenate features from chromsomes 6 and 17 to a new reference fasta  \n#cd ~/workspace/inputs/reference/fusion\n#cat ./per-feature/GRCh38.6.*.fa ./per-feature/GRCh38.17.*.fa &gt; chr617.fa\n#rm -rf per-feature\n</code></pre> To get one read pair each for normal and tumor, merge the chr6_and_chr17 only RNA-seq fastqs.</p> <pre><code>mkdir -p ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion\ncd ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion\ncat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz &gt; RNAseq_Norm_R1.fastq.gz\ncat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz &gt; RNAseq_Norm_R2.fastq.gz\ncat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz &gt; RNAseq_Tumor_R1.fastq.gz\ncat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz &gt; RNAseq_Tumor_R2.fastq.gz\n</code></pre> <ul> <li>Subsample fastqs to allow fusion alignment to run quickly (5 min): <pre><code># Use seqtk to take subsamples of the 30% of the fastq read pairs  \nseqtk sample -s100 RNAseq_Norm_R1.fastq.gz 0.3 &gt; subRNAseq_Norm_R1.fastq.gz\nseqtk sample -s100 RNAseq_Norm_R2.fastq.gz 0.3 &gt; subRNAseq_Norm_R2.fastq.gz\nseqtk sample -s100 RNAseq_Tumor_R1.fastq.gz 0.3 &gt; subRNAseq_Tumor_R1.fastq.gz\nseqtk sample -s100 RNAseq_Tumor_R2.fastq.gz 0.3 &gt; subRNAseq_Tumor_R2.fastq.gz\n</code></pre></li> </ul>"},{"location":"lab_session/processing_of_rnaseq/#run-fusion-alignment","title":"Run Fusion Alignment","text":"<ul> <li>Create kallisto index:</li> </ul> <pre><code>#mkdir -p ~/workspace/rnaseq/fusion\ncd ~/workspace/rnaseq/fusion\nkallisto index -i index.617.idx -k 31 --make-unique ~/workspace/inputs/reference/fusion/chr617.fa\n</code></pre>"},{"location":"lab_session/processing_of_rnaseq/#call-fusions","title":"Call fusions","text":"<pre><code>cd ~/workspace/rnaseq/fusion\n#Quantify potential fusions (**15 min**):\nnohup kallisto quant -i index.617.idx --fusion -o kquant-norm617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R2.fastq.gz 2&gt; kallisto_quant_N_bam.out &amp;\n\nnohup kallisto quant -i index.617.idx --fusion -o kquant-tumor617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R2.fastq.gz 2&gt; kallisto_quant_T_bam.out &amp;\n\n#Call fusions with pizzly: **120 mins**\ncd ~/workspace/rnaseq/fusion\nnohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-norm617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output norm-fuse617 kquant-norm617/fusion.txt 2&gt; pizzly_N_bam.out &amp;\n\nnohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-tumor617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output tumor-fuse617 kquant-tumor617/fusion.txt 2&gt; pizzly_T_bam.out &amp;\n</code></pre> <ul> <li>If using 30% of reads with the above process, expect about 13,000 retained transcripts from normal and about 3,000 retained transcripts from tumor. See next section to investigate the output of the pizzly fusion calling.</li> </ul> <pre><code>#To go the fusion directory\ncd ~/workspace/rnaseq/fusion\n\n# ALREADY RUN: Get R scripts for later use\n#wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/mod.grolar.R\n#wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/import_Pizzly.R\nscp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/*.json .\nscp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/mod20220117.grolar.R .\n\n#list files\nls -halt\n\n# Open R on AWS and set working directory \nR\nsetwd(\"~/workspace/rnaseq/fusion/\")\n#Check the path\ngetwd()\n\n# Load required packages\nlibrary(cowplot)\nlibrary(jsonlite)\nlibrary(dplyr)\nlibrary(EnsDb.Hsapiens.v86)\nlibrary(ggplot2)\nlibrary(chimeraviz)\n\n# Change to your own output location\nsetwd(\"~\")\n\n# Load data\nsuffix = \"617.json\"\nprint(suffix)\nJSON_files = list.files(path = \"~\", pattern = paste0(\"*\",suffix))\nprint(JSON_files)\nIds = gsub(suffix, \"\", JSON_files)\nprint(Ids)\n\n# Assuming you have used GRCh38 gene models\nedb &lt;- EnsDb.Hsapiens.v86\nlistColumns(edb)\nsupportedFilters(edb)\n\n# Load the functoin https://github.com/MattBashton/grolar/blob/master/grolar.R\nsource(\"mod20220117.grolar.R\")\n\n# Suffix which apears after sample id in output file name\n# Use above funciton on all output files\nlapply(Ids, function(x) GetFusionz_and_namez(x, suffix = \"617.json\") )\n\n# This function will flatten out the JSON giving you a list of gene A and gene B sorted by splitcount then paircount\n# Each fusion now has a unique identifier, sequence positions, and distance values for genes from the same chromosome:\n\n# Read the flattened files\nnormal=read.table(\"~/norm-fuse_fusions_filt_sorted.txt\", header=T)\ntumor=read.table(\"~/tumor-fuse_fusions_filt_sorted.txt\", header=T)\n\n#Investigate the first five rows\nhead(normal, 5)\nhead(tumor, 5)\n\n# Common filtering tasks for fusion output include removing fusions from the tumor sample which are present in the normal, and removing fusions for which the is little support by pair and split read counts:\n\nnormal$genepr=paste0(normal$geneA.name,\".\",normal$geneB.name)\ntumor$genepr=paste0(tumor$geneA.name,\".\",tumor$geneB.name)\nuniqueTumor=subset(tumor, !(tumor$genepr %in% normal$genepr))\nnrow(uniqueTumor)==nrow(tumor)\n[1] FALSE\nnrow(tumor)-nrow(uniqueTumor)\n[1] 2\n# There are two fusions (or at least fusion gene pairs) from the normal sample which are also present in the tumor. \n# Examine the output of- \nshared_src_tumor=subset(tumor, (tumor$genepr %in% normal$genepr))\nshared_src_normal=subset(normal, (normal$genepr %in% tumor$genepr))\nshared_src_tumor\nshared_src_normal\n\n# Filtering by counts:\n\n# Merge normal and tumor data\nnormal$sample=\"normal\"\ntumor$sample=\"tumor\"\nallfusions=rbind(normal,tumor)\n# Compare counts of paired and split reads\ntapply(allfusions$paircount, allfusions$sample, summary)\ntapply(allfusions$splitcount, allfusions$sample, summary)\n\n# As a density plot\np1=ggplot(allfusions, aes(paircount, fill=sample))+\ngeom_density(alpha=.4)+\ngeom_vline(xintercept=2)+\ncoord_fixed(ratio=15)\n\np2=ggplot(allfusions, aes(splitcount, fill=sample))+\ngeom_density(alpha=.4)+coord_cartesian(ylim= c(0,.2))+\ngeom_vline(xintercept=5)+\ncoord_fixed(ratio=200)\n\nplot_grid(p1,p2, ncol = 2, rel_heights = c(1,1))\n\nnrow(allfusions)\nallfusions=allfusions[which(allfusions$paircount &gt;= 2 &amp; allfusions$splitcount &gt;= 5),]\nnrow(allfusions)\n#write.table(allfusions, \"allfusions.txt\")\nwrite.table(allfusions, file=\"allfusions.txt\",row.names=F, col.names=T, quote=F, sep=\"\\t\")\n\n#Chimeraviz is an R package for visualizing fusions from RNA-seq data. The chimeraviz package has import functions built in for a variety of fusion-finder programs, but not for pizzly. We will have to load our own import function that you downloaded above:\n\n# Enter R, install and load chimeraviz \nR\nsource(\"https://bioconductor.org/biocLite.R\")\nbiocLite(\"chimeraviz\")\n# (if asked to update old packages, you can ignore- Update all/some/none? [a/s/n]:)\nlibrary(chimeraviz)\n\n# Use the pizzly importer script to import fusion data\nsource(\"./import_Pizzly.R\")\n#  You can view the function by calling it without variables\n#importPizzly\nfusions = importPizzly(\"./allfusions.txt\",\"hg38\")\nplot_circle(fusions)\n</code></pre>"},{"location":"lab_session/real_life_example_cli/","title":"Real life example of how to use command line features - vt 2023","text":"<p>This is an example of how to use various UNIX/bash/command line features for finding files, assigning variables, parsing and modifying text strings etc. The purpose in the example is to find files containing quality control metrics, and creating a table of certain variables from these files. </p> <p>This is not to be run as part of the lab, just a demonstration of a real life example when multiple various command line features are utilized.</p> <p>First, move to the directory of interest and find the files:</p> <p><pre><code>cd ~/Courses/cancer\\ bioinfo\\ course\\ v2/cmd_line_demo  # move to project directory\npwd  # check present working directory\nls -lhtr  # list files with detailed info (-l), human readable size (-h), in time order (-t), reveresed order with latest last (-r)\nfind . -name \"*hsmetrics.txt\"  # find the files in this directory that ends with \"txt\"\nfind . -name \"*hsmetrics.txt\" | wc -l  # count the number of found files\nfiles=($(find . -name \"*hsmetrics.txt\" | sort))  # assign a sorted list of the files to an array\necho ${files[@]}  # print all items of the array\necho ${#files[@]}  # print number of items of the array\n</code></pre> Output : </p> <pre><code>## /Users/rebber/Courses/cancer bioinfo course v2/cmd_line_demo\n## total 136\n## -rw-r--r--  1 rebber  staff   7.0K Apr 14 12:01 PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   7.8K Apr 14 12:01 PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   8.1K Apr 14 12:01 PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   5.3K Apr 14 12:01 PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##        4\n## ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## 4\n</code></pre> <p>Now we have the files of interest in the array <code>$files</code>. We can use this array for running further operations on the files.</p> <p>Inspect the files: <pre><code>ls -lh ${files[@]}  # list files with detailed view and human readable sizes\nwc -l ${files[@]}  # count number of rows of all files\nless -SN ${files[@]}  # open files in simple file viewer, with lines chopped (-S) and numbered (-N), next file can be accessed with :n, more options available in help page by pressing \"h\"\nhead -n10 ${files[0]}  # view 10 first lines of first file\ntail -n3 ${files[0]}  # view 3 last lines of first file\n</code></pre></p> <p>Output : <pre><code>## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##      213 ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##      213 ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##      213 ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##      213 ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n##      852 total\n## ## htsjdk.samtools.metrics.StringHeader\n## # CollectHsMetrics BAIT_INTERVALS=[/nfs/PROBIO/new_design_eval/targets/all_baits_twist.interval_list] BAIT_SET_NAME=PB TARGET_INTERVALS=[/nfs/PROBIO/new_design_eval/targets/all_customized_targets_twist.interval_list] INPUT=/dev/stdin OUTPUT=metrics/realignment_cust_targets/HsMetrics/MarkDuplicates/PB/PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt METRIC_ACCUMULATION_LEVEL=[ALL_READS] TMP_DIR=[/scratch/tmp/rebber] REFERENCE_SEQUENCE=/nfs/PROBIO/autoseq-genome/genome/human_g1k_v37_decoy.fasta    NEAR_DISTANCE=250 MINIMUM_MAPPING_QUALITY=20 MINIMUM_BASE_QUALITY=20 CLIP_OVERLAPPING_READS=true INCLUDE_INDELS=false COVERAGE_CAP=200 SAMPLE_SIZE=10000 ALLELE_FRACTION=[0.001, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.3, 0.5] VERBOSITY=INFO QUIET=false VALIDATION_STRINGENCY=STRICT COMPRESSION_LEVEL=5 MAX_RECORDS_IN_RAM=500000 CREATE_INDEX=false CREATE_MD5_FILE=false GA4GH_CLIENT_SECRETS=client_secrets.json USE_JDK_DEFLATER=false USE_JDK_INFLATER=false\n## ## htsjdk.samtools.metrics.StringHeader\n## # Started on: Tue May 26 17:12:11 CEST 2020\n## \n## ## METRICS CLASS picard.analysis.directed.HsMetrics\n## BAIT_SET BAIT_TERRITORY  BAIT_DESIGN_EFFICIENCY  ON_BAIT_BASES   NEAR_BAIT_BASES OFF_BAIT_BASES  PCT_SELECTED_BASES  PCT_OFF_BAIT    ON_BAIT_VS_SELECTED MEAN_BAIT_COVERAGE  PCT_USABLE_BASES_ON_BAIT    PCT_USABLE_BASES_ON_TARGET  FOLD_ENRICHMENT HS_LIBRARY_SIZE HS_PENALTY_10X  HS_PENALTY_20X  HS_PENALTY_30X  HS_PENALTY_40X  HS_PENALTY_50X  HS_PENALTY_100X TARGET_TERRITORY    GENOME_SIZE TOTAL_READS PF_READS    PF_BASES    PF_UNIQUE_READS PF_UQ_READS_ALIGNED PF_BASES_ALIGNED    PF_UQ_BASES_ALIGNED ON_TARGET_BASES PCT_PF_READS    PCT_PF_UQ_READS PCT_PF_UQ_READS_ALIGNED MEAN_TARGET_COVERAGE    MEDIAN_TARGET_COVERAGE  MAX_TARGET_COVERAGE ZERO_CVG_TARGETS_PCT    PCT_EXC_DUPE    PCT_EXC_ADAPTER PCT_EXC_MAPQ    PCT_EXC_BASEQ   PCT_EXC_OVERLAP PCT_EXC_OFF_TARGET  FOLD_80_BASE_PENALTY    PCT_TARGET_BASES_1X PCT_TARGET_BASES_2X PCT_TARGET_BASES_10X    PCT_TARGET_BASES_20X    PCT_TARGET_BASES_30X    PCT_TARGET_BASES_40X    PCT_TARGET_BASES_50X    PCT_TARGET_BASES_100X   AT_DROPOUT  GC_DROPOUT  HET_SNP_SENSITIVITY HET_SNP_Q   SAMPLE  LIBRARY READ_GROUP\n## PB   290831  0.91208 682291515   195630134   1507472292  0.368041    0.631959    0.777167    2346.006839 0.281894    0.169875    3085.649544     0   0   0   0   0   0   265261  3137454505  16795512    16795512    2420380851  16795512    16679404    2385393941  2385393941  411162960   1   1   0.993087    1550.031705 200 7219    0   0   0   0.101408    0.004983    0.298104    0.423367    7.750159    0.999612    0.999585    0.999582    0.999529    0.999457    0.999416    0.999359    0.99928 7.694876    0.179254    0.999597    34          \n## \n## ## HISTOGRAM java.lang.Integer\n## 199  0   0\n## 200  265028  0\n</code></pre></p> <p>Extract sample names from file names: <pre><code>ls -lh ${files[@]}\n\n# Method 1: all at once\n# print content of array with line break as separator to get one item on each line\n# use cut to get the second field of each line when separator is \"/\"\n# use cut to get the first field of each line when separator is \".\"\nprintf '%s\\n' ${files[@]} | cut -f 2 -d \"/\" | cut -f 1 -d \".\"\n\n# Method 2: one at a time in for loop\n# loop over each of the items in the $files array\n# get the basename for the file, i.e. just the file name not full path \n# use sed to remove everything matching pattern \".PB.hsmetrics.txt\" (actually replace it with nothing)\nfor f in ${files[@]}; do\nbasename $f | sed 's/.PB.hsmetrics.txt//g'\ndone\n</code></pre> Output: <pre><code>## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## PB-P-HD1-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD2-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD3-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD6-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD1-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD2-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD3-CFDNA-1811-KH20190925-PB20190927\n## PB-P-HD6-CFDNA-1811-KH20190925-PB20190927\n</code></pre> Other combinations of the tools in the two methods can also be used to get the same results.  For most tasks there are many possible ways to solve them on the command line. Just use whatever tools you are familiar with, and when there is something you can't figure out how to do, google offers tons of knowledge and ideas.</p> <p>Some more things that can be done with a for loop: <pre><code>for f in ${files[@]}; do\n# assign output of commands to a variable with the $() construction:\nsamp=$(basename $f | sed 's/.PB.hsmetrics.txt//g')\n# check conditions with if, here if $samp varaible matches some regex:\nif [[ $samp =~ HD1 ]]; then\necho \"This is HD1 sample:\"\nelif [[ $samp =~ HD2 ]]; then\necho \"This is HD2 sample:\"\nelse echo \"This another sample:\"\nfi\necho $samp\n# search for the sample name in the file content with grep, -c gives the count of hits\nhits=$(grep -c \"$samp\" $f)\necho \"File $f contains sample name $samp $hits times\"\necho  # empty line\ndone\n</code></pre> Output: <pre><code>## This is HD1 sample:\n## PB-P-HD1-CFDNA-1811-KH20190925-PB20190927\n## File ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt contains sample name PB-P-HD1-CFDNA-1811-KH20190925-PB20190927 1 times\n## \n## This is HD2 sample:\n## PB-P-HD2-CFDNA-1811-KH20190925-PB20190927\n## File ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt contains sample name PB-P-HD2-CFDNA-1811-KH20190925-PB20190927 1 times\n## \n## This another sample:\n## PB-P-HD3-CFDNA-1811-KH20190925-PB20190927\n## File ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt contains sample name PB-P-HD3-CFDNA-1811-KH20190925-PB20190927 1 times\n## \n## This another sample:\n## PB-P-HD6-CFDNA-1811-KH20190925-PB20190927\n## File ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt contains sample name PB-P-HD6-CFDNA-1811-KH20190925-PB20190927 1 times\n</code></pre> To create a table with the info from specific rows and columns we can use the awk program. In this case we will take columns MEAN_TARGET_COVERAGE (number 34) and FOLD_ENRICHMENT (number 13). The relevant data is on line 8, with the header on line 7.  NR: row number in total; FNR: row number in file. \"&gt;\" writes the output to a new file column -t aligns the output columns nicely <pre><code>awk -F \"\\t\" -v OFS=\"\\t\" 'NR==7 {print \"file\", $34, $13}; FNR==8 {print FILENAME, $34, $13}' \\\n${files[@]} &gt; qc_summary.txt\nls -lhtr \ncat qc_summary.txt\ncat qc_summary.txt | column -t\n</code></pre> Output: <pre><code>## total 144\n## -rw-r--r--  1 rebber  staff   7.0K Apr 14 12:01 PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   7.8K Apr 14 12:01 PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   8.1K Apr 14 12:01 PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   5.3K Apr 14 12:01 PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.slurm.log\n## -rw-r--r--  1 rebber  staff   4.1K Apr 14 12:01 PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt\n## -rw-r--r--  1 rebber  staff   382B Apr 14 17:56 qc_summary.txt\n## file MEAN_TARGET_COVERAGE    FOLD_ENRICHMENT\n## ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt 1550.031705 3085.649544\n## ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt 2283.624973 3389.760665\n## ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt 2599.327089 3688.749254\n## ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt 1173.933741 4831.030898\n## file                                                          MEAN_TARGET_COVERAGE  FOLD_ENRICHMENT\n## ./PB-P-HD1-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt  1550.031705           3085.649544\n## ./PB-P-HD2-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt  2283.624973           3389.760665\n## ./PB-P-HD3-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt  2599.327089           3688.749254\n## ./PB-P-HD6-CFDNA-1811-KH20190925-PB20190927.PB.hsmetrics.txt  1173.933741           4831.030898\n</code></pre></p>"},{"location":"lab_session/structural_variation/","title":"Calling structural variation","text":"<p>In this practical session we will look into structural rearrangements. The cfDNA sequencing was performed on commercial biomaterial from a patient with mCRPC. </p> <p>The in-solution hybridisation based capture assay, developed for the clinical trial (ProBio)[https://www.probiotrial.org] was applied.</p>"},{"location":"lab_session/structural_variation/#the-probio-assay","title":"The ProBio Assay","text":"<p>Assay explained: </p> <p>Go to the folder that contain the ProBio ctDNA data from the mCRPC commercial reference sample</p>"},{"location":"lab_session/structural_variation/#create-mini-bam-files-and-run-delly","title":"Create mini-bam files and run Delly","text":"<pre><code>cd ~/workspace/svs\n\n# First create smaller bam files for faster \n# processing since we already know where the structural variants are (on chromsome 10 and 21).\n\n# To create the smaller bam file we will be using bedtools, \n# one of the most useful tools in genomics. \n# The mapped sequencing is fed into intersectBed and all \n# reads overlapping with chr10 or chr21 are kept. The rest are discarded.\n# This is an example where the genome build needs to be known. \n# Are we using chr10 or just 10? How can you find out?\n# Where is this info to be found at the terminal?\n# Clue - its a very quick thing to check in the current folder.\n\n\n# Create a bed file that can be used to slice out the desired chromsomes\n\n#Create an empty file and then add tab.delimited lines\necho -ne \"\" &gt; chr.bed\necho -e \"10\\t1\\t135534747\" &gt;&gt; chr.bed\necho -e \"21\\t1\\t48129895\" &gt;&gt; chr.bed\n\n#check the file\ncat chr.bed\n\n#OBS - the commands below will be using nohup which\n# means that the commands will be sent and running\n# in the backgrond. You should immediately get the \n# prompt back after running nohup\n\n# This takes a couple of minutes\nBAM=./bams/RB-P-ResBio12-CFDNA-sample12-KH-C3-nodups.bam\nnohup intersectBed -a $BAM -b chr.bed &gt; T_mini.bam  2&gt; nohup_Tmini.log &amp;\n\nBAM2=./bams/PB-P-HD2-N-1811-KH-C3-nodups.bam\nnohup intersectBed -a $BAM2 -b chr.bed &gt; N_mini.bam 2&gt; nohup_Nmini.log &amp;\n\n#If you want to check what is running in the background type\n# \"htop\" and you will see an overview of the processes running\n# and the CPU load of your AWS instance.\n# Quit \"htop\" by pressing \"q\".\n\n#After the chr selection has been performed, check which chromosomes that re in the mini-bam file\nsamtools view N_mini.bam | cut -f3 | sort | uniq -c\nsamtools view T_mini.bam | cut -f3 | sort | uniq -c\n\n\n#Index the bam files\nsamtools index T_mini.bam\nsamtools index N_mini.bam\n\n# If you want to run dell on the entire bam file - takes one hour\n#nohup delly call -o ./RB-P-ResBio12-CFDNA-sample12-KH-C3_PB-P-HD2-N-1811-KH-C3_somatic_delly.bcf -g ./human_g1k_v37_decoy.fasta ./bams/RB-P-ResBio12-CFDNA-sample12-KH-C3-nodups.bam ./bams/PB-P-HD2-N-1811-KH-C3-nodups.bam &gt; delly_nohup.log &amp;\n\n#Now run Delly on the samll bam files, write to a .bcf file\nnohup delly call -o ./mini.bcf -g ./human_g1k_v37_decoy.fasta ./T_mini.bam ./N_mini.bam &gt; delly_nohup.log &amp;\n\n#On the entire bam file\n#bcftools convert -O v -o ./RB-P-ResBio12-CFDNA-sample12-KH-C3_PB-P-HD2-N-1811-KH-C3_somatic_delly.vcf ./RB-P-ResBio12-CFDNA-sample12-KH-C3_PB-P-HD2-N-1811-KH-C3_somatic_delly.bcf\n\nbcftools convert -O v -o ./mini.vcf ./mini.bcf\n</code></pre>"},{"location":"lab_session/structural_variation/#filtering-using-bcftools","title":"Filtering using bcftools","text":"<p>Now we will filter with some basic filters to remove false positive variants <pre><code>cd ~/workspace/svs/\n# RB-P-ResBio12-CFDNA-sample12-KH-C3_delly.vcf \n# to check total number of variants \n\n#If processing the entire bam files.\n#VCF_FILE=RB-P-ResBio12-CFDNA-sample12-KH-C3_PB-P-HD2-N-1811-KH-C3_somatic_delly.vcf\nVCF_FILE=mini.vcf\n\ngrep -v \"^#\" $VCF_FILE  -c\n# 49\n\n# 1. Filter only \"PASS\" variants \nbcftools view -f \"PASS\"  $VCF_FILE | grep -v \"^#\" -c #37\nbcftools view -f \"PASS\"  $VCF_FILE &gt; VCF_NEW.vcf\n\n# 2. Filter by INFO column PE - Paired-End read SR - Split Read support \nbcftools filter -i 'INFO/PE&gt;15 | INFO/SR&gt;15' VCF_NEW.vcf | grep -v \"^#\" -c #5\nbcftools filter -i 'INFO/PE&gt;15 | INFO/SR&gt;15' VCF_NEW.vcf &gt; VCF_NEW_FILTERED.vcf\n#have a look at the output\nless -SN VCF_NEW_FILTERED.vcf\n</code></pre></p>"},{"location":"lab_session/structural_variation/#download-and-view-the-structural-variants","title":"Download and view the structural variants","text":"<p>Now we will filter with some basic filters to remove false positive variants <pre><code># Download the output to your computer\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/svs/*.bam .\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/svs/*.bai .\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/svs/VCF_NEW_FILTERED.vcf .\n\n#Now we will download a couple of files with data supporting the variants from an internally developed tool for identifying structural rearrangements\nwget https://course-5534.s3.amazonaws.com/svs/evidence_svs.sort.bam\nwget https://course-5534.s3.amazonaws.com/svs/evidence_svs.sort.bam.bai\nwget https://course-5534.s3.amazonaws.com/svs/sample-cfdna-svcaller-DEL.gtf\nwget https://course-5534.s3.amazonaws.com/svs/sample-cfdna-svcaller-TRA.gtf\n</code></pre></p>"},{"location":"lab_session/structural_variation/#investigate-the-variants-in-igv","title":"Investigate the variants in IGV","text":"<ul> <li>Remember - these files are mapped to hg19</li> <li>Open the following files in IGV:<ul> <li>T_mini.bam</li> <li>N_mini.bam</li> <li>VCF_NEW_FILTERED.vcf</li> <li>evidence_svs.sort.bam<ul> <li>this bam file contains reads that are the output from a internally developed variant caller (SVcaller)</li> </ul> </li> <li>sample-cfdna-svcaller-DEL.gtf</li> <li>sample-cfdna-svcaller-TRA.gtf<ul> <li>The gtf-files mark the variants identified by SVcaller</li> </ul> </li> </ul> </li> </ul> <p>It is possible to select multiple files at the same time by pressing CMD (mac, use other key on windows, likely CTRL) </p> <p>You should now see something like this: </p> <p>Make sure you can see the soft-clipped reads: </p> <p>This sample was analysed using multiple structural variant callers and two variants are known</p> <ul> <li>One variant in PTEN</li> <li>One variant on chr21 resulting in the TMPRSS2-ERG gene fusion</li> <li>Write PTEN and press \"GO\" or just \"ENTER\"</li> </ul> <p></p> <ul> <li>Was the variant detected by Delly?</li> <li>Zoom in on the variant in PTEN </li> </ul> <p></p> <ul> <li>For the T_mini.bam<ul> <li>Color alignments \"no color\"</li> <li>Group alignments \"none\"</li> <li>Sort alignments \"base\"</li> </ul> </li> </ul> <p></p> <ul> <li>Right click on a read in the evicence_svs.sort.bam<ul> <li>Select \"View mate region in split screen\".</li> <li>This is a very useful way of seing both ends of a structural variant.</li> </ul> </li> </ul> <p></p> <p></p> <ul> <li> <p>Try to answer the question in the screenshot above.</p> </li> <li> <p>Now, have a look at the TMPRSS2-ERG fusion</p> <ul> <li>Go here: chr21:39,766,196-42,940,331</li> <li>Is this variant found by Delly? How do you see that?</li> <li>Zoom in on the left end, how many reads are supportin the variant according to Delly?<ul> <li>Click on the VCF file and look for PE (paired-end) and SR (split-read) info. </li> </ul> </li> </ul> </li> </ul>"},{"location":"lab_session/variant_calling/","title":"Calling germline- and somatic mutations","text":""},{"location":"lab_session/variant_calling/#germline-variants","title":"Germline variants","text":"<p>We will start by calling germline variants. For this purpose we will use the GATK HaplotypeCaller from the Broad Institute. We will use this tool on the germline DNA bam files that were processed yesterday. </p> <p>Active forum and tutorials for the GATK software suit is available here.</p> <p>The HaplotypeCaller can be run in single- or joint sample mode. The joint sample mode gives improved accuracy, especially in samples with low coverage. For simplicity, we will only use the single sample mode in this course. </p> <p>When haplotype caller detects variation it performes local realignment of the reads which improves variant accuracy, especially in e.g. repetitive regions. </p> <pre><code>#Make sure that $GATK_REGIONS is set correctly\necho $GATK_REGIONS\n\n#Create working dir for germline results if not already created\nmkdir -p ~/workspace/germline\ncd ~/workspace/germline\n\n#The argument --java-options '-Xmx12g' tells GATK to use 12GB of memory\n#To list the haplotype caller arguments run \ngatk --java-options '-Xmx12g' HaplotypeCaller --help\n\n#Call variants for exome data\ngatk --java-options '-Xmx12g' HaplotypeCaller \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.vcf \\\n--bam-output ~/workspace/germline/Exome_Norm_HC_out.bam $GATK_REGIONS\n</code></pre>"},{"location":"lab_session/variant_calling/#explore-the-ouput-files","title":"Explore the ouput files","text":""},{"location":"lab_session/variant_calling/#realigned-bam-file","title":"Realigned bam file","text":"<p>Navigate to the folder where you want to place the BAMs e.g. ~/course_bams</p> <pre><code>#Copy the haplotype caller realigned bam-file to your local computer \nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_out.ba* .\n</code></pre> <p>Launch IGV (remember hg38) - Open the bam files     - Exome_Norm_sorted_mrkdup_bqsr.bam (intput file to haplotype caller).     - Exome_Norm_HC_out.bam (realigned bam file by haplotype caller). - Navigate to chr17:76286974-76287203.     - Color alignments by: no color.     - Enable ruler in the menue bar.     - In the first base of the indel: Sort aligments according to base.     - Select squished view.     - Zoom out. - Try to answer the following:     - What characterises misaligned variant-supporting reads in non-realigned bam file?     - Are there any false positive variants take in the non-realigned bam file? - OBS: Leave IGV open, we will use it in the below section.</p> <p></p>"},{"location":"lab_session/variant_calling/#vcf-variant-call-format-file","title":"VCF (variant call format) file","text":"<pre><code>#Make sure you hare in the same directory as the output files\ncd ~/workspace/germline\n\n#List files\nls -halt\n\n#Open the VCF file\nless -SN Exome_Norm_HC_calls.vcf\n</code></pre> <p>The VCF file format is quite complex, the specification can be found here.</p> <p>Try to do the following:</p> <ul> <li>Identify the VCF header section.</li> <li>Identify the data column header.</li> <li>Find the variant on chr17 pos 106742 in the VCF file.<ul> <li>Lead, type /106742 and press ENTER</li> <li>What is the read depth in this position?</li> <li>Is it a high quality variant?</li> <li>Go go IGV in this position<ul> <li>Select expanded view.</li> <li>Color alignments by read strand.</li> <li>Sort alignments by base.</li> <li>Does the visual impression in IGV support the quality of the variant?</li> </ul> </li> </ul> </li> <li>Identify a way to quickly separate SNPs and INDELs in the VCF.</li> <li>Find the variant on chr17 pos 1787008 in the VCF file<ul> <li>Is this a high quality variant?</li> <li>Navgiate in IGV to the position.</li> <li>Try to explain the information in the VCF to what is shown in IGV, is it correct?</li> </ul> </li> </ul>"},{"location":"lab_session/variant_calling/#filtering-germline-variants","title":"Filtering germline variants","text":"<p>As noted above, the raw output of GATK HaplotypeCaller will include many variants with varying degrees of quality. For various reasons we might wish to further filter these to a higher confidence set of variants. The hard-filtering approached applied here is futher described in detail at the GATK webside. </p>"},{"location":"lab_session/variant_calling/#separate-snps-from-indels-in-the-variant-call-set","title":"Separate SNPs from Indels in the variant call set","text":"<p>First, we will separate out the SNPs and Indels from the VCF into new separate VCFs. Note that the variant type (SNP, INDEL, MIXED, etc) is not stored explicitly in the vcf but instead inferred from the genotypes. We will use a versatile GATK tool called <code>SelectVariants</code>. This command can be used for all kinds of simple filtering or subsetting purposes. We will run it twice to select by variant type, once for SNPs, and then again for Indels, to produce two new VCFs.</p> <pre><code>cd ~/workspace/germline/\ngatk --java-options '-Xmx12g' SelectVariants \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type SNP \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf\ngatk --java-options '-Xmx12g' SelectVariants \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type INDEL \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf\n</code></pre>"},{"location":"lab_session/variant_calling/#apply-filters-to-the-snp-and-indel-call-sets","title":"Apply filters to the SNP and Indel call sets","text":"<p>Next, we will perform so-called hard-filtering by applying a number of cutoffs. Multiple filters can be combined arbitrarily. Each can be given its own name so that you can later determine which one or more filters a variant fails. Visit the GATK documentation on hard-filtering to learn more about the following hard filtering options. Notice that different filters and cutoffs are recommended for SNVs and Indels. This is why we first split them into separate files.</p> <pre><code>cd ~/workspace/germline/\ngatk --java-options '-Xmx12g' VariantFiltration \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf \\\n--filter-expression \"QD &lt; 2.0\" --filter-name \"QD_lt_2\" \\\n--filter-expression \"FS &gt; 60.0\" --filter-name \"FS_gt_60\" \\\n--filter-expression \"MQ &lt; 40.0\" --filter-name \"MQ_lt_40\" \\\n--filter-expression \"MQRankSum &lt; -12.5\" --filter-name \"MQRS_lt_n12.5\" \\\n--filter-expression \"ReadPosRankSum &lt; -8.0\" --filter-name \"RPRS_lt_n8\" \\\n--filter-expression \"SOR &gt; 3.0\" --filter-name \"SOR_gt_3\" \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf\n\ngatk --java-options '-Xmx12g' VariantFiltration \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf \\\n--filter-expression \"QD &lt; 2.0\" --filter-name \"QD_lt_2\" \\\n--filter-expression \"FS &gt; 200.0\" --filter-name \"FS_gt_200\" \\\n--filter-expression \"ReadPosRankSum &lt; -20.0\" --filter-name \"RPRS_lt_n20\" \\\n--filter-expression \"SOR &gt; 10.0\" --filter-name \"SOR_gt_10\" \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf\n</code></pre> <p>Notice that warnings are given with regards to MQRankSum and ReadPosRankSum, these are only calculated if the site is called as heterozygous. E.g. for this site (inspect in IGV and in the VCF file):</p> <ul> <li>chr17:1067361</li> </ul> <p>This was discussed at the GATK forum.</p> <p>The variants are now marked as PASS or or filter-name. Use <code>grep -v</code> and <code>head</code> to skip past all the VCF header lines and view the first few records. </p> <p><pre><code>#inspect the 10 first variants that did not pass\ngrep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -vP \"\\tPASS\\t\" | head -10\n#inspect the 10 first variants that did pass\ngrep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -P \"\\tPASS\\t\" | head -10\n</code></pre> Try to do the following:</p> <ul> <li>Count the number of variants that failed and passed filtering.</li> </ul>"},{"location":"lab_session/variant_calling/#merge-filtered-snp-and-indel-vcfs-back-together","title":"Merge filtered SNP and INDEL vcfs back together","text":"<pre><code>gatk --java-options '-Xmx12g' MergeVcfs \\\n-I ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf \\\n-I ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf\n\n#Nbr of variants in the SNP file\ngrep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | wc -l\n\n#Nbr of variants in the INDEL file\ngrep -v \"##\" Exome_Norm_HC_calls.indels.filtered.vcf | wc -l\n\n#Nbr of variants in the merged file\ngrep -v \"##\" Exome_Norm_HC_calls.filtered.vcf | wc -l\n</code></pre>"},{"location":"lab_session/variant_calling/#extract-pass-variants-only","title":"Extract PASS variants only","text":"<p>It would also be convenient to have a vcf with only passing variants. For this, we can go back the <code>GATK SelectVariants</code> tool. This will be run much as above except with the <code>--exlude-filtered</code> option instead of <code>-select-type</code>.</p> <pre><code>gatk --java-options '-Xmx12g' SelectVariants \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf \\\n-O ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\\n--exclude-filtered\n</code></pre>"},{"location":"lab_session/variant_calling/#perform-annotation-of-filtered-variants","title":"Perform annotation of filtered variants","text":"<p>Now that we have high-confidence, filtered variants, we want to start understanding which of these variants might be clinically or biologically relevant. Ensembl's Variant Effect Predictor (VEP) annotation software is a powerful tool for annotating variants with a great deal of biological features. This includes such information as protein consequence (non-coding or coding), population frequencies, links to external databases, various scores designed to estimate the importance of individual variants on protein function, and much more.</p> <p>Note, the first time you run VEP it will create a fasta index for the reference genome in VEP cache. Therefore, it will take longer to run that first time but should speed up substantially for subsequent runs on files with similar numbers of variants.</p> <pre><code>#VEP annotate hard-filtered exome results\n\n####### Already done code below\n#cd ~/workspace/vep_cache/\n#wget https://github.com/Ensembl/VEP_plugins/archive/refs/heads/release/104.zip\n#unzip 104.zip \n#ls -halt ~/workspace/vep_cache/\n#Already done\n#mv VEP_plugins-release-104 Plugins\n#ls -halt ~/workspace/vep_cache/Plugins\n\n#get annotation fasta for homo_sapiens \n#cd ~/workspace/vep_cache/homo_sapiens\n#Already done\n#wget ftp://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.toplevel.fa.gz\n\n####### Start here\ncd ~/workspace/germline\n#Remember, we are using the REF ~/workspace/inputs/references/genome/ref_genome.fa which only contains chr6 and chr17.\nnohup vep --cache --dir_cache ~/workspace/vep_cache \\\n--dir_plugins ~/workspace/vep_cache/Plugins \\\n--fasta ~/workspace/inputs/references/genome/ref_genome.fa \\\n--fork 8 --assembly=GRCh38 --offline --vcf \\\n-i ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\\n-o ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf \\\n--check_existing --total_length --allele_number --no_escape --everything \\\n--use_given_ref --force_overwrite --coding_only &amp;\n\n#Open the VEP vcf, row 39 and 40, contains information about the VEP annotation.\nless -SN Exome_Norm_HC_calls.filtered.PASS.vep.vcf\n#Or just grep the VEP info.\ngrep \"##\" Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"VEP\" | less -SN\n#Copy the html VEP summary to your local computer\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf_summary.html .\n</code></pre> <p>Open the HTML file.</p> <p>Answer the following:</p> <ul> <li>How many variants were processed by VEP?</li> <li>What is the most common kind of consequence?</li> <li>Why are only variants the coding regions detected?</li> <li>Open the Exome_Norm_HC_out.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV <ul> <li>Find a missense variant and examine it in IGV, does the variant look real?</li> <li>E.g. take chr17 744946<ul> <li>Find the variant in the VEP VCF as well as IGV.</li> </ul> </li> </ul> </li> <li>VEP assigns each variant a consequence types, as described here.<ul> <li>What is the difference between splice region variants and splice donor/acceptor variants?</li> <li>The --pick option is used, as described here, do you think it makes sense to use? </li> </ul> </li> </ul>"},{"location":"lab_session/variant_calling/#somatic-variants","title":"Somatic variants","text":"<p>We will run multiple variant callers and merging the callset.</p>"},{"location":"lab_session/variant_calling/#varscan","title":"Varscan","text":"<p>First off is VARSCAN, that employs a robust heuristic/statistic approach to call variants that meet desired thresholds for read depth, base quality, variant allele frequency, and statistical significance.</p> <p>As seen below varscan uses the mpileup command from samtools. What the mpileup command does can be explored here</p> <pre><code>mkdir -p ~/workspace/somatic/varscan\ncd ~/workspace/somatic/varscan\n\n#Have a look at the input data to the varscan caller\nsamtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\\n--no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\\n~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam | less -SN\n\n# Run varscan\njava -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar somatic \\\n&lt;(samtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\\n--no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\\n~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam) \\\n~/workspace/somatic/varscan/exome --min-var-freq 0.05 --mpileup 1 --output-vcf\n\nls -halt\n\njava -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\\nexome.snp.vcf exome.snp --min-tumor-freq 0.05 --max-normal-freq 0.01\n\njava -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\\nexome.indel.vcf exome.indel --min-tumor-freq 0.05 --max-normal-freq 0.01\n\nfind ~/workspace/somatic/varscan -name '*.vcf' -exec bgzip -f {} \\;\nfind ~/workspace/somatic/varscan -name '*.vcf.gz' -exec tabix -f {} \\;\n\ngatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V exome.snp.Somatic.vcf.gz --mask exome.snp.Somatic.hc.vcf.gz \\\n--mask-name \"processSomatic\" --filter-not-in-mask -O exome.snp.Somatic.hc.filter.vcf.gz\n\ngatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-V exome.indel.Somatic.vcf.gz --mask exome.indel.Somatic.hc.vcf.gz \\\n--mask-name \"processSomatic\" --filter-not-in-mask -O exome.indel.Somatic.hc.filter.vcf.gz\n\nbcftools concat -a -o exome.vcf.gz -O z exome.snp.Somatic.hc.filter.vcf.gz exome.indel.Somatic.hc.filter.vcf.gz\ntabix -f ~/workspace/somatic/varscan/exome.vcf.gz\n</code></pre>"},{"location":"lab_session/variant_calling/#strelka","title":"Strelka","text":"<p>Now we are going to run the second variant caller, STRELKA. Strelka calls germline and somatic small variants from mapped sequencing reads and is optimized for rapid clinical analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs. Both germline and somatic callers include a final empirical variant rescoring step using a random forest model to reflect numerous features indicative of call reliability which may not be represented in the core variant calling probability model.</p> <pre><code>mkdir -p ~/workspace/somatic/strelka/exome\ncd ~\n\nsource activate strelka-env\n~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py \\\n--normalBam=workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n--tumorBam=workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\\n--referenceFasta=workspace/inputs/references/genome/ref_genome.fa \\\n--exome --runDir=workspace/somatic/strelka/exome\n\n#Please specify according to the number of cpus available or how many you would like to allocate to this job. In this case, four were given.\n# Runtime: ~ 3min\npython2 ~/workspace/somatic/strelka/exome/runWorkflow.py -m local -j 8\n\nconda deactivate\n\ncd ~/workspace/somatic/strelka/exome/results/variants\nzcat somatic.snvs.vcf.gz | \\\nawk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\"&gt;\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\\n&gt; somatic.snvs.gt.vcf\nzcat somatic.indels.vcf.gz | \\\nawk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\"&gt;\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\\n&gt; somatic.indels.gt.vcf\nfind ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf\" -exec bgzip -f {} \\;\nfind ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf.gz\" -exec tabix -f {} \\;\n\nbcftools concat -a -o exome.vcf.gz -O z somatic.snvs.gt.vcf.gz somatic.indels.gt.vcf.gz\n\ntabix exome.vcf.gz\n</code></pre>"},{"location":"lab_session/variant_calling/#mutect2","title":"Mutect2","text":"<p>Last up is MuTect2. MuTect2 is a somatic SNP and indel caller that combines the DREAM challenge-winning somatic genotyping engine of the original MuTect (Cibulskis et al., 2013) with the assembly-based machinery of HaplotypeCaller.</p>"},{"location":"lab_session/variant_calling/#exome-data-commands","title":"Exome data commands","text":"<pre><code>#Obtaining germline resource from GATK\ncd ~/workspace/inputs/references\n#Already done\n#gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz .\n#gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz.tbi .\n\nmkdir -p ~/workspace/somatic/mutect\ncd ~/workspace/somatic/mutect\n\n#Creating a panel of normals\n# Runtime: ~ 17min\nnohup gatk --java-options \"-Xmx12G\" Mutect2 \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n-tumor-sample HCC1395BL_DNA -O Exome_Norm_PON.vcf.gz &amp;\n\n##### WAIT FOR THIS NOHUP STEP TO FINISH BEFORE CONTINUING #####\n##### WAIT FOR THIS NOHUP STEP TO FINISH BEFORE CONTINUING #####\n##### WAIT FOR THIS NOHUP STEP TO FINISH BEFORE CONTINUING #####\n\n#Running Mutect2 Using latest version of GATK\n# Runtime: ~20m\nnohup gatk --java-options \"-Xmx12G\" Mutect2 \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa \\\n-I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\\n-tumor HCC1395_DNA -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\\n-normal HCC1395BL_DNA --germline-resource ~/workspace/inputs/references/af-only-gnomad.hg38.vcf.gz \\\n--af-of-alleles-not-in-resource 0.00003125 --panel-of-normals \\\n~/workspace/somatic/mutect/Exome_Norm_PON.vcf.gz -O ~/workspace/somatic/mutect/exome.vcf.gz \\\n-L chr6 -L chr17 &amp;\n\n##### WAIT FOR NOHUP FOR THIS STEP TO FINISH BEFORE CONTINUING #####\n##### WAIT FOR NOHUP FOR THIS STEP TO FINISH BEFORE CONTINUING #####\n##### WAIT FOR NOHUP FOR THIS STEP TO FINISH BEFORE CONTINUING #####\n\n\n# Filtering mutect variants\ngatk --java-options \"-Xmx12G\" FilterMutectCalls -V ~/workspace/somatic/mutect/exome.vcf.gz \\\n-O ~/workspace/somatic/mutect/exome_filtered.vcf.gz \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa\n\n#Running mutect2 using gatk version 3.6\n#java -Xmx12g -jar /usr/local/bin/GenomeAnalysisTK.jar -T MuTect2 --disable_auto_index_creation_and_locking_when_reading_rods -R ~/workspace/data/raw_data/references/ref_genome.fa -I:tumor ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Tumor_sorted_mrkdup_bqsr.bam -I:Normal ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Norm_sorted_mrkdup_bqsr.bam --dbsnp ~/workspace/data/raw_data/references/Homo_sapiens_assembly38.dbsnp138.vcf.gz --cosmic ~/workspace/data/raw_data/references/Cosmic_v79.dictsorted.vcf.gz -o ~/workspace/data/results/somatic/mutect/exome.vcf.gz -L ~/workspace/data/results/inputs/SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.interval_list\n\necho ~/workspace/somatic/mutect/exome_filtered.vcf.gz &gt; ~/workspace/somatic/mutect/exome_vcf.fof\nbcftools concat --allow-overlaps --remove-duplicates \\\n--file-list ~/workspace/somatic/mutect/exome_vcf.fof --output-type z \\\n--output ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz\n\ntabix ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz\n</code></pre>"},{"location":"lab_session/variant_calling/#now-we-are-going-to-merge-the-variants-detected-from-all-three-variant-callers","title":"Now we are going to merge the variants detected from all three variant callers","text":"<p>The reason for merging is that the variant callers are working differently internally to identify somatic alterations, with different strengths and weaknesses. With outputs from all three algorithms, we can now merge the variants to generate a comprehensive list of detected variants:</p> <pre><code># Unzip the vcf.gz files before combining Variants\ncd ~/workspace/somatic\nls -halt ~/workspace/somatic/varscan/exome.vcf.gz\nls -halt ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz\nls -halt ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz\n\ngunzip ~/workspace/somatic/varscan/exome.vcf.gz\ngunzip ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz\ngunzip ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz\n\n#Need to change header sample names in vcf file produced by mutect2 in order to combine variants with those from other algorithms\nsed -i 's/HCC1395BL_DNA/NORMAL/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf\nsed -i 's/HCC1395_DNA/TUMOR/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf\n\n# (UNIQUIFY command) java -Xmx4g -jar /usr/local/bin/GenomeAnalysisTK.jar -T CombineVariants -R ~/workspace/data/raw_data/references/ref_genome.fa -genotypeMergeOptions UNIQUIFY --variant:varscan ~/workspace/data/results/somatic/varscan/exome.vcf --variant:strelka ~/workspace/data/results/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/data/results/somatic/mutect/new_gatk_files/exome.vcf -o ~/workspace/data/results/somatic/exome.unique.vcf.gz\n#java -Xmx24g -jar ~/workspace/bin/GenomeAnalysisTK.jar -T  CombineVariants -R ~/workspace/inputs/references/genome/ref_genome.fa -genotypeMergeOptions PRIORITIZE --rod_priority_list mutect,varscan,strelka --variant:varscan ~/workspace/somatic/varscan/exome.vcf --variant:strelka ~/workspace/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/somatic/mutect/exome.vcf -o ~/workspace/somatic/exome.merged.vcf.gz\n\njava -Xmx12g -jar ~/workspace/bin/picard.jar MergeVcfs \\\n-I ~/workspace/somatic/varscan/exome.vcf \\\n-I ~/workspace/somatic/strelka/exome/results/variants/exome.vcf \\\n-I ~/workspace/somatic/mutect/mutect_exome_filtered.vcf \\\n-O ~/workspace/somatic/exome.merged.vcf\n\nbgzip -c ~/workspace/somatic/exome.merged.vcf &gt; ~/workspace/somatic/exome.merged.vcf.gz\ntabix -p vcf ~/workspace/somatic/exome.merged.vcf.gz\n</code></pre>"},{"location":"lab_session/variant_calling/#left-align-and-trim","title":"Left Align and Trim","text":"<p>The reason for left align the variants and trim then is explained here. <pre><code>cd ~/workspace/somatic/\n\ngatk --java-options \"-Xmx12G\" LeftAlignAndTrimVariants \\\n-V ~/workspace/somatic/exome.merged.vcf.gz \\\n-O exome.merged.leftalignandtrim.vcf \\\n-R ~/workspace/inputs/references/genome/ref_genome.fa\n</code></pre> Note that when running on chromosome 6 and 17 merged variants file, this gave 0 variants aligned.</p>"},{"location":"lab_session/variant_calling/#we-will-split-multi-allelic-variants-into-multiple-records","title":"We will split multi-allelic variants into multiple records","text":"<pre><code>cd ~/workspace/somatic/\n\nvt decompose -s ~/workspace/somatic/exome.merged.leftalignandtrim.vcf \\\n-o ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf\n</code></pre>"},{"location":"lab_session/variant_calling/#basic-filtering-on-somatic-variants","title":"Basic Filtering on Somatic Variants","text":"<p>First, let's do a basic filtering for <code>PASS</code> only variants on our merged and normalized vcf file: <pre><code>cd ~/workspace/somatic\ngatk --java-options \"-Xmx12G\" SelectVariants -R ~/workspace/inputs/references/genome/ref_genome.fa \\\n--exclude-filtered -V ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf \\\n-O ~/workspace/somatic/exome.merged.norm.pass_only.vcf\n</code></pre></p>"},{"location":"lab_session/variant_calling/#annotation-with-vep","title":"Annotation with VEP.","text":"<p>Again, we will use VEP to annotate the somatic variants as we did for the germline variants. </p> <pre><code>cd ~/workspace/somatic\n# Runtime: ~4min\n\n#ssh -o ServerAliveInterval=300 -i course_EC2_01.pem ubuntu@ec2-52-23-206-90.compute-1.amazonaws.com\n#source .bashrc\n#cd workspace/somatic/\nnohup vep --cache --dir_cache ~/workspace/vep_cache \\\n--dir_plugins ~/workspace/vep_cache/Plugins \\\n--fasta ~/workspace/inputs/references/genome/ref_genome.fa --fork 8 \\\n--assembly=GRCh38 --offline --vcf -i ~/workspace/somatic/exome.merged.norm.pass_only.vcf \\\n-o ~/workspace/somatic/exome.merged.norm.annotated.vcf \\\n--check_existing --total_length --allele_number  --no_escape --everything \\\n--use_given_ref --force_overwrite &amp;\n\n\n#Copy the html VEP summary to your local computer\nscp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/somatic/exome.merged.norm.annotated.vcf_summary.html .\n</code></pre> <p>Open the HTML file.</p> <p>Answer the following:</p> <ul> <li>How many variants were processed by VEP?</li> <li>Any diffrence in the distribution of variants vs. the germline variants? </li> <li>The variant categories e.g. intron_variant and regulatory_region_variant should be approached carefully, why?</li> </ul>"},{"location":"lab_session/variant_calling/#inspecting-variants-in-igv","title":"Inspecting variants in IGV","text":"<ul> <li> <p>Download the procdessed DNA bam files to your local machine if you have not done so yet, see the session \"Introduction to IGV\"</p> </li> <li> <p>Open the Exome_Tumor_sorted_mrkdup_bqsr.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV.</p> </li> <li> <p>Keep the VCF file open in a terminal window on AWS</p> </li> <li>Try to find a relevant variant in TP53 in the VCF file on AWS <pre><code>#Can be done in many ways, this is just an example.\ncd ~/workspace/somatic\ngrep \"TP53\" exome.merged.norm.annotated.vcf | less -SN\n#### Or \nless -SN exome.merged.norm.annotated.vcf\n#In the less window type /TP53\n</code></pre></li> <li>Why is the TP53 variant in three rows in the VCF file? <ul> <li>lead: how many somatic variant callers were run?</li> </ul> </li> <li> <p>Go to the position (chr17 7675088) on IGV in your local machine </p> <ul> <li>Is it present in the normal DNA?<ul> <li>Do you still think it is valid? How come the variant can be present in the germline DNA?</li> </ul> </li> </ul> </li> <li> <p>Do the same thing for a stop_gained and a frameshift variant <pre><code>cd ~/workspace/somatic\ngrep \"stop_gained\" exome.merged.norm.annotated.vcf | less -SN\n</code></pre></p> </li> <li>The top variant is seen only once, this is just identified correctly by one caller because it changes two bases in a row<ul> <li>open chr6 1930220 in IGV</li> </ul> </li> </ul> <p></p> <ul> <li>let us convert the vcf-file to a MAF file format<ul> <li>note all information is not kept but it simplifies looking at the variants <pre><code>cd ~/workspace/somatic\nperl ~/workspace/bin/mskcc-vcf2maf/vcf2maf.pl --input-vcf ~/workspace/somatic/exome.merged.norm.annotated.vcf --output-maf ~/workspace/somatic/exome.merged.norm.annotated.maf  --tumor-id TUMOR --normal-id NORMAL --inhibit-vep --ref-fasta ~/workspace/inputs/references/genome/ref_genome.fa\n\nless -SN ~/workspace/somatic/exome.merged.norm.annotated.maf\n\n#Count the number of variant types\ncut -f9 ~/workspace/somatic/exome.merged.norm.annotated.maf | sort | uniq -c\n#As you see the variant nomenclature is not the same in VCF and MAF\n\ngrep Nonsense_Mutation ~/workspace/somatic/exome.merged.norm.annotated.maf | less -SN </code></pre></li> </ul> </li> <li> <p>Have a look at the nosense variant in CCDC40 in IGV</p> <ul> <li>chr17 80050160</li> <li>What is the VAF?</li> <li>What is the VAF of the TP53 variant?</li> <li>Reflections?<ul> <li>Lead: clonality</li> </ul> </li> </ul> </li> <li> <p>Can you find a variant in BRCA1? </p> <ul> <li>What is the impact? Is it relevant?</li> </ul> </li> <li> <p>There was a BRCA 1 variant in the germline that we did not dicsuss earlier <pre><code>#Let us grep \"BRCA1\" and send the output to another grep command and take the \"HIGH\" impact variants\ngrep BRCA1 ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"HIGH\" | less -SN </code></pre></p> </li> <li>let us inspect this variant in IGV, why is one allele missing in the tumor?<ul> <li>This is the variant location: chr17   43057078  </li> </ul> </li> </ul>"},{"location":"lectures/lectures/","title":"Lectures","text":"Day 1 : Introduction to cancer genome and mutational processes <ol> <li>Course overview PDF</li> <li>An introduction to the cancer genome and mutational processes in cancer. PDF</li> <li>Practical considerations for performing cancer genomics. PDF</li> <li>How to connect to AWS server. PDF</li> </ol> Day 2 : Liquid Biopsies <ol> <li>Liquid biopsies PDF</li> <li>The clinical impact of analysing the cancer genome. PDF</li> <li>Lab Introduction. PDF</li> </ol> Day 3 : SNV calling <ol> <li>QC metrics for DNA sequencing. PDF</li> <li>Somatic and germline variant calling. PDF</li> <li>RNA sequencing. PDF</li> </ol> Day 4 : GSR calling &amp; RNA-Seq <ol> <li>Copy number analysis. PDF</li> <li>Structural variant analysis using NGS. PDF</li> <li>RNA sequencing etc.Lab intro. PDF</li> </ol> Day 5 : Manual curation and interpretation,  Bioinformatics pipelines <ol> <li>Bioinformatics pipelines &amp; HTC computing environments. PDF</li> <li>Clinical trials. How to curate somatic- and germline variation for clinical use. PDF</li> <li>Interpretation of Genomics - I PDF</li> <li>Interpretation of Genomics - II PDF</li> <li>Interpretation of Genomics - III PDF</li> <li>Interpretation of Genomics - IV PDF</li> </ol>"}]}