{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Clinical Cancer Genomics Course \u00b6 This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory. The setup and practical exercises are a modified version of an existing course generously shared under an MIT and Creative Commons license. This is the first version of the Clinical Cancer Genomics course, later updated iterations of the course will be shared according to the same principle. Learning Outcomes \u00b6 At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation. Contents of the course \u00b6 An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use. Literature and other teaching material \u00b6 Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note: Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam. Additional Information \u00b6 2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Course Details"},{"location":"#welcome-to-clinical-cancer-genomics-course","text":"This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory. The setup and practical exercises are a modified version of an existing course generously shared under an MIT and Creative Commons license. This is the first version of the Clinical Cancer Genomics course, later updated iterations of the course will be shared according to the same principle.","title":"Welcome to Clinical Cancer Genomics Course"},{"location":"#learning-outcomes","text":"At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation.","title":"Learning Outcomes"},{"location":"#contents-of-the-course","text":"An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use.","title":"Contents of the course"},{"location":"#literature-and-other-teaching-material","text":"Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note: Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam.","title":"Literature and other teaching material"},{"location":"#additional-information","text":"2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Additional Information"},{"location":"annotations/","text":"HUMAN GENOME ANNOTATION FILES \u00b6 Files for annotation of variation in the human genome \u00b6 cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh Index the variation files \u00b6 cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz Interval files and coordinates for the exome sequencing assay \u00b6 # change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed Calculate the size of the SeqCap v3 exome \u00b6 #This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed Create an inverval list for the exome bed files \u00b6 # first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"HUMAN GENOME ANNOTATION FILES"},{"location":"annotations/#human-genome-annotation-files","text":"","title":"HUMAN GENOME ANNOTATION FILES"},{"location":"annotations/#files-for-annotation-of-variation-in-the-human-genome","text":"cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh","title":"Files for annotation of variation in the human genome"},{"location":"annotations/#index-the-variation-files","text":"cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz","title":"Index the variation files"},{"location":"annotations/#interval-files-and-coordinates-for-the-exome-sequencing-assay","text":"# change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed","title":"Interval files and coordinates for the exome sequencing assay"},{"location":"annotations/#calculate-the-size-of-the-seqcap-v3-exome","text":"#This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed","title":"Calculate the size of the SeqCap v3 exome"},{"location":"annotations/#create-an-inverval-list-for-the-exome-bed-files","text":"# first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Create an inverval list for the exome bed files"},{"location":"installation/","text":"INSTALLATION NOTES \u00b6 This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies. Prepare for installation \u00b6 For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin Install Samtools \u00b6 Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help Install PICARD \u00b6 PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h Install BWA \u00b6 BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa Install GATK 4 \u00b6 GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate Install VEP 93.4 \u00b6 VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help Install Varscan \u00b6 Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar Install BCFtools \u00b6 BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h Install Strelka \u00b6 Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h Install Sambamba \u00b6 Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba Install HISAT2 \u00b6 HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help Install StringTie \u00b6 StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h Install Gffcompare \u00b6 Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare Install R \u00b6 R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")' Install copyCat \u00b6 copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")' Install CNVnator \u00b6 CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator Install CNVkit \u00b6 CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate Install Kallisto \u00b6 Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto Install Pizzly \u00b6 Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help Manta \u00b6 Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate mosdepth \u00b6 mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth bam-readcount \u00b6 bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount vt \u00b6 vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt vcf-annotation-tools \u00b6 VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h Install seqtk \u00b6 Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk Activate the tools environment \u00b6 source .bashrc","title":"INSTALLATION NOTES"},{"location":"installation/#installation-notes","text":"This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies.","title":"INSTALLATION NOTES"},{"location":"installation/#prepare-for-installation","text":"For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin","title":"Prepare for installation"},{"location":"installation/#install-samtools","text":"Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help","title":"Install Samtools"},{"location":"installation/#install-picard","text":"PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h","title":"Install PICARD"},{"location":"installation/#install-bwa","text":"BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa","title":"Install BWA"},{"location":"installation/#install-gatk-4","text":"GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate","title":"Install GATK 4"},{"location":"installation/#install-vep-934","text":"VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help","title":"Install VEP 93.4"},{"location":"installation/#install-varscan","text":"Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar","title":"Install Varscan"},{"location":"installation/#install-bcftools","text":"BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h","title":"Install BCFtools"},{"location":"installation/#install-strelka","text":"Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h","title":"Install Strelka"},{"location":"installation/#install-sambamba","text":"Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba","title":"Install Sambamba"},{"location":"installation/#install-hisat2","text":"HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help","title":"Install HISAT2"},{"location":"installation/#install-stringtie","text":"StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h","title":"Install StringTie"},{"location":"installation/#install-gffcompare","text":"Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare","title":"Install Gffcompare"},{"location":"installation/#install-r","text":"R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")'","title":"Install R"},{"location":"installation/#install-copycat","text":"copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")'","title":"Install copyCat"},{"location":"installation/#install-cnvnator","text":"CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator","title":"Install CNVnator"},{"location":"installation/#install-cnvkit","text":"CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate","title":"Install CNVkit"},{"location":"installation/#install-kallisto","text":"Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto","title":"Install Kallisto"},{"location":"installation/#install-pizzly","text":"Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help","title":"Install Pizzly"},{"location":"installation/#manta","text":"Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate","title":"Manta"},{"location":"installation/#mosdepth","text":"mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth","title":"mosdepth"},{"location":"installation/#bam-readcount","text":"bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount","title":"bam-readcount"},{"location":"installation/#vt","text":"vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt","title":"vt"},{"location":"installation/#vcf-annotation-tools","text":"VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h","title":"vcf-annotation-tools"},{"location":"installation/#install-seqtk","text":"Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk","title":"Install seqtk"},{"location":"installation/#activate-the-tools-environment","text":"source .bashrc","title":"Activate the tools environment"},{"location":"references/","text":"HUMAN GENOME REFERENCE FILES \u00b6 Download a refernce file for human genome \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\" Split the long fasta by chromosome \u00b6 # Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/ Explore the contents of the reference genome file \u00b6 # View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"? Index the fasta files \u00b6 # first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa Transcriptome reference files \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf Explore the contents of the transcriptome reference files \u00b6 #How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome \u00b6 cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome Create a reference transcriptome index for use with Kallisto \u00b6 cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"HUMAN GENOME REFERENCE FILES"},{"location":"references/#human-genome-reference-files","text":"","title":"HUMAN GENOME REFERENCE FILES"},{"location":"references/#download-a-refernce-file-for-human-genome","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\"","title":"Download a refernce file for human genome"},{"location":"references/#split-the-long-fasta-by-chromosome","text":"# Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/","title":"Split the long fasta by chromosome"},{"location":"references/#explore-the-contents-of-the-reference-genome-file","text":"# View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"?","title":"Explore the contents of the reference genome file"},{"location":"references/#index-the-fasta-files","text":"# first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa","title":"Index the fasta files"},{"location":"references/#transcriptome-reference-files","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf","title":"Transcriptome reference files"},{"location":"references/#explore-the-contents-of-the-transcriptome-reference-files","text":"#How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c","title":"Explore the contents of the transcriptome reference files"},{"location":"references/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","text":"cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome"},{"location":"references/#create-a-reference-transcriptome-index-for-use-with-kallisto","text":"cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Create a reference transcriptome index for use with Kallisto"},{"location":"schedule/","text":"Day 1 : 2021-01-24 - Monday \u00b6 Time Topic Responsible Location 9:00-9:45 An introduction to the cancer genome and mutational processes in cancer. Johan Zoom 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer (conti). Johan Zoom 10:45-11:00 Break 11:00-11:45 Practical considerations for performing cancer genomics Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Basic unix AWK Investigate the workspace directories, raw data and install R, R-packages ggplot tutorial Johan & Rebecka & Sarath Zoom Day 2 : 2021-01-25 - Tuesday \u00b6 Time Topic Responsible Location 9:00-9:45 Liquid biopsies Johan Zoom 9:45-10:00 Break 10:00-10:45 The clinical impact of analysing the cancer genome. Felix Zoom 10:45-11:00 Break 11:00-11:45 Lab Introduction Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Looking into the human genome reference. Annotatation files needed for bioinformatic processing. Executing commands for processing and checking the quality of DNA sequencing data. Introduction to the Integrative Genomics Viewer (IGV). Johan & Rebecka & Sarath Zoom Day 3 : 2021-01-26 - Wednesday \u00b6 Time Topic Responsible Location 9:00-9:45 Bioinformatics pipelines & HTC computing environments. Venkatesh Zoom 9:45-10:00 Break 10:00-10:45 Processing and QC of DNA- and RNA sequencing data && Somatic and germline variant callers. Rebecka Zoom 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers (continued) && Lab introduction. Rebecka Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Calling somatic and germline mutations. Filtering variants. Familiarizing with the variant formats. Visualising variants in IGV. Johan & Rebecka & Sarath Zoom Day 4 : 2021-01-27 - Thursday \u00b6 Time Topic Responsible Location 9:00-9:45 Somatic- and germline copy-number alterations. Markus Zoom 9:45-10:00 Break 10:00-10:45 Structural variation. Johan Zoom 10:45-11:00 Break 11:00-11:45 RNA sequencing && Lab intro Markus && Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions RNA analysis, expression and fusion calling. Analysis of copy number alterations. Identification of structural variants. Inspecting variants Johan & Rebecka & Markus & Sarath Zoom Day 5 : 2021-01-28 - Friday \u00b6 Time Topic Responsible Location 9:00-9:45 Clinical trials. How to curate somatic- and germline variation for clinical use. Johan Zoom 9:45-10:00 Break 10:00-10:45 How to curate somatic- and germline variation for clinical use (continued). Johan Zoom 10:45-11:00 Break 11:00-11:45 Annotating, interpreting and reporting somatic- and germline variation for clinical use. David Tamborero Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions: TBA David Tamborero & Johan & Rebecka & Sarath Zoom","title":"Schedule"},{"location":"schedule/#day-1-2021-01-24-monday","text":"Time Topic Responsible Location 9:00-9:45 An introduction to the cancer genome and mutational processes in cancer. Johan Zoom 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer (conti). Johan Zoom 10:45-11:00 Break 11:00-11:45 Practical considerations for performing cancer genomics Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Basic unix AWK Investigate the workspace directories, raw data and install R, R-packages ggplot tutorial Johan & Rebecka & Sarath Zoom","title":"Day 1 : 2021-01-24 - Monday"},{"location":"schedule/#day-2-2021-01-25-tuesday","text":"Time Topic Responsible Location 9:00-9:45 Liquid biopsies Johan Zoom 9:45-10:00 Break 10:00-10:45 The clinical impact of analysing the cancer genome. Felix Zoom 10:45-11:00 Break 11:00-11:45 Lab Introduction Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Looking into the human genome reference. Annotatation files needed for bioinformatic processing. Executing commands for processing and checking the quality of DNA sequencing data. Introduction to the Integrative Genomics Viewer (IGV). Johan & Rebecka & Sarath Zoom","title":"Day 2 : 2021-01-25 - Tuesday"},{"location":"schedule/#day-3-2021-01-26-wednesday","text":"Time Topic Responsible Location 9:00-9:45 Bioinformatics pipelines & HTC computing environments. Venkatesh Zoom 9:45-10:00 Break 10:00-10:45 Processing and QC of DNA- and RNA sequencing data && Somatic and germline variant callers. Rebecka Zoom 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers (continued) && Lab introduction. Rebecka Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions Calling somatic and germline mutations. Filtering variants. Familiarizing with the variant formats. Visualising variants in IGV. Johan & Rebecka & Sarath Zoom","title":"Day 3 : 2021-01-26 - Wednesday"},{"location":"schedule/#day-4-2021-01-27-thursday","text":"Time Topic Responsible Location 9:00-9:45 Somatic- and germline copy-number alterations. Markus Zoom 9:45-10:00 Break 10:00-10:45 Structural variation. Johan Zoom 10:45-11:00 Break 11:00-11:45 RNA sequencing && Lab intro Markus && Johan Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions RNA analysis, expression and fusion calling. Analysis of copy number alterations. Identification of structural variants. Inspecting variants Johan & Rebecka & Markus & Sarath Zoom","title":"Day 4 : 2021-01-27 - Thursday"},{"location":"schedule/#day-5-2021-01-28-friday","text":"Time Topic Responsible Location 9:00-9:45 Clinical trials. How to curate somatic- and germline variation for clinical use. Johan Zoom 9:45-10:00 Break 10:00-10:45 How to curate somatic- and germline variation for clinical use (continued). Johan Zoom 10:45-11:00 Break 11:00-11:45 Annotating, interpreting and reporting somatic- and germline variation for clinical use. David Tamborero Zoom 12:00-13:00 Lunch 13:00-17:00 Lab sessions: TBA David Tamborero & Johan & Rebecka & Sarath Zoom","title":"Day 5 : 2021-01-28 - Friday"},{"location":"aws/aws_ec2_instances/","text":"AWS Login \u00b6 Note : Please use the new course_EC2_02.pem/course_EC2_02.ppk key file for aws login. Name Status AWS instance login Address Alexandra running ubuntu@ec2-54-92-199-113.compute-1.amazonaws.com Binbin running ubuntu@ec2-107-20-20-82.compute-1.amazonaws.com Cecilia running ubuntu@ec2-107-21-88-61.compute-1.amazonaws.com Hao running ubuntu@ec2-18-234-84-227.compute-1.amazonaws.com Ioanna running ubuntu@ec2-54-165-103-100.compute-1.amazonaws.com Jiacheng running ubuntu@ec2-54-242-115-154.compute-1.amazonaws.com Johanna running ubuntu@ec2-54-144-243-152.compute-1.amazonaws.com Julia running ubuntu@ec2-54-242-223-44.compute-1.amazonaws.com Kang running ubuntu@ec2-107-20-63-117.compute-1.amazonaws.com Kejia running ubuntu@ec2-54-82-15-158.compute-1.amazonaws.com Konstantin running ubuntu@ec2-3-90-220-195.compute-1.amazonaws.com Mireia running ubuntu@ec2-54-226-184-69.compute-1.amazonaws.com Natasa running ubuntu@ec2-54-242-241-194.compute-1.amazonaws.com Nikita running ubuntu@ec2-54-163-203-157.compute-1.amazonaws.com Rasha running ubuntu@ec2-54-221-117-165.compute-1.amazonaws.com Shifeng running ubuntu@ec2-52-91-102-116.compute-1.amazonaws.com Una running ubuntu@ec2-54-242-107-2.compute-1.amazonaws.com Weiwei running ubuntu@ec2-52-90-127-18.compute-1.amazonaws.com Yiyi running ubuntu@ec2-54-82-47-94.compute-1.amazonaws.com Yun running ubuntu@ec2-3-91-244-199.compute-1.amazonaws.com YuPei running ubuntu@ec2-3-80-132-206.compute-1.amazonaws.com","title":"AWS Instances Login"},{"location":"aws/aws_ec2_instances/#aws-login","text":"Note : Please use the new course_EC2_02.pem/course_EC2_02.ppk key file for aws login. Name Status AWS instance login Address Alexandra running ubuntu@ec2-54-92-199-113.compute-1.amazonaws.com Binbin running ubuntu@ec2-107-20-20-82.compute-1.amazonaws.com Cecilia running ubuntu@ec2-107-21-88-61.compute-1.amazonaws.com Hao running ubuntu@ec2-18-234-84-227.compute-1.amazonaws.com Ioanna running ubuntu@ec2-54-165-103-100.compute-1.amazonaws.com Jiacheng running ubuntu@ec2-54-242-115-154.compute-1.amazonaws.com Johanna running ubuntu@ec2-54-144-243-152.compute-1.amazonaws.com Julia running ubuntu@ec2-54-242-223-44.compute-1.amazonaws.com Kang running ubuntu@ec2-107-20-63-117.compute-1.amazonaws.com Kejia running ubuntu@ec2-54-82-15-158.compute-1.amazonaws.com Konstantin running ubuntu@ec2-3-90-220-195.compute-1.amazonaws.com Mireia running ubuntu@ec2-54-226-184-69.compute-1.amazonaws.com Natasa running ubuntu@ec2-54-242-241-194.compute-1.amazonaws.com Nikita running ubuntu@ec2-54-163-203-157.compute-1.amazonaws.com Rasha running ubuntu@ec2-54-221-117-165.compute-1.amazonaws.com Shifeng running ubuntu@ec2-52-91-102-116.compute-1.amazonaws.com Una running ubuntu@ec2-54-242-107-2.compute-1.amazonaws.com Weiwei running ubuntu@ec2-52-90-127-18.compute-1.amazonaws.com Yiyi running ubuntu@ec2-54-82-47-94.compute-1.amazonaws.com Yun running ubuntu@ec2-3-91-244-199.compute-1.amazonaws.com YuPei running ubuntu@ec2-3-80-132-206.compute-1.amazonaws.com","title":"AWS Login"},{"location":"lab_session/R_for_bioinformatics/","text":"Programming in R for Bioinformatics \u00b6 Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer. What is R? \u00b6 R is a language and environment for statistical computing and graphics developed in 1993. It provides a wide variety of statistical and graphical techniques (linear and nonlinear modeling, statistical tests, time series analysis, classification, clustering, \u2026), and is highly extensible, meaning that the user community can write new R tools. It is a GNU project (Free and Open Source). The R language has its roots in the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and now, R is developed by the R Development Core Team, of which Chambers is a member. R is named partly after the first names of the first two R authors (Robert Gentleman and Ross Ihaka), and partly as a play on the name of S. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R. Some of R\u2019s strengths: \u00b6 The ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS. R can be extended (easily) via packages. R has its own LaTeX-like documentation format, which is used to supply comprehensive documentation, both on-line in a number of formats and in hardcopy. It has a vast community both in academia and in business. It\u2019s FREE! The R environment \u00b6 R is an integrated suite of software facilities for data manipulation, calculation and graphical display. It includes an effective data handling and storage facility, a suite of operators for calculations on arrays, in particular matrices, a large, coherent, integrated collection of intermediate tools for data analysis, graphical facilities for data analysis and display either on-screen or on hardcopy, and a well-developed, and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities. The term \u201cenvironment\u201d is intended to characterize it as a fully planned and coherent system, rather than an incremental accretion of very specific and inflexible tools, as is frequently the case with other data analysis software. R, like S, is designed around a true computer language, and it allows users to add additional functionality by defining new functions. Much of the system is itself written in the R dialect of S, which makes it easy for users to follow the algorithmic choices made. For computationally-intensive tasks, C, C++ and Fortran code can be linked and called at run time. Advanced users can write C code to manipulate R objects directly. Many users think of R as a statistics system. The R group prefers to think of it of an environment within which statistical techniques are implemented. The R Homepage \u00b6 The R homepage has a wealth of information on it, R-project.org On the homepage you can: Learn more about R * Download R * Get Documentation (official and user supplied) * Get access to CRAN \u2018Comprehensive R archival network\u2019 * RStudio * RStudio started in 2010, to offer R a more full featured integrated development environment (IDE) and modeled after matlabs IDE. RStudio has many features: \u00b6 syntax highlighting code completion smart indentation \u201cProjects\u201d workspace browser and data viewer embedded plots Markdown notebooks, Sweave authoring and knitr with one click pdf or html runs on all platforms and over the web etc. etc. etc. RStudio and its team have contributed to many R packages. These include: Tidyverse \u2013 R packages for data science, including ggplot2, dplyr, tidyr, and purrr Shiny \u2013 An interactive web technology RMarkdown \u2013 Insert R code into markdown documents knitr \u2013 Dynamic reports combining R, TeX, Markdown & HTML packrat \u2013 Package dependency tool devtools \u2013 Package development tool 1. Getting started \u00b6 Let\u2019s start RStudio 2. Open a new RScript File \u00b6 File -> New File -> R Script RStudio_newfile Then save the new empty file as Intro2R.R File -> Save as -> Intro2R.R 3. Basics of your environment \u00b6 The R prompt is the \u2018>\u2019 , when R is expecting more (command is not complete) you see a \u2018+\u2019 4. Writing and running R commands \u00b6 In the source editor (top left by default) type getwd() Then on the line Control + Enter (Linux/Windows), Command + Enter (Mac) to execute the line. 5. The assignment operator ( <- ) vs equals ( = ) \u00b6 The assignment operator is used assign data to a variable x <- 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 In this case, the equal sign works as well x = 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 But you should NEVER EVER DO THIS 1:10 -> x x [1] 1 2 3 4 5 6 7 8 9 10 The two act the same in most cases. The difference in assignment operators is clearer when you use them to set an argument value in a function call. For example: median(x = 1:10) x Error: object 'x' not found In this case, x is declared within the scope of the function, so it does not exist in the user workspace. median(x <- 1:10) x [1] 1 2 3 4 5 6 7 8 9 10 In this case, x is declared in the user workspace, so you can use it after the function call has been completed. There is a general preference among the R community for using <- for assignment (other than in function signatures) 6. The RStudio Cheat Sheets \u00b6 rstudio-ide.pdf spend 15m getting to know RStudio a little Import and export data in R \u00b6 R base function read.table() is a general funciton that can be used to read a file in table format. The data will be imported as a data frame. # To read a local file. If you have downloaded the raw_counts.txt file to your local machine, you may use the following command to read it in, by providing the full path for the file location. The way to specify the full path is the same as taught in the command line session. Here we assume raw_counts.txt is in our current working directory data <- read.table ( file = \"./raw_counts.txt\" , sep = \"\\t\" , header = T , stringsAsFactors = F ) # There is a very convenient way to read files from the internet. data <- read.table ( file = \"https://course-cg-5534.s3.amazonaws.com/awk_exercise/raw_counts.txt\" , sep = \"\\t\" , header = T , stringsAsFactors = F ) Take a look at the beginning part of the data frame. head ( data ) ## C61 C62 C63 C64 C91 C92 C93 C94 I561 I562 I563 I564 I591 I592 ## AT1G01010 322 346 256 396 372 506 361 342 638 488 440 479 770 430 ## AT1G01020 149 87 162 144 189 169 147 108 163 141 119 147 182 156 ## AT1G01030 15 32 35 22 24 33 21 35 18 8 54 35 23 8 ## AT1G01040 687 469 568 651 885 978 794 862 799 769 725 715 811 567 ## AT1G01046 1 1 5 4 5 3 0 2 4 3 1 0 2 8 ## AT1G01050 1447 1032 1083 1204 1413 1484 1138 938 1247 1516 984 1044 1374 1355 ## I593 I594 I861 I862 I863 I864 I891 I892 I893 I894 ## AT1G01010 656 467 143 453 429 206 567 458 520 474 ## AT1G01020 153 177 43 144 114 50 161 195 157 144 ## AT1G01030 16 24 42 17 22 39 26 28 39 30 ## AT1G01040 831 694 345 575 605 404 735 651 725 591 ## AT1G01046 8 1 0 4 0 3 5 7 0 5 ## AT1G01050 1437 1577 412 1338 1051 621 1434 1552 1248 1186 Depending on the format of the file, several variants of read.table() are available to make reading a file easier. read.csv(): for reading \"comma separated value\" files (.csv). read.csv2(): variant used in countries that use a comma \",\" as decimal point and a semicolon \";\" as field separators. read.delim(): for reading \"tab separated value\" files (\".txt\"). By default, point(\".\") is used as decimal point. read.delim2(): for reading \"tab separated value\" files (\".txt\"). By default, comma (\",\") is used as decimal point. Choosing the correct function (or parameters) is important! If we use read.csv() to read our tab-delimited file, it becomes a mess. data2 <- read.csv ( file = \"https://raw_counts.txt\" , stringsAsFactors = F ) head ( data2 ) ## C61.C62.C63.C64.C91.C92.C93.C94.I561.I562.I563.I564.I591.I592.I593.I594.I861.I862.I863.I864.I891.I892.I893.I894 ## 1 AT1G01010\\t322\\t346\\t256\\t396\\t372\\t506\\t361\\t342\\t638\\t488\\t440\\t479\\t770\\t430\\t656\\t467\\t143\\t453\\t429\\t206\\t567\\t458\\t520\\t474 ## 2 AT1G01020\\t149\\t87\\t162\\t144\\t189\\t169\\t147\\t108\\t163\\t141\\t119\\t147\\t182\\t156\\t153\\t177\\t43\\t144\\t114\\t50\\t161\\t195\\t157\\t144 ## 3 AT1G01030\\t15\\t32\\t35\\t22\\t24\\t33\\t21\\t35\\t18\\t8\\t54\\t35\\t23\\t8\\t16\\t24\\t42\\t17\\t22\\t39\\t26\\t28\\t39\\t30 ## 4 AT1G01040\\t687\\t469\\t568\\t651\\t885\\t978\\t794\\t862\\t799\\t769\\t725\\t715\\t811\\t567\\t831\\t694\\t345\\t575\\t605\\t404\\t735\\t651\\t725\\t591 ## 5 AT1G01046\\t1\\t1\\t5\\t4\\t5\\t3\\t0\\t2\\t4\\t3\\t1\\t0\\t2\\t8\\t8\\t1\\t0\\t4\\t0\\t3\\t5\\t7\\t0\\t5 ## 6 AT1G01050\\t1447\\t1032\\t1083\\t1204\\t1413\\t1484\\t1138\\t938\\t1247\\t1516\\t984\\t1044\\t1374\\t1355\\t1437\\t1577\\t412\\t1338\\t1051\\t621\\t1434\\t1552\\t1248\\t1186 However, the read.csv() function is appropriate for a comma-delimited file. data3 <- read.csv ( file = \"https://raw_counts.csv\" , stringsAsFactors = F ) head ( data3 ) ## C61 C62 C63 C64 C91 C92 C93 C94 I561 I562 I563 I564 I591 I592 ## AT1G01010 322 346 256 396 372 506 361 342 638 488 440 479 770 430 ## AT1G01020 149 87 162 144 189 169 147 108 163 141 119 147 182 156 ## AT1G01030 15 32 35 22 24 33 21 35 18 8 54 35 23 8 ## AT1G01040 687 469 568 651 885 978 794 862 799 769 725 715 811 567 ## AT1G01046 1 1 5 4 5 3 0 2 4 3 1 0 2 8 ## AT1G01050 1447 1032 1083 1204 1413 1484 1138 938 1247 1516 984 1044 1374 1355 ## I593 I594 I861 I862 I863 I864 I891 I892 I893 I894 ## AT1G01010 656 467 143 453 429 206 567 458 520 474 ## AT1G01020 153 177 43 144 114 50 161 195 157 144 ## AT1G01030 16 24 42 17 22 39 26 28 39 30 ## AT1G01040 831 694 345 575 605 404 735 651 725 591 ## AT1G01046 8 1 0 4 0 3 5 7 0 5 ## AT1G01050 1437 1577 412 1338 1051 621 1434 1552 1248 1186 Since the data contained in these files is the same, we don't need to keep three copies. identical ( data , data3 ) ## [1] TRUE rm ( data2 , data3 ) R base function write.table() can be used to export data to a file. # To write to a file called \"output.txt\" in your current working directory. write.table ( data [ 1 : 20 ,], file = \"output.txt\" , sep = \"\\t\" , quote = F , row.names = T , col.names = T ) It is also possible to export data to a csv file. write.csv() write.csv2() Basic statistics in R \u00b6 Description R_function Mean mean() Standard deviation sd() Variance var() Minimum min() Maximum max() Median median() Range of values: minimum and maximum range() Sample quantiles quantile() Generic function summary() Interquartile range IQR() Calculate the mean expression for each sample. apply ( data , 2 , mean ) ## V1 V2 V3 V4 V5 V6 V7 ## 0.1800418 -0.2534468 -0.1897400 -0.5426865 -0.1894511 -0.2531511 -0.3651648 Calculate the range of expression for each sample. apply ( data , 2 , range ) ## V1 V2 V3 V4 V5 V6 V7 ## [1,] -0.6225434 -2.452988 -1.1885474 -1.5352739 -1.311441 -1.3231301 -3.108072 ## [2,] 0.9157785 1.558738 0.5844574 0.5020093 1.986243 0.8357217 1.515365 Calculate the quantiles of each samples. apply ( data , 2 , quantile ) ## V1 V2 V3 V4 V5 V6 ## 0% -0.62254340 -2.4529883 -1.1885474 -1.53527385 -1.31144098 -1.3231301 ## 25% -0.24319891 -1.4878457 -0.4367767 -1.03930207 -0.83116242 -0.9900499 ## 50% 0.08886876 0.2487017 -0.3765026 -0.58285066 -0.50073601 -0.4826310 ## 75% 0.68229320 0.9235564 0.2629829 -0.05204306 0.08105049 0.5890407 ## 100% 0.91577852 1.5587378 0.5844574 0.50200930 1.98624311 0.8357217 ## V7 ## 0% -3.10807211 ## 25% -0.84236920 ## 50% -0.05402889 ## 75% 0.38766034 ## 100% 1.51536487 Simple data visualization \u00b6 Scatter plot and line plot can be produced using the function plot(). x <- c ( 1 : 50 ) y <- 1 + sqrt ( x ) / 2 plot ( x , y ) plot ( x , y , type = \"l\" ) # plot both the points and lines ## first plot points plot ( x , y ) lines ( x , y , type = \"l\" ) ## lines() can only be used to add information to a graph, while it cannot produce a graph on its own. boxplot() can be used to summarize data. boxplot ( data , xlab = \"Sample ID\" , ylab = \"Raw Counts\" ) add more details to the plot. boxplot ( data , xlab = \"Sample ID\" , ylab = \"Raw Counts\" , main = \"Expression levels\" , col = \"blue\" , border = \"black\" ) x <- rnorm ( 1000 ) boxplot ( x ) hist() can be used to create histograms of data. hist ( x ) # use user defined break points hist ( x , breaks = seq ( range ( x )[ 1 ] -1 , range ( x )[ 2 ] +1 , by = 0.5 )) # clear plotting device/area dev.off () ## null device ## 1 ggplot2 \u00b6 ggplot2 is an R package for making visualizations and is based on the \u2018Grammar of Graphics\u2019 . It is often referred as one of the best packages for visualizations . Actually , there are other tools for making graphs in R such as Lattice and base R graphics . However , ggplot2 is straightforwardly easy to make any kind of graphs even complex ones . It is easy to add complexity or take it away which makes ggplot2 superior among these three . Since you can\u2019t go back and forth once you created a plot in base R or in lattice . That\u2019s enough for comparison , let\u2019s start learning it now. Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer. Grammar of Graphics Just as the grammar of language helps us construct meaningful sentences out of words, the Grammar of Graphics helps us to construct graphical figures out of different visual elements. This grammar gives us a way to talk about parts of a plot: all the circles, lines, arrows, and words that are combined into a diagram for visualizing data. Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham to describe the components of a plot, including the data being plotted the geometric objects (circles, lines, etc.) that appear on the plot a set of mappings from variables in the data to the aesthetics (appearance) of the geometric objects a statistical transformation used to calculate the data values used in the plot a position adjustment for locating each geometric object on the plot a scale (e.g., range of values) for each aesthetic mapping used a coordinate system used to organize the geometric objects the facets or groups of data shown in different plots Similar to how tidyr and dplyr provide efficient data transformation and manipulation, ggplot2 provides more efficient ways to create specific visual images. ggplot objects can be highly complex, but the basic order of layers will usually look like this: Begin with the baseline ggplot() command - this \u201copens\u201d the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command Add \u201cgeom\u201d layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix. Add design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation A simple example of skeleton code is as follows. We will explain each component in the sections below. # plot data from my_data columns as red points ggplot ( data = my_data ) + # use the dataset \"my_data\" geom_point ( # add a layer of points (dots) mapping = aes ( x = col1 , y = col2 ), # \"map\" data column to axes color = \"red\" ) + # other specification for the geom labs () + # here you add titles, axes labels, etc. theme () # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.) Starting up with ggplot \u00b6 Firstly , we need to install ggplot2 package install.packages ( \"ggplot2\" ) library ( \"ggplot2\" ) The Basics of plotting with ggplot \u00b6 In order to create a plot, you: Call the ggplot() function which creates a blank canvas Specify aesthetic mappings, which specifies how you want to map variables to visual aspects. In this case we are simply mapping the displ and hwy variables to the x- and y-axes. You then add new layers that are geometric objects which will show up on the plot. In this case we add geom_point to add a layer with points (dot) elements as the geometric shapes to represent the data. # create canvas ggplot ( data ) # variables of interest mapped ggplot ( data , aes ( x = displ , y = hwy )) # data plotted ggplot ( data , aes ( x = displ , y = hwy )) + geom_point () To try ggplot hands-on, we will follow the original ggplot basics chapter from The Epidemiologist R Handbook (Batra, Neale, et al. The Epidemiologist R Handbook. 2021). Now let's go to ggplot hands-on exercise. \u00b6 (clicking on the link above will take you to the handbook's website, please feel free to bookmark the page for later reference.)","title":"R for Bioinformatics"},{"location":"lab_session/R_for_bioinformatics/#programming-in-r-for-bioinformatics","text":"Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer.","title":"Programming in R for Bioinformatics"},{"location":"lab_session/R_for_bioinformatics/#what-is-r","text":"R is a language and environment for statistical computing and graphics developed in 1993. It provides a wide variety of statistical and graphical techniques (linear and nonlinear modeling, statistical tests, time series analysis, classification, clustering, \u2026), and is highly extensible, meaning that the user community can write new R tools. It is a GNU project (Free and Open Source). The R language has its roots in the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R was created by Ross Ihaka and Robert Gentleman at the University of Auckland, New Zealand, and now, R is developed by the R Development Core Team, of which Chambers is a member. R is named partly after the first names of the first two R authors (Robert Gentleman and Ross Ihaka), and partly as a play on the name of S. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.","title":"What is R?"},{"location":"lab_session/R_for_bioinformatics/#some-of-rs-strengths","text":"The ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS. R can be extended (easily) via packages. R has its own LaTeX-like documentation format, which is used to supply comprehensive documentation, both on-line in a number of formats and in hardcopy. It has a vast community both in academia and in business. It\u2019s FREE!","title":"Some of R\u2019s strengths:"},{"location":"lab_session/R_for_bioinformatics/#the-r-environment","text":"R is an integrated suite of software facilities for data manipulation, calculation and graphical display. It includes an effective data handling and storage facility, a suite of operators for calculations on arrays, in particular matrices, a large, coherent, integrated collection of intermediate tools for data analysis, graphical facilities for data analysis and display either on-screen or on hardcopy, and a well-developed, and effective programming language which includes conditionals, loops, user-defined recursive functions and input and output facilities. The term \u201cenvironment\u201d is intended to characterize it as a fully planned and coherent system, rather than an incremental accretion of very specific and inflexible tools, as is frequently the case with other data analysis software. R, like S, is designed around a true computer language, and it allows users to add additional functionality by defining new functions. Much of the system is itself written in the R dialect of S, which makes it easy for users to follow the algorithmic choices made. For computationally-intensive tasks, C, C++ and Fortran code can be linked and called at run time. Advanced users can write C code to manipulate R objects directly. Many users think of R as a statistics system. The R group prefers to think of it of an environment within which statistical techniques are implemented.","title":"The R environment"},{"location":"lab_session/R_for_bioinformatics/#the-r-homepage","text":"The R homepage has a wealth of information on it, R-project.org On the homepage you can: Learn more about R * Download R * Get Documentation (official and user supplied) * Get access to CRAN \u2018Comprehensive R archival network\u2019 * RStudio * RStudio started in 2010, to offer R a more full featured integrated development environment (IDE) and modeled after matlabs IDE.","title":"The R Homepage"},{"location":"lab_session/R_for_bioinformatics/#rstudio-has-many-features","text":"syntax highlighting code completion smart indentation \u201cProjects\u201d workspace browser and data viewer embedded plots Markdown notebooks, Sweave authoring and knitr with one click pdf or html runs on all platforms and over the web etc. etc. etc. RStudio and its team have contributed to many R packages. These include: Tidyverse \u2013 R packages for data science, including ggplot2, dplyr, tidyr, and purrr Shiny \u2013 An interactive web technology RMarkdown \u2013 Insert R code into markdown documents knitr \u2013 Dynamic reports combining R, TeX, Markdown & HTML packrat \u2013 Package dependency tool devtools \u2013 Package development tool","title":"RStudio has many features:"},{"location":"lab_session/R_for_bioinformatics/#1-getting-started","text":"Let\u2019s start RStudio","title":"1. Getting started"},{"location":"lab_session/R_for_bioinformatics/#2-open-a-new-rscript-file","text":"File -> New File -> R Script RStudio_newfile Then save the new empty file as Intro2R.R File -> Save as -> Intro2R.R","title":"2. Open a new RScript File"},{"location":"lab_session/R_for_bioinformatics/#3-basics-of-your-environment","text":"The R prompt is the \u2018>\u2019 , when R is expecting more (command is not complete) you see a \u2018+\u2019","title":"3. Basics of your environment"},{"location":"lab_session/R_for_bioinformatics/#4-writing-and-running-r-commands","text":"In the source editor (top left by default) type getwd() Then on the line Control + Enter (Linux/Windows), Command + Enter (Mac) to execute the line.","title":"4. Writing and running R commands"},{"location":"lab_session/R_for_bioinformatics/#5-the-assignment-operator-vs-equals","text":"The assignment operator is used assign data to a variable x <- 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 In this case, the equal sign works as well x = 1:10 x [1] 1 2 3 4 5 6 7 8 9 10 But you should NEVER EVER DO THIS 1:10 -> x x [1] 1 2 3 4 5 6 7 8 9 10 The two act the same in most cases. The difference in assignment operators is clearer when you use them to set an argument value in a function call. For example: median(x = 1:10) x Error: object 'x' not found In this case, x is declared within the scope of the function, so it does not exist in the user workspace. median(x <- 1:10) x [1] 1 2 3 4 5 6 7 8 9 10 In this case, x is declared in the user workspace, so you can use it after the function call has been completed. There is a general preference among the R community for using <- for assignment (other than in function signatures)","title":"5. The assignment operator ( &lt;- ) vs equals ( = )"},{"location":"lab_session/R_for_bioinformatics/#6-the-rstudio-cheat-sheets","text":"rstudio-ide.pdf spend 15m getting to know RStudio a little","title":"6. The RStudio Cheat Sheets"},{"location":"lab_session/R_for_bioinformatics/#import-and-export-data-in-r","text":"R base function read.table() is a general funciton that can be used to read a file in table format. The data will be imported as a data frame. # To read a local file. If you have downloaded the raw_counts.txt file to your local machine, you may use the following command to read it in, by providing the full path for the file location. The way to specify the full path is the same as taught in the command line session. Here we assume raw_counts.txt is in our current working directory data <- read.table ( file = \"./raw_counts.txt\" , sep = \"\\t\" , header = T , stringsAsFactors = F ) # There is a very convenient way to read files from the internet. data <- read.table ( file = \"https://course-cg-5534.s3.amazonaws.com/awk_exercise/raw_counts.txt\" , sep = \"\\t\" , header = T , stringsAsFactors = F ) Take a look at the beginning part of the data frame. head ( data ) ## C61 C62 C63 C64 C91 C92 C93 C94 I561 I562 I563 I564 I591 I592 ## AT1G01010 322 346 256 396 372 506 361 342 638 488 440 479 770 430 ## AT1G01020 149 87 162 144 189 169 147 108 163 141 119 147 182 156 ## AT1G01030 15 32 35 22 24 33 21 35 18 8 54 35 23 8 ## AT1G01040 687 469 568 651 885 978 794 862 799 769 725 715 811 567 ## AT1G01046 1 1 5 4 5 3 0 2 4 3 1 0 2 8 ## AT1G01050 1447 1032 1083 1204 1413 1484 1138 938 1247 1516 984 1044 1374 1355 ## I593 I594 I861 I862 I863 I864 I891 I892 I893 I894 ## AT1G01010 656 467 143 453 429 206 567 458 520 474 ## AT1G01020 153 177 43 144 114 50 161 195 157 144 ## AT1G01030 16 24 42 17 22 39 26 28 39 30 ## AT1G01040 831 694 345 575 605 404 735 651 725 591 ## AT1G01046 8 1 0 4 0 3 5 7 0 5 ## AT1G01050 1437 1577 412 1338 1051 621 1434 1552 1248 1186 Depending on the format of the file, several variants of read.table() are available to make reading a file easier. read.csv(): for reading \"comma separated value\" files (.csv). read.csv2(): variant used in countries that use a comma \",\" as decimal point and a semicolon \";\" as field separators. read.delim(): for reading \"tab separated value\" files (\".txt\"). By default, point(\".\") is used as decimal point. read.delim2(): for reading \"tab separated value\" files (\".txt\"). By default, comma (\",\") is used as decimal point. Choosing the correct function (or parameters) is important! If we use read.csv() to read our tab-delimited file, it becomes a mess. data2 <- read.csv ( file = \"https://raw_counts.txt\" , stringsAsFactors = F ) head ( data2 ) ## C61.C62.C63.C64.C91.C92.C93.C94.I561.I562.I563.I564.I591.I592.I593.I594.I861.I862.I863.I864.I891.I892.I893.I894 ## 1 AT1G01010\\t322\\t346\\t256\\t396\\t372\\t506\\t361\\t342\\t638\\t488\\t440\\t479\\t770\\t430\\t656\\t467\\t143\\t453\\t429\\t206\\t567\\t458\\t520\\t474 ## 2 AT1G01020\\t149\\t87\\t162\\t144\\t189\\t169\\t147\\t108\\t163\\t141\\t119\\t147\\t182\\t156\\t153\\t177\\t43\\t144\\t114\\t50\\t161\\t195\\t157\\t144 ## 3 AT1G01030\\t15\\t32\\t35\\t22\\t24\\t33\\t21\\t35\\t18\\t8\\t54\\t35\\t23\\t8\\t16\\t24\\t42\\t17\\t22\\t39\\t26\\t28\\t39\\t30 ## 4 AT1G01040\\t687\\t469\\t568\\t651\\t885\\t978\\t794\\t862\\t799\\t769\\t725\\t715\\t811\\t567\\t831\\t694\\t345\\t575\\t605\\t404\\t735\\t651\\t725\\t591 ## 5 AT1G01046\\t1\\t1\\t5\\t4\\t5\\t3\\t0\\t2\\t4\\t3\\t1\\t0\\t2\\t8\\t8\\t1\\t0\\t4\\t0\\t3\\t5\\t7\\t0\\t5 ## 6 AT1G01050\\t1447\\t1032\\t1083\\t1204\\t1413\\t1484\\t1138\\t938\\t1247\\t1516\\t984\\t1044\\t1374\\t1355\\t1437\\t1577\\t412\\t1338\\t1051\\t621\\t1434\\t1552\\t1248\\t1186 However, the read.csv() function is appropriate for a comma-delimited file. data3 <- read.csv ( file = \"https://raw_counts.csv\" , stringsAsFactors = F ) head ( data3 ) ## C61 C62 C63 C64 C91 C92 C93 C94 I561 I562 I563 I564 I591 I592 ## AT1G01010 322 346 256 396 372 506 361 342 638 488 440 479 770 430 ## AT1G01020 149 87 162 144 189 169 147 108 163 141 119 147 182 156 ## AT1G01030 15 32 35 22 24 33 21 35 18 8 54 35 23 8 ## AT1G01040 687 469 568 651 885 978 794 862 799 769 725 715 811 567 ## AT1G01046 1 1 5 4 5 3 0 2 4 3 1 0 2 8 ## AT1G01050 1447 1032 1083 1204 1413 1484 1138 938 1247 1516 984 1044 1374 1355 ## I593 I594 I861 I862 I863 I864 I891 I892 I893 I894 ## AT1G01010 656 467 143 453 429 206 567 458 520 474 ## AT1G01020 153 177 43 144 114 50 161 195 157 144 ## AT1G01030 16 24 42 17 22 39 26 28 39 30 ## AT1G01040 831 694 345 575 605 404 735 651 725 591 ## AT1G01046 8 1 0 4 0 3 5 7 0 5 ## AT1G01050 1437 1577 412 1338 1051 621 1434 1552 1248 1186 Since the data contained in these files is the same, we don't need to keep three copies. identical ( data , data3 ) ## [1] TRUE rm ( data2 , data3 ) R base function write.table() can be used to export data to a file. # To write to a file called \"output.txt\" in your current working directory. write.table ( data [ 1 : 20 ,], file = \"output.txt\" , sep = \"\\t\" , quote = F , row.names = T , col.names = T ) It is also possible to export data to a csv file. write.csv() write.csv2()","title":"Import and export data in R"},{"location":"lab_session/R_for_bioinformatics/#basic-statistics-in-r","text":"Description R_function Mean mean() Standard deviation sd() Variance var() Minimum min() Maximum max() Median median() Range of values: minimum and maximum range() Sample quantiles quantile() Generic function summary() Interquartile range IQR() Calculate the mean expression for each sample. apply ( data , 2 , mean ) ## V1 V2 V3 V4 V5 V6 V7 ## 0.1800418 -0.2534468 -0.1897400 -0.5426865 -0.1894511 -0.2531511 -0.3651648 Calculate the range of expression for each sample. apply ( data , 2 , range ) ## V1 V2 V3 V4 V5 V6 V7 ## [1,] -0.6225434 -2.452988 -1.1885474 -1.5352739 -1.311441 -1.3231301 -3.108072 ## [2,] 0.9157785 1.558738 0.5844574 0.5020093 1.986243 0.8357217 1.515365 Calculate the quantiles of each samples. apply ( data , 2 , quantile ) ## V1 V2 V3 V4 V5 V6 ## 0% -0.62254340 -2.4529883 -1.1885474 -1.53527385 -1.31144098 -1.3231301 ## 25% -0.24319891 -1.4878457 -0.4367767 -1.03930207 -0.83116242 -0.9900499 ## 50% 0.08886876 0.2487017 -0.3765026 -0.58285066 -0.50073601 -0.4826310 ## 75% 0.68229320 0.9235564 0.2629829 -0.05204306 0.08105049 0.5890407 ## 100% 0.91577852 1.5587378 0.5844574 0.50200930 1.98624311 0.8357217 ## V7 ## 0% -3.10807211 ## 25% -0.84236920 ## 50% -0.05402889 ## 75% 0.38766034 ## 100% 1.51536487","title":"Basic statistics in R"},{"location":"lab_session/R_for_bioinformatics/#simple-data-visualization","text":"Scatter plot and line plot can be produced using the function plot(). x <- c ( 1 : 50 ) y <- 1 + sqrt ( x ) / 2 plot ( x , y ) plot ( x , y , type = \"l\" ) # plot both the points and lines ## first plot points plot ( x , y ) lines ( x , y , type = \"l\" ) ## lines() can only be used to add information to a graph, while it cannot produce a graph on its own. boxplot() can be used to summarize data. boxplot ( data , xlab = \"Sample ID\" , ylab = \"Raw Counts\" ) add more details to the plot. boxplot ( data , xlab = \"Sample ID\" , ylab = \"Raw Counts\" , main = \"Expression levels\" , col = \"blue\" , border = \"black\" ) x <- rnorm ( 1000 ) boxplot ( x ) hist() can be used to create histograms of data. hist ( x ) # use user defined break points hist ( x , breaks = seq ( range ( x )[ 1 ] -1 , range ( x )[ 2 ] +1 , by = 0.5 )) # clear plotting device/area dev.off () ## null device ## 1","title":"Simple data visualization"},{"location":"lab_session/R_for_bioinformatics/#ggplot2","text":"ggplot2 is an R package for making visualizations and is based on the \u2018Grammar of Graphics\u2019 . It is often referred as one of the best packages for visualizations . Actually , there are other tools for making graphs in R such as Lattice and base R graphics . However , ggplot2 is straightforwardly easy to make any kind of graphs even complex ones . It is easy to add complexity or take it away which makes ggplot2 superior among these three . Since you can\u2019t go back and forth once you created a plot in base R or in lattice . That\u2019s enough for comparison , let\u2019s start learning it now. Tip: In order to take most out of this tutorial you should not miss reading any lines in this tutorial and follow the flow and write codes on your computer. Grammar of Graphics Just as the grammar of language helps us construct meaningful sentences out of words, the Grammar of Graphics helps us to construct graphical figures out of different visual elements. This grammar gives us a way to talk about parts of a plot: all the circles, lines, arrows, and words that are combined into a diagram for visualizing data. Originally developed by Leland Wilkinson, the Grammar of Graphics was adapted by Hadley Wickham to describe the components of a plot, including the data being plotted the geometric objects (circles, lines, etc.) that appear on the plot a set of mappings from variables in the data to the aesthetics (appearance) of the geometric objects a statistical transformation used to calculate the data values used in the plot a position adjustment for locating each geometric object on the plot a scale (e.g., range of values) for each aesthetic mapping used a coordinate system used to organize the geometric objects the facets or groups of data shown in different plots Similar to how tidyr and dplyr provide efficient data transformation and manipulation, ggplot2 provides more efficient ways to create specific visual images. ggplot objects can be highly complex, but the basic order of layers will usually look like this: Begin with the baseline ggplot() command - this \u201copens\u201d the ggplot and allow subsequent functions to be added with +. Typically the dataset is also specified in this command Add \u201cgeom\u201d layers - these functions visualize the data as geometries (shapes), e.g. as a bar graph, line plot, scatter plot, histogram (or a combination!). These functions all start with geom_ as a prefix. Add design elements to the plot such as axis labels, title, fonts, sizes, color schemes, legends, or axes rotation A simple example of skeleton code is as follows. We will explain each component in the sections below. # plot data from my_data columns as red points ggplot ( data = my_data ) + # use the dataset \"my_data\" geom_point ( # add a layer of points (dots) mapping = aes ( x = col1 , y = col2 ), # \"map\" data column to axes color = \"red\" ) + # other specification for the geom labs () + # here you add titles, axes labels, etc. theme () # here you adjust color, font, size etc of non-data plot elements (axes, title, etc.)","title":"ggplot2"},{"location":"lab_session/R_for_bioinformatics/#starting-up-with-ggplot","text":"Firstly , we need to install ggplot2 package install.packages ( \"ggplot2\" ) library ( \"ggplot2\" )","title":"Starting up with ggplot"},{"location":"lab_session/R_for_bioinformatics/#the-basics-of-plotting-with-ggplot","text":"In order to create a plot, you: Call the ggplot() function which creates a blank canvas Specify aesthetic mappings, which specifies how you want to map variables to visual aspects. In this case we are simply mapping the displ and hwy variables to the x- and y-axes. You then add new layers that are geometric objects which will show up on the plot. In this case we add geom_point to add a layer with points (dot) elements as the geometric shapes to represent the data. # create canvas ggplot ( data ) # variables of interest mapped ggplot ( data , aes ( x = displ , y = hwy )) # data plotted ggplot ( data , aes ( x = displ , y = hwy )) + geom_point () To try ggplot hands-on, we will follow the original ggplot basics chapter from The Epidemiologist R Handbook (Batra, Neale, et al. The Epidemiologist R Handbook. 2021).","title":"The Basics of plotting with ggplot"},{"location":"lab_session/R_for_bioinformatics/#now-lets-go-to-ggplot-hands-on-exercise","text":"(clicking on the link above will take you to the handbook's website, please feel free to bookmark the page for later reference.)","title":"Now let's go to ggplot hands-on exercise."},{"location":"lab_session/analysis_of_cnv/","text":"Analysis of copy number variation \u00b6 Copy number analysis tools \u00b6 There are plenty of tools available for copy number analysis today, and you will likely be able to find one for free that suits your particular application. Although tailored for different types of sequence data, general processing steps of all copy number analysis tools include: 1. Quantification of sequence read depth throughout the reference genome as a measure of DNA abundance in the sample 1. Removal of sample-specific systematic noise using features such as GC content and mappability 1. Removal of assay-specific systematic noise using normal (non-cancer) reference samples 1. Segmentation - partitioning of the reference genome so that each segment can be assigned one copy number 1. Copy number calling, assigning each segment some estimate of the number of copies per cell 1. Combine normalized coverage with SNP allele frequencies to support copy number estimates In this exercise we will perform the above steps using R, and a BAM file of aligned reads and a VCF file of SNPs as input. Download data and prepare R \u00b6 The data used in this exercise is available for download here: copy_number_files.tar.bz2 Extract the content into a local folder on your computer. tar xjvf copy_number_files.tar.bz2 Several packages from the Bioconductor repository are used in this exercise, most importantly GenomicRanges which facilitates working with anything located on the reference genome. Here is a good overview. Parse sequence target definition \u00b6 We use targeted sequencing of a prostate cancer sample in this exercise, as the resulting data files are of a more convenient size than whole genome or exome. This hybrid-capture panel covers a few hundred cancer genes and a few thousands of additional targets intended to improve copy number and structural variant detection. In RStudio, navigate to the folder containing the exercise files: setwd ( 'your/path/to/exercise_folder' ) Tip: Open a new Rmarkdown file and save it in the exercise data folder. Keep your code as shown in the template, and write your comments and answers to questions between code chunks. Click knit any time to run all code and generate a pdf or html report. This way, when you are through this exercise, your report is done too. The BED (Browser Extensible Data) format is a text file format used to store genomic regions as coordinates and associated annotations. The data are presented in the form of columns separated by spaces or tabs. This format was developed during the Human Genome Project and then adopted by other sequencing projects. Parse and investigate the target definition BED file. library ( data.table ) bed_file <- 'targets.bed' file.exists ( bed_file ) targets <- fread ( bed_file ) targets # Let the first column be the target number targets <- cbind ( 1 : nrow ( targets ), targets ) # Set useful column names colnames ( targets ) <- c ( 'target' , 'chromosome' , 'start' , 'end' ) # Check target length targets [, length := end - start ] summary ( targets ) # Check target distribution over chromosomes targets [, table ( chromosome )] The R code in this exercise features some data.table syntax. If you are more familiar with base R or tidyverse syntax, a comparison between them can be found here . Annotate with gene symbol \u00b6 Before parsing the sequence data, let's assign gene symbols to targets that overlap a gene. We'll take advantage of databases accessible through Bioconductor packages. First, we need to create a GenomicRanges object representing our targets: library ( GenomicRanges ) target_ranges <- makeGRangesFromDataFrame ( targets ) target_ranges seqlevelsStyle ( target_ranges ) As you may recognize, chromosomes have the NCBI/EMBL sequence naming style (1,...). This matches the reference genome used with our sequence data. Let's create a second GenomicRanges object featuring the UCSC names (chr1,...) for use with some database queries. ucsc_ranges <- target_ranges seqlevelsStyle ( ucsc_ranges ) <- \"UCSC\" ucsc_ranges We then use the detailRanges function from the csaw package to overlap our target ranges with known genes. library ( org.Hs.eg.db ) library ( TxDb.Hsapiens.UCSC.hg19.knownGene ) library ( csaw ) d <- detailRanges ( ucsc_ranges , orgdb = org.Hs.eg.db , txdb = TxDb.Hsapiens.UCSC.hg19.knownGene ) d as.data.table ( d ) There are many ways to achieve the same thing. You should focus mostly on what we do, and not worry too much about exactly how. Google is your friend and if you know what you want do do, you can always find a way. We only care about some level of overlap at this point, let's just retrieve the gene symbols and add as a column to the table of targets. We can then see how targets allocate to genes. library ( stringr ) targets $ gene <- str_remove ( string = d $ overlap , pattern = ':.*' ) targets table ( targets $ gene ) as.data.table ( table ( targets $ gene )) as.data.table ( table ( targets $ gene ))[ N > 25 ] Parse sequence read counts \u00b6 We are now ready to access the BAM file and count the number of read pairs (i.e. fragments) that map to each target. For this we use the bamsignals package and the GenomicRanges object with NCBI-style sequence names. To count a read pair, we require it's midpoint to fall within a target, and that it is not flagged as a duplicate . library ( bamsignals ) library ( Rsamtools ) tumor_bam <- 'Sample1.bam' file.exists ( tumor_bam ) file.exists ( paste0 ( tumor_bam , '.bai' )) targets $ coverage <- bamCount ( tumor_bam , target_ranges , paired.end = \"midpoint\" , filteredFlag = 1024 , verbose = F ) targets summary ( targets ) Strictly speaking, read count is not equal to sequence coverage. But given the fragment length and target size in this example, we can safely call the result sequence coverage. Zero coverage will be a problem later as it log-transforms to -Inf. We set those to 1: targets [ coverage == 0 , coverage := 1 ] Plot sequence coverage \u00b6 Let's investigate the raw sequence coverage across targets. You are probably already familiar with ggplot2 . library ( ggplot2 ); theme_set ( theme_bw ()) # Simple ggplot ggplot ( data = targets ) + geom_point ( mapping = aes ( x = target , y = coverage )) # Some adjustments ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = target , y = coverage ), size = . 2 ) # To use start position, we can separate the plot by chromosome. ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage ), size = . 5 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) The figure may now look squished in RStudio's bottom-right plots pane. Click the Zoom button and maximize the new window. Choose a few genes to highlight in the plot. myGenes <- c ( 'AR' , 'ATM' , 'BRCA1' , 'BRCA2' , 'PTEN' , 'TMPRSS2' , 'ERG' ) targets $ label = '' targets [ gene %in% myGenes , label := gene ][ label == '' , label := NA ] ggplot () + ylim ( c ( 0 , 3500 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage , col = label ), size = . 2 ) ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage , col = label ), size = . 5 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) Model and correct for GC content bias \u00b6 Sequence GC content is known to affect PCR amplification and sequence coverage. Let's retrieve the GC content for the targets and investigate if there is a bias in our data set. library ( BSgenome.Hsapiens.UCSC.hg19 ) library ( BSgenome ) library ( Repitools ) targets $ gc <- gcContentCalc ( ucsc_ranges , organism = Hsapiens ) ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) There seems to be a modest effect of target GC content on sequence coverage. Let's build a loess model of the effect. We leave the X chromosome out. loess_tumor <- loess ( coverage ~ gc , data = targets , subset = chromosome != 'X' , family = \"symmetric\" , control = loess.control ( surface = \"direct\" )) To plot the model, we predict the sequence coverage for GC content ranging from 1% to 100%: tumor_gc_line <- data.table ( x = 1 : 100 / 100 , y = predict ( loess_tumor , data.table ( gc = 1 : 100 / 100 ))) ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) + geom_line ( data = tumor_gc_line , mapping = aes ( x = x , y = y ), col = 'blue' ) We seem to predict negative coverage at some high GC content. We'll try the same model on log2 of coverage. It will now return the predicted log2 coverage, which we can either keep using, or transform back to coverage. We'll keep using coverage for now. loess_tumor <- loess ( log2 ( coverage ) ~ gc , data = targets , subset = chromosome != 'X' , family = \"symmetric\" , control = loess.control ( surface = \"direct\" )) # prediction is log2(coverage), 2^prediction equals predicted coverage tumor_gc_line <- data.table ( x = 1 : 100 / 100 , y = 2 ^ predict ( loess_tumor , data.table ( gc = 1 : 100 / 100 ))) ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) + geom_line ( data = tumor_gc_line , mapping = aes ( x = x , y = y ), col = 'blue' ) To adjust for GC content bias, we divide the observed coverage with the predicted (using targets' GC content). This new coverage ratio should be a slightly better measure of DNA abundance. targets [, coverage_ratio := coverage / 2 ^ predict ( loess_tumor , gc )] targets To compare the new metric with the previous, we can plot them side-by-side. With patchwork we can use arithmetic operators to combine and align multiple plots. library ( patchwork ) p1 <- ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 05 ) p2 <- ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage_ratio ), alpha = . 05 ) p1 + p2 p1 <- ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = target , y = coverage , fill = label ), shape = 21 ) p2 <- ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) p1 / p2 + plot_layout ( guides = 'collect' ) p1 <- ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage , fill = label ), shape = 21 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 4 )) p2 <- ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = start , y = coverage_ratio , fill = label ), shape = 21 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 4 )) p1 + p2 + plot_layout ( guides = 'collect' ) The GC content correction seems to have had a small but positive effect. It is common to also use either a matched normal or a pool of normal reference samples (sequenced similarly) to remove some assay-specific noise. We have omitted that step here. One important reason to perform copy number analysis of cancer samples is to find homozygous deletions of tumor suppressor genes. If a patient has a hemizygous germline deletion (loss of one copy) affecting a tumor suppressor gene, and somatic deletion of the healthy copy in their tumor, would the resulting homozygous deletion still be visible if the coverage ratio of the tumor sample was divided by that of the normal sample? Segment the data \u00b6 Although we can already spot some apparent gains and losses, statistical tools can help us better estimate copy number segments, for which we can then calculate the most likely copy number given the observation. Circular binary segmentation (CBS) is probably the most commonly used segmentation method. Here we use the PSCBS (\"Parent specific\" CBS, as it can also use SNP allele data) R package as a wrapper to perform basic CBS on the GC content-adjusted coverage ratio. CBS requires the DNA abundance ratio to be log-transformed, making its distribution more normal-like. Conveniently we have no zeroes in the data (that would become -Inf). library ( PSCBS ) targets [, log_ratio := log2 ( coverage_ratio )] targets segments <- segmentByCBS ( y = targets $ log_ratio ) segments After segmentation we can transform the segment mean values back and plot the segments with the targets. segments <- as.data.table ( segments )[, coverage_ratio := 2 ^ mean ] segments ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) Note that the segmentation is based only on the target log-ratio and order, and that segment start and end refer to target number. To avoid segments spanning across chromosomes, and breakpoints just missing the chromosome boundary, we can add chromosomes to the segmentation call. The chromosome vector is required to be numeric. segments <- segmentByCBS ( y = targets $ log_ratio , chromosome = as.numeric ( str_replace ( targets $ chromosome , 'X' , '23' ))) segments segments <- as.data.table ( segments )[, coverage_ratio := 2 ^ mean ][ ! is.na ( chromosome ), -1 ] segments ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) CBS can also take a vector of chromosomal positions as input, in which case the resulting segment start and end positions are based on them. We use the target midpoints: segments_pos <- segmentByCBS ( y = targets $ log_ratio , chromosome = as.numeric ( str_replace ( targets $ chromosome , 'X' , '23' )), x = targets $ start +60 ) segments_pos <- as.data.table ( segments_pos )[, coverage_ratio := 2 ^ mean ][ ! is.na ( chromosome ), -1 ] segments_pos # convert chromosomes back to NCBI segments_pos [, chromosome := str_replace ( as.character ( chromosome ), '23' , 'X' )] ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = start , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments_pos , col = 'green' , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 6 )) Target density is only about one per megabase, with much higher density in some cancer genes. How many targets do you think a deletion would have to cover for us to be able to find it? What else might influence sensitivity? Let's now take another look at segmented targets plotted in order, as they are very unevenly distributed over the genome. We can easily spot some deletions affecting 2-3 genes in our selection, as well as amplification of another. Take a closer look at BRCA2 and part of PTEN. p1 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p2 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p1 / p2 Coverage ratio (observed sequence coverage relative to the expected, given target GC content), is near 0.5, or 50%. For a largely diploid genome this is exactly what to expect if these segments were hemizygously deleted, i.e., one out of the two homologous copies would have been lost. This would also fit reasonably well with gain of 1 copy of 8q (resulting in a coverage ratio of 1.5) and 3 extra copies of AR (being on the X chromosome, tumor cells would have quadrupled its single original AR copy, resulting in the coverage ratio going from about 0.5 to near 2.). But this solution would require the tumor cell content (\" purity \") in the sample to be near 100%, which is rare unless the sample comes from a cell line. Note also that several other segments have a mean coverage ratio that would not fit this solution well. There are actually multiple other combinations of copy number and purity that would fit the observation reasonably well. Our measure of relative DNA abundance - coverage ratio - is automatically centered around 1, regardless of the average copy number (\" ploidy \") of the sequenced genome. Let's assume the cancer cells had twice the number of copies of all chromosomes. This would not double the average sequence coverage of the sample, and we would also not observe twice the sequence coverage relative to our GC content model for most of the genome. We have analyzed a certain amount of DNA, not a certain number of cells. The average DNA abundance measured throughout the genome, whether it is raw sequence coverage, coverage ratio, or log ratio, does not increase in samples with a higher average copy number (more DNA per cell). Instead the average measured DNA abundance will be observed at the average copy number, whatever that is. Does that make sense? We are also likely to have some normal DNA content in the sample. If both the tumor genome and normal genome are near diploid on average, and the tumor cell content is near 50%, homozygous deletion (loss of both homologous copies) would be observed at a coverge ratio near 0.5, as for every tumor cell that would contribute zero copies to the DNA extraction, one normal cell would contribute its normal two. As loss of one copy in the tumor cell fraction would then appear at about 0.75 and gains would appear near 1.25, 1.5, ..., this solution also appears to fit our data reasonably well. Fortunately there is another tool we can use to inform our copy number estimates. We can use SNPs from a small variant caller and investigate their variant allele ratios. Parse SNP allele ratio \u00b6 Let's use the VariantAnnotation package to parse a VCF file containing SNPs. library ( VariantAnnotation ) vcf <- readVcf ( 'Sample1.vcf' ) vcf g <- geno ( vcf ) g g $ AD as.data.table ( g $ AD ) as.data.table ( g $ DP ) The AD column of the VCF file contains the number of reads supporting the reference and alternative allele. The DP column contains the total read depth. Let's make a table of SNPs containing the alt-allele coverage ratio. snp_table <- data.table ( id = names ( vcf ), AD = sapply ( g $ AD , \"[[\" , 2 ), DP = unlist ( g $ DP [, 1 ])) snp_table snp_table [, allele_ratio := round ( AD / DP , 3 )] snp_table A GenomicRanges object defining the SNPs' positions on the reference genome can be accessed with the rowRanges() function. We can now overlap that with the GenomicRanges representing our targets and use the resulting Hits object to assign allele ratio to targets. overlaps <- findOverlaps ( target_ranges , rowRanges ( vcf )) overlaps targets $ allele_ratio <- NA targets $ allele_ratio [ queryHits ( overlaps )] <- snp_table $ allele_ratio [ subjectHits ( overlaps )] targets Let's plot the SNP allele ratio with the coverage ratio and see if the copy number status becomes more clear. Note that many targets contain no SNP, and that many SNPs are homozygous. The allele ratio of a homozygous SNP is 0 or 1 and unaffected by copy number alteration. p1 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p2 <- ggplot () + geom_point ( data = targets , mapping = aes ( x = target , y = allele_ratio , fill = label ), shape = 21 ) p1 / p2 + plot_layout ( guides = 'collect' ) We can see that for most segments, particularly segments with a coverage ratio near 1, heterozygous SNPs have allele ratios near 0.5. This indicates that the average copy number may be 2. What do you think is the tumor cell content, and what has most likely happened to PTEN and BRCA2? Estimating genome-wide copy number \u00b6 Although the ploidy and purity of this example are relatively clear, that is not always the case. There are several methods available that fits your data to potential ploidies and purities, and assigns the copy numbers corresponding to the best fit. Beware, this is somewhat prone to error, especially when the purity is below 50%, if there are few copy number alterations, or if there is some tumor cell heterogeneity. Also, reporting the total copy number of every segment does not always make sense. You should always review the result and curate if necessary. Investigate further \u00b6 Most copy number analysis tools use log-transformed coverage ratio (log-ratio). What may be the advantages of this? Homozygous deletions are rarely larger than one, occationally a few, megabases. What appears to be the sizes of the deletions affecting PTEN and BRCA2? Let's assume you suspect that this patient may have a TMPRSS2-ERG fusion. Is that supported by copy number data? Is there a MYC amplification? How many copies of AR would you say there are in each tumor cell? Let's assume we have a segment with somatic copy-neutral loss of heterozogosity, i.e. one of the two homologous copies was lost and the other duplicated. What coverage ratio would we observe in this sample, and at what allele ratio(s) would the (germline-heterozygous) SNPs likely appear?","title":"Copy Number Variations"},{"location":"lab_session/analysis_of_cnv/#analysis-of-copy-number-variation","text":"","title":"Analysis of copy number variation"},{"location":"lab_session/analysis_of_cnv/#copy-number-analysis-tools","text":"There are plenty of tools available for copy number analysis today, and you will likely be able to find one for free that suits your particular application. Although tailored for different types of sequence data, general processing steps of all copy number analysis tools include: 1. Quantification of sequence read depth throughout the reference genome as a measure of DNA abundance in the sample 1. Removal of sample-specific systematic noise using features such as GC content and mappability 1. Removal of assay-specific systematic noise using normal (non-cancer) reference samples 1. Segmentation - partitioning of the reference genome so that each segment can be assigned one copy number 1. Copy number calling, assigning each segment some estimate of the number of copies per cell 1. Combine normalized coverage with SNP allele frequencies to support copy number estimates In this exercise we will perform the above steps using R, and a BAM file of aligned reads and a VCF file of SNPs as input.","title":"Copy number analysis tools"},{"location":"lab_session/analysis_of_cnv/#download-data-and-prepare-r","text":"The data used in this exercise is available for download here: copy_number_files.tar.bz2 Extract the content into a local folder on your computer. tar xjvf copy_number_files.tar.bz2 Several packages from the Bioconductor repository are used in this exercise, most importantly GenomicRanges which facilitates working with anything located on the reference genome. Here is a good overview.","title":"Download data and prepare R"},{"location":"lab_session/analysis_of_cnv/#parse-sequence-target-definition","text":"We use targeted sequencing of a prostate cancer sample in this exercise, as the resulting data files are of a more convenient size than whole genome or exome. This hybrid-capture panel covers a few hundred cancer genes and a few thousands of additional targets intended to improve copy number and structural variant detection. In RStudio, navigate to the folder containing the exercise files: setwd ( 'your/path/to/exercise_folder' ) Tip: Open a new Rmarkdown file and save it in the exercise data folder. Keep your code as shown in the template, and write your comments and answers to questions between code chunks. Click knit any time to run all code and generate a pdf or html report. This way, when you are through this exercise, your report is done too. The BED (Browser Extensible Data) format is a text file format used to store genomic regions as coordinates and associated annotations. The data are presented in the form of columns separated by spaces or tabs. This format was developed during the Human Genome Project and then adopted by other sequencing projects. Parse and investigate the target definition BED file. library ( data.table ) bed_file <- 'targets.bed' file.exists ( bed_file ) targets <- fread ( bed_file ) targets # Let the first column be the target number targets <- cbind ( 1 : nrow ( targets ), targets ) # Set useful column names colnames ( targets ) <- c ( 'target' , 'chromosome' , 'start' , 'end' ) # Check target length targets [, length := end - start ] summary ( targets ) # Check target distribution over chromosomes targets [, table ( chromosome )] The R code in this exercise features some data.table syntax. If you are more familiar with base R or tidyverse syntax, a comparison between them can be found here .","title":"Parse sequence target definition"},{"location":"lab_session/analysis_of_cnv/#annotate-with-gene-symbol","text":"Before parsing the sequence data, let's assign gene symbols to targets that overlap a gene. We'll take advantage of databases accessible through Bioconductor packages. First, we need to create a GenomicRanges object representing our targets: library ( GenomicRanges ) target_ranges <- makeGRangesFromDataFrame ( targets ) target_ranges seqlevelsStyle ( target_ranges ) As you may recognize, chromosomes have the NCBI/EMBL sequence naming style (1,...). This matches the reference genome used with our sequence data. Let's create a second GenomicRanges object featuring the UCSC names (chr1,...) for use with some database queries. ucsc_ranges <- target_ranges seqlevelsStyle ( ucsc_ranges ) <- \"UCSC\" ucsc_ranges We then use the detailRanges function from the csaw package to overlap our target ranges with known genes. library ( org.Hs.eg.db ) library ( TxDb.Hsapiens.UCSC.hg19.knownGene ) library ( csaw ) d <- detailRanges ( ucsc_ranges , orgdb = org.Hs.eg.db , txdb = TxDb.Hsapiens.UCSC.hg19.knownGene ) d as.data.table ( d ) There are many ways to achieve the same thing. You should focus mostly on what we do, and not worry too much about exactly how. Google is your friend and if you know what you want do do, you can always find a way. We only care about some level of overlap at this point, let's just retrieve the gene symbols and add as a column to the table of targets. We can then see how targets allocate to genes. library ( stringr ) targets $ gene <- str_remove ( string = d $ overlap , pattern = ':.*' ) targets table ( targets $ gene ) as.data.table ( table ( targets $ gene )) as.data.table ( table ( targets $ gene ))[ N > 25 ]","title":"Annotate with gene symbol"},{"location":"lab_session/analysis_of_cnv/#parse-sequence-read-counts","text":"We are now ready to access the BAM file and count the number of read pairs (i.e. fragments) that map to each target. For this we use the bamsignals package and the GenomicRanges object with NCBI-style sequence names. To count a read pair, we require it's midpoint to fall within a target, and that it is not flagged as a duplicate . library ( bamsignals ) library ( Rsamtools ) tumor_bam <- 'Sample1.bam' file.exists ( tumor_bam ) file.exists ( paste0 ( tumor_bam , '.bai' )) targets $ coverage <- bamCount ( tumor_bam , target_ranges , paired.end = \"midpoint\" , filteredFlag = 1024 , verbose = F ) targets summary ( targets ) Strictly speaking, read count is not equal to sequence coverage. But given the fragment length and target size in this example, we can safely call the result sequence coverage. Zero coverage will be a problem later as it log-transforms to -Inf. We set those to 1: targets [ coverage == 0 , coverage := 1 ]","title":"Parse sequence read counts"},{"location":"lab_session/analysis_of_cnv/#plot-sequence-coverage","text":"Let's investigate the raw sequence coverage across targets. You are probably already familiar with ggplot2 . library ( ggplot2 ); theme_set ( theme_bw ()) # Simple ggplot ggplot ( data = targets ) + geom_point ( mapping = aes ( x = target , y = coverage )) # Some adjustments ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = target , y = coverage ), size = . 2 ) # To use start position, we can separate the plot by chromosome. ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage ), size = . 5 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) The figure may now look squished in RStudio's bottom-right plots pane. Click the Zoom button and maximize the new window. Choose a few genes to highlight in the plot. myGenes <- c ( 'AR' , 'ATM' , 'BRCA1' , 'BRCA2' , 'PTEN' , 'TMPRSS2' , 'ERG' ) targets $ label = '' targets [ gene %in% myGenes , label := gene ][ label == '' , label := NA ] ggplot () + ylim ( c ( 0 , 3500 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage , col = label ), size = . 2 ) ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage , col = label ), size = . 5 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 )","title":"Plot sequence coverage"},{"location":"lab_session/analysis_of_cnv/#model-and-correct-for-gc-content-bias","text":"Sequence GC content is known to affect PCR amplification and sequence coverage. Let's retrieve the GC content for the targets and investigate if there is a bias in our data set. library ( BSgenome.Hsapiens.UCSC.hg19 ) library ( BSgenome ) library ( Repitools ) targets $ gc <- gcContentCalc ( ucsc_ranges , organism = Hsapiens ) ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) There seems to be a modest effect of target GC content on sequence coverage. Let's build a loess model of the effect. We leave the X chromosome out. loess_tumor <- loess ( coverage ~ gc , data = targets , subset = chromosome != 'X' , family = \"symmetric\" , control = loess.control ( surface = \"direct\" )) To plot the model, we predict the sequence coverage for GC content ranging from 1% to 100%: tumor_gc_line <- data.table ( x = 1 : 100 / 100 , y = predict ( loess_tumor , data.table ( gc = 1 : 100 / 100 ))) ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) + geom_line ( data = tumor_gc_line , mapping = aes ( x = x , y = y ), col = 'blue' ) We seem to predict negative coverage at some high GC content. We'll try the same model on log2 of coverage. It will now return the predicted log2 coverage, which we can either keep using, or transform back to coverage. We'll keep using coverage for now. loess_tumor <- loess ( log2 ( coverage ) ~ gc , data = targets , subset = chromosome != 'X' , family = \"symmetric\" , control = loess.control ( surface = \"direct\" )) # prediction is log2(coverage), 2^prediction equals predicted coverage tumor_gc_line <- data.table ( x = 1 : 100 / 100 , y = 2 ^ predict ( loess_tumor , data.table ( gc = 1 : 100 / 100 ))) ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 2 ) + geom_line ( data = tumor_gc_line , mapping = aes ( x = x , y = y ), col = 'blue' ) To adjust for GC content bias, we divide the observed coverage with the predicted (using targets' GC content). This new coverage ratio should be a slightly better measure of DNA abundance. targets [, coverage_ratio := coverage / 2 ^ predict ( loess_tumor , gc )] targets To compare the new metric with the previous, we can plot them side-by-side. With patchwork we can use arithmetic operators to combine and align multiple plots. library ( patchwork ) p1 <- ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage ), alpha = . 05 ) p2 <- ggplot ( data = targets ) + geom_point ( mapping = aes ( x = gc , y = coverage_ratio ), alpha = . 05 ) p1 + p2 p1 <- ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = target , y = coverage , fill = label ), shape = 21 ) p2 <- ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) p1 / p2 + plot_layout ( guides = 'collect' ) p1 <- ggplot ( data = targets ) + ylim ( c ( 0 , 3500 )) + geom_point ( mapping = aes ( x = start , y = coverage , fill = label ), shape = 21 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 4 )) p2 <- ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = start , y = coverage_ratio , fill = label ), shape = 21 ) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 4 )) p1 + p2 + plot_layout ( guides = 'collect' ) The GC content correction seems to have had a small but positive effect. It is common to also use either a matched normal or a pool of normal reference samples (sequenced similarly) to remove some assay-specific noise. We have omitted that step here. One important reason to perform copy number analysis of cancer samples is to find homozygous deletions of tumor suppressor genes. If a patient has a hemizygous germline deletion (loss of one copy) affecting a tumor suppressor gene, and somatic deletion of the healthy copy in their tumor, would the resulting homozygous deletion still be visible if the coverage ratio of the tumor sample was divided by that of the normal sample?","title":"Model and correct for GC content bias"},{"location":"lab_session/analysis_of_cnv/#segment-the-data","text":"Although we can already spot some apparent gains and losses, statistical tools can help us better estimate copy number segments, for which we can then calculate the most likely copy number given the observation. Circular binary segmentation (CBS) is probably the most commonly used segmentation method. Here we use the PSCBS (\"Parent specific\" CBS, as it can also use SNP allele data) R package as a wrapper to perform basic CBS on the GC content-adjusted coverage ratio. CBS requires the DNA abundance ratio to be log-transformed, making its distribution more normal-like. Conveniently we have no zeroes in the data (that would become -Inf). library ( PSCBS ) targets [, log_ratio := log2 ( coverage_ratio )] targets segments <- segmentByCBS ( y = targets $ log_ratio ) segments After segmentation we can transform the segment mean values back and plot the segments with the targets. segments <- as.data.table ( segments )[, coverage_ratio := 2 ^ mean ] segments ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) Note that the segmentation is based only on the target log-ratio and order, and that segment start and end refer to target number. To avoid segments spanning across chromosomes, and breakpoints just missing the chromosome boundary, we can add chromosomes to the segmentation call. The chromosome vector is required to be numeric. segments <- segmentByCBS ( y = targets $ log_ratio , chromosome = as.numeric ( str_replace ( targets $ chromosome , 'X' , '23' ))) segments segments <- as.data.table ( segments )[, coverage_ratio := 2 ^ mean ][ ! is.na ( chromosome ), -1 ] segments ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) CBS can also take a vector of chromosomal positions as input, in which case the resulting segment start and end positions are based on them. We use the target midpoints: segments_pos <- segmentByCBS ( y = targets $ log_ratio , chromosome = as.numeric ( str_replace ( targets $ chromosome , 'X' , '23' )), x = targets $ start +60 ) segments_pos <- as.data.table ( segments_pos )[, coverage_ratio := 2 ^ mean ][ ! is.na ( chromosome ), -1 ] segments_pos # convert chromosomes back to NCBI segments_pos [, chromosome := str_replace ( as.character ( chromosome ), '23' , 'X' )] ggplot ( data = targets ) + ylim ( c ( 0 , 2.5 )) + geom_point ( mapping = aes ( x = start , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments_pos , col = 'green' , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) + facet_wrap ( facets = vars ( chromosome ), ncol = 2 ) + theme ( panel.spacing = unit ( 0 , \"lines\" ), strip.text.x = element_text ( size = 6 )) Target density is only about one per megabase, with much higher density in some cancer genes. How many targets do you think a deletion would have to cover for us to be able to find it? What else might influence sensitivity? Let's now take another look at segmented targets plotted in order, as they are very unevenly distributed over the genome. We can easily spot some deletions affecting 2-3 genes in our selection, as well as amplification of another. Take a closer look at BRCA2 and part of PTEN. p1 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = chromosome ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p2 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p1 / p2 Coverage ratio (observed sequence coverage relative to the expected, given target GC content), is near 0.5, or 50%. For a largely diploid genome this is exactly what to expect if these segments were hemizygously deleted, i.e., one out of the two homologous copies would have been lost. This would also fit reasonably well with gain of 1 copy of 8q (resulting in a coverage ratio of 1.5) and 3 extra copies of AR (being on the X chromosome, tumor cells would have quadrupled its single original AR copy, resulting in the coverage ratio going from about 0.5 to near 2.). But this solution would require the tumor cell content (\" purity \") in the sample to be near 100%, which is rare unless the sample comes from a cell line. Note also that several other segments have a mean coverage ratio that would not fit this solution well. There are actually multiple other combinations of copy number and purity that would fit the observation reasonably well. Our measure of relative DNA abundance - coverage ratio - is automatically centered around 1, regardless of the average copy number (\" ploidy \") of the sequenced genome. Let's assume the cancer cells had twice the number of copies of all chromosomes. This would not double the average sequence coverage of the sample, and we would also not observe twice the sequence coverage relative to our GC content model for most of the genome. We have analyzed a certain amount of DNA, not a certain number of cells. The average DNA abundance measured throughout the genome, whether it is raw sequence coverage, coverage ratio, or log ratio, does not increase in samples with a higher average copy number (more DNA per cell). Instead the average measured DNA abundance will be observed at the average copy number, whatever that is. Does that make sense? We are also likely to have some normal DNA content in the sample. If both the tumor genome and normal genome are near diploid on average, and the tumor cell content is near 50%, homozygous deletion (loss of both homologous copies) would be observed at a coverge ratio near 0.5, as for every tumor cell that would contribute zero copies to the DNA extraction, one normal cell would contribute its normal two. As loss of one copy in the tumor cell fraction would then appear at about 0.75 and gains would appear near 1.25, 1.5, ..., this solution also appears to fit our data reasonably well. Fortunately there is another tool we can use to inform our copy number estimates. We can use SNPs from a small variant caller and investigate their variant allele ratios.","title":"Segment the data"},{"location":"lab_session/analysis_of_cnv/#parse-snp-allele-ratio","text":"Let's use the VariantAnnotation package to parse a VCF file containing SNPs. library ( VariantAnnotation ) vcf <- readVcf ( 'Sample1.vcf' ) vcf g <- geno ( vcf ) g g $ AD as.data.table ( g $ AD ) as.data.table ( g $ DP ) The AD column of the VCF file contains the number of reads supporting the reference and alternative allele. The DP column contains the total read depth. Let's make a table of SNPs containing the alt-allele coverage ratio. snp_table <- data.table ( id = names ( vcf ), AD = sapply ( g $ AD , \"[[\" , 2 ), DP = unlist ( g $ DP [, 1 ])) snp_table snp_table [, allele_ratio := round ( AD / DP , 3 )] snp_table A GenomicRanges object defining the SNPs' positions on the reference genome can be accessed with the rowRanges() function. We can now overlap that with the GenomicRanges representing our targets and use the resulting Hits object to assign allele ratio to targets. overlaps <- findOverlaps ( target_ranges , rowRanges ( vcf )) overlaps targets $ allele_ratio <- NA targets $ allele_ratio [ queryHits ( overlaps )] <- snp_table $ allele_ratio [ subjectHits ( overlaps )] targets Let's plot the SNP allele ratio with the coverage ratio and see if the copy number status becomes more clear. Note that many targets contain no SNP, and that many SNPs are homozygous. The allele ratio of a homozygous SNP is 0 or 1 and unaffected by copy number alteration. p1 <- ggplot () + ylim ( c ( 0 , 2.5 )) + geom_point ( data = targets , mapping = aes ( x = target , y = coverage_ratio , fill = label ), shape = 21 ) + geom_segment ( data = segments , col = 'green' , size = 2 , mapping = aes ( x = start , xend = end , y = coverage_ratio , yend = coverage_ratio )) p2 <- ggplot () + geom_point ( data = targets , mapping = aes ( x = target , y = allele_ratio , fill = label ), shape = 21 ) p1 / p2 + plot_layout ( guides = 'collect' ) We can see that for most segments, particularly segments with a coverage ratio near 1, heterozygous SNPs have allele ratios near 0.5. This indicates that the average copy number may be 2. What do you think is the tumor cell content, and what has most likely happened to PTEN and BRCA2?","title":"Parse SNP allele ratio"},{"location":"lab_session/analysis_of_cnv/#estimating-genome-wide-copy-number","text":"Although the ploidy and purity of this example are relatively clear, that is not always the case. There are several methods available that fits your data to potential ploidies and purities, and assigns the copy numbers corresponding to the best fit. Beware, this is somewhat prone to error, especially when the purity is below 50%, if there are few copy number alterations, or if there is some tumor cell heterogeneity. Also, reporting the total copy number of every segment does not always make sense. You should always review the result and curate if necessary.","title":"Estimating genome-wide copy number"},{"location":"lab_session/analysis_of_cnv/#investigate-further","text":"Most copy number analysis tools use log-transformed coverage ratio (log-ratio). What may be the advantages of this? Homozygous deletions are rarely larger than one, occationally a few, megabases. What appears to be the sizes of the deletions affecting PTEN and BRCA2? Let's assume you suspect that this patient may have a TMPRSS2-ERG fusion. Is that supported by copy number data? Is there a MYC amplification? How many copies of AR would you say there are in each tumor cell? Let's assume we have a segment with somatic copy-neutral loss of heterozogosity, i.e. one of the two homologous copies was lost and the other duplicated. What coverage ratio would we observe in this sample, and at what allele ratio(s) would the (germline-heterozygous) SNPs likely appear?","title":"Investigate further"},{"location":"lab_session/annotation_files/","text":"Annotation files \u00b6 Files for annotation of variation in the human genome \u00b6 #These are available from the Broad institute in Boston, who also provides the GATK software suite. cd ~/workspace/inputs/references/gatk #Download the following files: # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Have a loot what the files contain. # Dont worry, we will look at VCF files later. gunzip -c 1000G_phase1.snps.high_confidence.hg38.vcf.gz | less -SN # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh Index the variation files \u00b6 cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz Interval files and coordinates for the exome sequencing assay \u00b6 These files contain the location of the exons in the human genome that were \"targeted\", that is for which targeting oligos were created (primary_targets). The files also contain the location of the actual oligos (capture_targets). # change directories cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # the software is availble from USCS, downloadble here: #wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver #chmod +x liftOver # the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed Calculate the size of the SeqCap v3 exome \u00b6 #This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed Create an inverval list for the exome bed files \u00b6 # first for the complete exome and probe bed file cd ~/workspace/inputs/references/ #Commands below already run #mkdir temp cd temp #Commands below already run #wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict #rm -fr ~/workspace/inputs/references/temp/ #Explore the interval-lists and what the files contain, scroll down until the coordinates are shown less -SN SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Annotation"},{"location":"lab_session/annotation_files/#annotation-files","text":"","title":"Annotation files"},{"location":"lab_session/annotation_files/#files-for-annotation-of-variation-in-the-human-genome","text":"#These are available from the Broad institute in Boston, who also provides the GATK software suite. cd ~/workspace/inputs/references/gatk #Download the following files: # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Have a loot what the files contain. # Dont worry, we will look at VCF files later. gunzip -c 1000G_phase1.snps.high_confidence.hg38.vcf.gz | less -SN # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh","title":"Files for annotation of variation in the human genome"},{"location":"lab_session/annotation_files/#index-the-variation-files","text":"cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz","title":"Index the variation files"},{"location":"lab_session/annotation_files/#interval-files-and-coordinates-for-the-exome-sequencing-assay","text":"These files contain the location of the exons in the human genome that were \"targeted\", that is for which targeting oligos were created (primary_targets). The files also contain the location of the actual oligos (capture_targets). # change directories cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # the software is availble from USCS, downloadble here: #wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver #chmod +x liftOver # the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed","title":"Interval files and coordinates for the exome sequencing assay"},{"location":"lab_session/annotation_files/#calculate-the-size-of-the-seqcap-v3-exome","text":"#This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed","title":"Calculate the size of the SeqCap v3 exome"},{"location":"lab_session/annotation_files/#create-an-inverval-list-for-the-exome-bed-files","text":"# first for the complete exome and probe bed file cd ~/workspace/inputs/references/ #Commands below already run #mkdir temp cd temp #Commands below already run #wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict #rm -fr ~/workspace/inputs/references/temp/ #Explore the interval-lists and what the files contain, scroll down until the coordinates are shown less -SN SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Create an inverval list for the exome bed files"},{"location":"lab_session/aws_env/","text":"The workspace on AWS and installations \u00b6 Due to the current covid-situation in Sweden we converted the course from on-site to on-line using zoom for lectures and the Amazon Web Services (AWS) for the practical sessions. Each student will get an \"instance\" of a linux computer in AWS to perform the labwork. During the labwork we will also use zoom for discussions and support. To be able to connect to AWS on windows you need to be able to download the SSH client PuTTY ( https://www.putty.org ). If you have a mac or a linux/unix computer nothing is needed to preinstall to work on the comman line. To perform the exercises of the course you also need to install the following software on your local machine: IGV . Choose IGV that comes bundled with Java for your operating system. This workshop requires a large number of different bioinformatics tools. Thes have been pre-installed on AWS. For installation instructions of the differnet tools, see the list at the end of this document which links to the website of each software tool. R \u00b6 R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor . If you have not installed R yet: - Download R to your local machine: - https://www.r-project.org - Double-click on the icon and start the installation process. Install the following R libraries on your local machine: #Open R and run the following code: packages <- c ( \"devtools\" , \"BiocManager\" , \"dplyr\" , \"tidyr\" , \"ggplot2\" , \"data.table\" , \"patchwork\" , \"stringr\" , \"jsonlite\" , \"reshape2\" , \"cowplot\" ) install.packages ( packages , dependencies = TRUE ) BiocManager :: install ( \"biomaRt\" , \"Biostrings\" , \"ensembldb\" , \"IRanges\" , \"EnsDb.Hsapiens.v86\" , \"bamsignals\" , \"BSgenome\" , \"BSgenome.Hsapiens.NCBI.hg19\" , \"csaw\" , \"DNAcopy\" , \"GenomicRanges\" , \"org.Hs.eg.db\" , \"Rsamtools\" , \"Repitools\" , \"TxDb.Hsapiens.UCSC.hg19.knownGene\" , \"VariantAnnotation\" , \"chimeraviz\" dependencies = TRUE ) # PSCBS requires DNAcopy install.packages ( \"PSCBS\" ) Note during the course you will be visualising data using R. If some of the libraries do not work on your local machine, you can run R and plot on AWS instead. However, then the plots will have to be dowloaded as pdf-files instead of being viewed interactively in R-studio. The above mentioned libraries are installed in R, available in the following path, /home/ubuntu/miniconda3/envs/r-rnaseq/bin/R , to access this R env follow the below instructions. - Using the command line do: # Start R terminal without GUI /home/ubuntu/miniconda3/envs/r-rnaseq/bin/R - Using R do: # Load libraries. Note, not all are needed every time, # just check here that they load without error. # Load each library and wait for it to finish before continuing library ( BiocManager ) library ( dplyr ) library ( tidyr ) library ( ggplot2 ) library ( IRanges ) library ( data.table ) library ( patchwork ) library ( PSCBS ) library ( stringr ) library ( jsonlite ) library ( reshape2 ) library ( cowplot ) library ( biomaRt ) library ( Biostrings ) library ( ensembldb ) library ( EnsDb.Hsapiens.v86 ) library ( bamsignals ) library ( BSgenome ) library ( BSgenome.Hsapiens.NCBI.hg19 ) library ( csaw ) library ( DNAcopy ) library ( GenomicRanges ) library ( org.Hs.eg.db ) library ( Rsamtools ) library ( Repitools ) library ( TxDb.Hsapiens.UCSC.hg19.knownGene ) library ( VariantAnnotation ) library ( chimeraviz ) # Check your current directory getwd () # Set to your home directory or some other directory setwd ( ~ ) # here we can run the R scripts # Let us test to plot and save as pdf using pre-loaded data in R # Check the data mtcars # Assign a plot to p p <- ggplot ( mtcars , aes ( mpg , wt )) + geom_point () # Save the p plot to the mtcars.pdf ggsave ( filename = \"mtcars.pdf\" , plot = p ) # How to download the pdf-file to your local machine, # see the section below, however first quit R. \"no\" # means that the R-workspace will not be saved. q ( \"no\" ) How to use the terminal download files to your local machine \u00b6 This will can be used for both downloading files plotted in R on AWS and other files and file types, such as bam-files that will be visualised in IGV. cd TO_PREFERRED_DIRECTORY scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/PATH_TO_PDF/FILENAME . # Before proceeding - test that you can get the \"mtcars.pdf\" file # downloaded to your local computer. If not, ask for assistance. Explore the workspace on AWS \u00b6 #Make sure the credentials are set for the \"pem\" (key) file. # If needed run: # chmod 400 course_ec2_new.pem # Log onto AWS cloud (make sure in are in the same directory as the pem key). ssh -i ./PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE # Check where you are pwd # Note \"~\"\" is a shortcut for the user directory. # Pre-load shortcut and settings source .bashrc # the workspace folder contains the course files and pre-installed bioinformatic tools cd workspace/ # The workspace folder contains the course files and pre-installed bioinformatic tools. Navigate to the folders, list the files and reflecton what is there cd ~/workspace/inputs ls -alF # Look around in the folders cd ~/workspace/bin ls -alF # Look around in the folders # Have a look at the raw exone sequencing data for the germline DNA cd ~/workspace/inputs/data/fastq/Exome_Norm # What are the files in this folder? # Explore the file format (gunzip unpacks zipped files) gunzip -c Exome_Norm_R1.fastq.gz | less -SN # The format is nicely described here: # https://en.wikipedia.org/wiki/FASTQ_format # Read through the fastq format to get an understanding of # base qualities from illumina DNA sequencing data. # The format will be discussed tomorrow during the lab intro. List of preinstalled bioinformatic tools \u00b6 bam-readcount \u00b6 bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). BCFtools \u00b6 BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). BWA \u00b6 BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. CNVkit \u00b6 CNVkit is a python based copy number caller designed for use with hybrid capture. It will not be applied during the course but is a frequently used tool for CNV-analysis of copy-number data. Delly \u00b6 Delly is a structural variant caller developed at EMBL. It uses paired-ends, split-reads and read-depth to sensitively and accurately delineate genomic rearrangements throughout the genome. FastQC \u00b6 FastQC is a quality control program for raw sequencing data. GATK 4 \u00b6 GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. Gffcompare \u00b6 Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. HISAT2 \u00b6 HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. Kallisto \u00b6 Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. mosdepth \u00b6 mosdepth is a program for determining depth in sequencing data. MultiQC \u00b6 MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples. PICARD \u00b6 PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. Try to answer the following: Who is Picard? Clue: \"TNG\" Pizzly \u00b6 Pizzly is a fusion detection algorithm which uses output from Kallisto. Sambamba \u00b6 Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. Samtools \u00b6 Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. seqtk \u00b6 Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. Strelka \u00b6 Strelka is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. StringTie \u00b6 StringTie is a software program to perform transcript assembly and quantification of RNAseq data. Varscan \u00b6 Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. vcf-annotation-tools \u00b6 VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. VEP \u00b6 VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. vt \u00b6 vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants.","title":"Workspace - AWS"},{"location":"lab_session/aws_env/#the-workspace-on-aws-and-installations","text":"Due to the current covid-situation in Sweden we converted the course from on-site to on-line using zoom for lectures and the Amazon Web Services (AWS) for the practical sessions. Each student will get an \"instance\" of a linux computer in AWS to perform the labwork. During the labwork we will also use zoom for discussions and support. To be able to connect to AWS on windows you need to be able to download the SSH client PuTTY ( https://www.putty.org ). If you have a mac or a linux/unix computer nothing is needed to preinstall to work on the comman line. To perform the exercises of the course you also need to install the following software on your local machine: IGV . Choose IGV that comes bundled with Java for your operating system. This workshop requires a large number of different bioinformatics tools. Thes have been pre-installed on AWS. For installation instructions of the differnet tools, see the list at the end of this document which links to the website of each software tool.","title":"The workspace on AWS and installations"},{"location":"lab_session/aws_env/#r","text":"R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor . If you have not installed R yet: - Download R to your local machine: - https://www.r-project.org - Double-click on the icon and start the installation process. Install the following R libraries on your local machine: #Open R and run the following code: packages <- c ( \"devtools\" , \"BiocManager\" , \"dplyr\" , \"tidyr\" , \"ggplot2\" , \"data.table\" , \"patchwork\" , \"stringr\" , \"jsonlite\" , \"reshape2\" , \"cowplot\" ) install.packages ( packages , dependencies = TRUE ) BiocManager :: install ( \"biomaRt\" , \"Biostrings\" , \"ensembldb\" , \"IRanges\" , \"EnsDb.Hsapiens.v86\" , \"bamsignals\" , \"BSgenome\" , \"BSgenome.Hsapiens.NCBI.hg19\" , \"csaw\" , \"DNAcopy\" , \"GenomicRanges\" , \"org.Hs.eg.db\" , \"Rsamtools\" , \"Repitools\" , \"TxDb.Hsapiens.UCSC.hg19.knownGene\" , \"VariantAnnotation\" , \"chimeraviz\" dependencies = TRUE ) # PSCBS requires DNAcopy install.packages ( \"PSCBS\" ) Note during the course you will be visualising data using R. If some of the libraries do not work on your local machine, you can run R and plot on AWS instead. However, then the plots will have to be dowloaded as pdf-files instead of being viewed interactively in R-studio. The above mentioned libraries are installed in R, available in the following path, /home/ubuntu/miniconda3/envs/r-rnaseq/bin/R , to access this R env follow the below instructions. - Using the command line do: # Start R terminal without GUI /home/ubuntu/miniconda3/envs/r-rnaseq/bin/R - Using R do: # Load libraries. Note, not all are needed every time, # just check here that they load without error. # Load each library and wait for it to finish before continuing library ( BiocManager ) library ( dplyr ) library ( tidyr ) library ( ggplot2 ) library ( IRanges ) library ( data.table ) library ( patchwork ) library ( PSCBS ) library ( stringr ) library ( jsonlite ) library ( reshape2 ) library ( cowplot ) library ( biomaRt ) library ( Biostrings ) library ( ensembldb ) library ( EnsDb.Hsapiens.v86 ) library ( bamsignals ) library ( BSgenome ) library ( BSgenome.Hsapiens.NCBI.hg19 ) library ( csaw ) library ( DNAcopy ) library ( GenomicRanges ) library ( org.Hs.eg.db ) library ( Rsamtools ) library ( Repitools ) library ( TxDb.Hsapiens.UCSC.hg19.knownGene ) library ( VariantAnnotation ) library ( chimeraviz ) # Check your current directory getwd () # Set to your home directory or some other directory setwd ( ~ ) # here we can run the R scripts # Let us test to plot and save as pdf using pre-loaded data in R # Check the data mtcars # Assign a plot to p p <- ggplot ( mtcars , aes ( mpg , wt )) + geom_point () # Save the p plot to the mtcars.pdf ggsave ( filename = \"mtcars.pdf\" , plot = p ) # How to download the pdf-file to your local machine, # see the section below, however first quit R. \"no\" # means that the R-workspace will not be saved. q ( \"no\" )","title":"R"},{"location":"lab_session/aws_env/#how-to-use-the-terminal-download-files-to-your-local-machine","text":"This will can be used for both downloading files plotted in R on AWS and other files and file types, such as bam-files that will be visualised in IGV. cd TO_PREFERRED_DIRECTORY scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/PATH_TO_PDF/FILENAME . # Before proceeding - test that you can get the \"mtcars.pdf\" file # downloaded to your local computer. If not, ask for assistance.","title":"How to use the terminal download files to your local machine"},{"location":"lab_session/aws_env/#explore-the-workspace-on-aws","text":"#Make sure the credentials are set for the \"pem\" (key) file. # If needed run: # chmod 400 course_ec2_new.pem # Log onto AWS cloud (make sure in are in the same directory as the pem key). ssh -i ./PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE # Check where you are pwd # Note \"~\"\" is a shortcut for the user directory. # Pre-load shortcut and settings source .bashrc # the workspace folder contains the course files and pre-installed bioinformatic tools cd workspace/ # The workspace folder contains the course files and pre-installed bioinformatic tools. Navigate to the folders, list the files and reflecton what is there cd ~/workspace/inputs ls -alF # Look around in the folders cd ~/workspace/bin ls -alF # Look around in the folders # Have a look at the raw exone sequencing data for the germline DNA cd ~/workspace/inputs/data/fastq/Exome_Norm # What are the files in this folder? # Explore the file format (gunzip unpacks zipped files) gunzip -c Exome_Norm_R1.fastq.gz | less -SN # The format is nicely described here: # https://en.wikipedia.org/wiki/FASTQ_format # Read through the fastq format to get an understanding of # base qualities from illumina DNA sequencing data. # The format will be discussed tomorrow during the lab intro.","title":"Explore the workspace on AWS"},{"location":"lab_session/aws_env/#list-of-preinstalled-bioinformatic-tools","text":"","title":"List of preinstalled bioinformatic tools"},{"location":"lab_session/aws_env/#bam-readcount","text":"bam-readcount is a program for determing read support for individual variants (SNVs and Indels only).","title":"bam-readcount"},{"location":"lab_session/aws_env/#bcftools","text":"BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF).","title":"BCFtools"},{"location":"lab_session/aws_env/#bwa","text":"BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license.","title":"BWA"},{"location":"lab_session/aws_env/#cnvkit","text":"CNVkit is a python based copy number caller designed for use with hybrid capture. It will not be applied during the course but is a frequently used tool for CNV-analysis of copy-number data.","title":"CNVkit"},{"location":"lab_session/aws_env/#delly","text":"Delly is a structural variant caller developed at EMBL. It uses paired-ends, split-reads and read-depth to sensitively and accurately delineate genomic rearrangements throughout the genome.","title":"Delly"},{"location":"lab_session/aws_env/#fastqc","text":"FastQC is a quality control program for raw sequencing data.","title":"FastQC"},{"location":"lab_session/aws_env/#gatk-4","text":"GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license.","title":"GATK 4"},{"location":"lab_session/aws_env/#gffcompare","text":"Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files.","title":"Gffcompare"},{"location":"lab_session/aws_env/#hisat2","text":"HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments.","title":"HISAT2"},{"location":"lab_session/aws_env/#kallisto","text":"Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data.","title":"Kallisto"},{"location":"lab_session/aws_env/#mosdepth","text":"mosdepth is a program for determining depth in sequencing data.","title":"mosdepth"},{"location":"lab_session/aws_env/#multiqc","text":"MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.","title":"MultiQC"},{"location":"lab_session/aws_env/#picard","text":"PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. Try to answer the following: Who is Picard? Clue: \"TNG\"","title":"PICARD"},{"location":"lab_session/aws_env/#pizzly","text":"Pizzly is a fusion detection algorithm which uses output from Kallisto.","title":"Pizzly"},{"location":"lab_session/aws_env/#sambamba","text":"Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality.","title":"Sambamba"},{"location":"lab_session/aws_env/#samtools","text":"Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license.","title":"Samtools"},{"location":"lab_session/aws_env/#seqtk","text":"Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module.","title":"seqtk"},{"location":"lab_session/aws_env/#strelka","text":"Strelka is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license.","title":"Strelka"},{"location":"lab_session/aws_env/#stringtie","text":"StringTie is a software program to perform transcript assembly and quantification of RNAseq data.","title":"StringTie"},{"location":"lab_session/aws_env/#varscan","text":"Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github.","title":"Varscan"},{"location":"lab_session/aws_env/#vcf-annotation-tools","text":"VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files.","title":"vcf-annotation-tools"},{"location":"lab_session/aws_env/#vep","text":"VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download.","title":"VEP"},{"location":"lab_session/aws_env/#vt","text":"vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants.","title":"vt"},{"location":"lab_session/basic_unix/","text":"Unix for Bioinformatics \u00b6 Outline: \u00b6 What is the command line? Directory Structure Syntax of a Command Options of a Command Command Line Basics (ls, pwd, Ctrl-C, man, alias, ls -lthra) Getting Around (cd) Absolute and Relative Paths Tab Completion History Repeats Itself (history, head, tail, ) Editing Yourself (Ctrl-A, Ctrl-E, Ctrl-K, Ctrl-W) Create and Destroy (echo, cat, rm, rmdir) Transferring Files (scp) Piping and Redirection (|, >, \u00bb, cut, sort, grep) Compressions and Archives (tar, gzip, gunzip) Forced Removal (rm -r) BASH Wildcard Characters (?, *, find, environment variables($), quotes/ticks) Manipulation of a FASTA file (cp, mv, wc -l/-c) Symbolic Links (ln -s) STDOUT and STDERR (>1, >2) Paste Command (paste, for loops) Shell Scripts and File Permissions (chmod, nano, ./) A Simple Unix cheat sheet \u00b6 Download Unix Cheat Sheet What is UNIX? \u00b6 UNIX is an operating system which was first developed in the 1960s, and has been under constant development ever since. By operating system, we mean the suite of programs which make the computer work. It is a stable, multi-user, multi-tasking system for servers, desktops and laptops. UNIX systems also have a graphical user interface (GUI) similar to Microsoft Windows which provides an easy to use environment. However, knowledge of UNIX is required for operations which aren't covered by a graphical program, or for when there is no windows interface available, for example, in a telnet session. Types of UNIX \u00b6 There are many different versions of UNIX, although they share common similarities. The most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X. Here in the School, we use Solaris on our servers and workstations, and Fedora Linux on the servers and desktop PCs. The UNIX operating system \u00b6 The UNIX operating system is made up of three parts; the kernel, the shell and the programs. The kernel \u00b6 The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore and communications in response to system calls. As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile (which has the effect of removing the file myfile). The shell searches the filestore for the file containing the program rm, and then requests the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished running, the shell then returns the UNIX prompt % to the user, indicating that it is waiting for further commands. The shell \u00b6 The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt (% on our systems). The adept user can customise his/her own shell, and users can use different shells on the same machine. This shell is an all-text display (most of the time your mouse doesn\u2019t work) and is accessed using an application called the \"terminal\" which usually looks like a black window with white letters or a white window with black letters by default. The tcsh shell has certain features to help the user inputting commands. Filename Completion - By typing part of the name of a command, filename or directory and pressing the [Tab] key, the tcsh shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will beep, prompting you to type a few more letters before pressing the tab key again. History - The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or type history for a list of previous commands. Files and processes \u00b6 Everything in UNIX is either a file or a process. A process is an executing program identified by a unique PID (process identifier). A file is a collection of data. They are created by users using text editors, running compilers etc. Examples of files: a document (report, essay etc.) the text of a program written in some high-level programming language instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file); a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files. The Directory Structure \u00b6 All the files are grouped together in the directory structure. The file-system is arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root (written as a slash / ) Absolute path: always starts with \u201d/\u201d - the root folder /home/ubuntu/workspace the folder (or file) \u201cworkspace\u201d in the folder \u201cubuntu\u201d or \" student#1 \" in the folder \u201chome\u201d in the folder from the root. Relative path: always relative to our current location. a single dot (.) refers to the current directory two dots (..) refers to the directory one level up \u00b6 Usually, /home is where the user accounts reside, ie. users\u2019 \u2018home\u2019 directories. For example, for a user that has a username of \u201c student#1 \u201d: their home directory is /home/student1. It is the directory that a user starts in after starting a new shell or logging into a remote server. The tilde (~) is a short form of a user\u2019s home directory. Starting an UNIX terminal \u00b6 \u00b6 After opening or logging into a terminal, system messages are often displayed, followed by the \u201cprompt\u201d. A prompt is a short text message at the start of the command line and ends with a $ in bash shell, commands are typed after the prompt. The prompt typically follows the form username@server:current_directory $ . If your screen looks like the one below, i.e. your see your a bunch of messages and then your ubuntu student instance number followed by \u201c @172 .31.84.105:~$\u201d at the beginning of the line, then you are successfully logged in. Unix Basics \u00b6 First some basics - how to look at your surroundings present working directory \u2026 where am I? \u00b6 pwd see the words separated by /. this is called the path. In unix, the location of each folder or file is shown like this.. this is more like the address or the way to find a folder or file. for example, my Desktop, Documents and Downloads folder usually are located in my home directory so in that case, it path to the folders will be /home/ubuntu/Desktop /home/ubuntu/Documents /home/ubuntu/Downloads Syntax of a command \u00b6 A command plus the required parameters/arguments The separator used in issuing a command is space, number of spaces does not matter. Now let's try some basic and simple commands \u00b6 list files here\u2026 you should see just the folders that we have created for you here and nothing else. ls list files somewhere else, like /tmp/ ls /tmp TIP! \u00b6 In unix one of the first things that\u2019s good to know is how to escape once you\u2019ve started something you don\u2019t want. Use Ctrl-c (shows as \u2018^C\u2019 in the terminal) to exit (kill) a command. In some cases, a different key sequence is required (Ctrl-d). Note that anything including and after a \u201c#\u201d symbol is ignored, i.e. a comment. So in all the commands below, you do not have to type anything including and past a \u201c#\u201d. Options \u00b6 Each command can act as a basic tool, or you can add \u2018options\u2019 or \u2018flags\u2019 that modify the default behavior of the tool. These flags come in the form of \u2018-v\u2019 \u2026 or, when it\u2019s a more descriptive word, two dashes: \u2018--verbose\u2019 \u2026 that\u2019s a common (but not universal) one that tells a tool that you want it to give you output with more detail. Sometimes, options require specifying amounts or strings, like \u2018-o results.txt\u2019 or \u2018--output results.txt\u2019 \u2026 or \u2018-n 4\u2019 or \u2018--numCPUs 4\u2019. Let\u2019s try some, and see what the man page for the \u2018list files\u2019 command \u2018ls\u2019 is like. ls -R Lists directories and files recursively. This will be a very long output, so use Ctrl-C to break out of it. Sometimes you have to press Ctrl-C many times to get the terminal to recognize it. In order to know which options do what, you can use the manual pages. To look up a command in the manual pages type \u201cman\u201d and then the command name. So to look up the options for \u201cls\u201d, type: man ls Navigate this page using the up and down arrow keys, PageUp and PageDown, and then use q to quit out of the manual. In this manual page, find the following options, quit the page, and then try those commands. You could even open another terminal, log in again, and run manual commands in that terminal. ls -l /usr/bin/ #long format, gives permission values, owner, group, size, modification time, and name Exercise: \u00b6 Feel free to see manual pages for these basic commands Commands and their Meanings man ls ls -a man mkdir Now see the difference between these three commands cd cd ~ cd .. change to parent directory Also these commands ls -l ls -a ls -l -a ls -la ls -ltrha ls -ltrha --color Quick aside: what if I want to use same options repeatedly? and be lazy? You can create a shortcut to another command using \u2018alias\u2019. alias ll = 'ls -lah' ll Getting Around \u00b6 The filesystem you\u2019re working on is like the branching root system of a tree. The top level, right at the root of the tree, is called the \u2018root\u2019 directory, specified by \u2018/\u2019 \u2026 which is the divider for directory addresses, or \u2018paths\u2019. Now let's see a little about the commands you checked in the exercise above. We move around using the \u2018change directory\u2019 command, \u2018cd\u2019. The command pwd return the present working directory. cd # no effect? that's because by itself it sends you home (to ~) cd / # go to root of tree's root system cd home # go to where everyone's homes are pwd cd username # use your actual home, not \"username\" pwd cd / pwd cd ~ # a shortcut to home, from anywhere pwd cd . # '.' always means *this* directory pwd cd .. # '..' always means *one directory up* pwd Absolute and Relative Paths \u00b6 You can think of paths like addresses. You can tell your friend how to go to a particular store from where they are currently (a \u2018relative\u2019 path), or from the main Interstate Highway that everyone uses (in this case, the root of the filesystem, \u2018/\u2019 \u2026 this is an \u2018absolute\u2019 path). Both are valid. But absolute paths can\u2019t be confused, because they always start off from the same place, and are unique. Relative paths, on the other hand, could be totally wrong for your friend if you assume they\u2019re somewhere they\u2019re not. With this in mind, let\u2019s try a few more: cd /usr/bin # let's start in /usr/bin #relative (start here, take one step up, then down through lib and gcc) cd ../lib/init/ pwd #absolute (start at root, take steps) cd /usr/lib/init/ pwd Now, because it can be a real pain to type out, or remember these long paths, we need to discuss \u2026 Tab Completion \u00b6 Using tab-completion is a must on the command line. A single <tab> auto-completes file or directory names when there\u2019s only one name that could be completed correctly. If multiple files could satisfy the tab-completion, then nothing will happen after the first <tab> . In this case, press <tab> a second time to list all the possible completing names. Note that if you\u2019ve already made a mistake that means that no files will ever be completed correctly from its current state, then <tab> \u2019s will do nothing. touch updates the timestamp on a file, here we use it to create three empty files. cd # go to your home directory mkdir ~/tmp cd ~/tmp touch one seven september ls o tab with no enter should complete to \u2018one\u2019, then enter ls s tab with no enter completes up to \u2018se\u2019 since that\u2019s in common between seven and september. tab again and no enter, this second tab should cause listing of seven and september. type \u2018v\u2019 then tab and no enter now it\u2019s unique to seven, and should complete to seven. enter runs \u2018ls seven\u2019 command. It cannot be overstated how useful tab completion is. You should get used to using it constantly. Watch experienced users type and they maniacally hit tab once or twice in between almost every character. You don\u2019t have to go that far, of course, but get used to constantly getting feedback from hitting tab and you will save yourself a huge amount of typing and trying to remember weird directory and filenames. TIME TO SHIFT GEARS AND PICK UP SOME SPEED NOW! \u00b6 History Repeats Itself \u00b6 Linux remembers everything you\u2019ve done (at least in the current shell session), which allows you to pull steps from your history, potentially modify them, and redo them. This can obviously save a lot of time and typing. The \u2018head\u2019 command views the first 10 (by default) lines of a file. The \u2018tail\u2019 commands views the last 10 (by default) lines of a file. Type \u2018man head\u2019 or \u2018man tail\u2019 to consult their manuals. <up arrow> # last command <up> # next-to-last command <down> # last command, again <down> # current command, empty or otherwise history # usually too much for one screen, so ... history | head # we discuss pipes (the vertical bar) below history | tail history | less # use 'q' to exit less ls -l pwd history | tail !560 # re-executes 560th command (yours will have different numbers; choose the one that recreates your really important result!) Editing Yourself \u00b6 Here are some more ways to make editing previous commands, or novel commands that you\u2019re building up, easier: <up><up> # go to some previous command, just to have something to work on <ctrl-a> # go to the beginning of the line <ctrl-e> # go to the end of the line #now use left and right to move to a single word (surrounded by whitespace: spaces or tabs) <ctrl-k> # delete from here to end of line <ctrl-w> # delete from here to beginning of preceeding word blah blah blah<ctrl-w><ctrl-w> # leaves you with only one 'blah' You can also search your history from the command line: <ctrl-r>fir # should find most recent command containing 'fir' string: echo 'first' > test.txt <enter> # to run command <ctrl-c> # get out of recursive search <ctr-r> # repeat <ctrl-r> to find successively older string matches Create and Destroy \u00b6 We already learned one command that will create a file, touch. Now let\u2019s look at create and removing files and directories. cd # home again mkdir ~/tmp2 cd ~/tmp2 echo 'Hello, world!' > first.txt echo text then redirect ( \u2018>\u2019 ) to a file. cat first.txt # 'cat' means 'concatenate', or just spit the contents of the file to the screen why \u2018concatenate\u2019? try this: \u00b6 cat first.txt first.txt first.txt > second.txt cat second.txt OK, let\u2019s destroy what we just created: cd ../ rmdir tmp2 # 'rmdir' meands 'remove directory', but this shouldn't work! rm tmp2/first.txt rm tmp2/second.txt # clear directory first rmdir tmp2 # should succeed now So, \u2018mkdir\u2019 and \u2018rmdir\u2019 are used to create and destroy (empty) directories. \u2018rm\u2019 to remove files. To create a file can be as simple as using \u2018echo\u2019 and the \u2018>\u2019 (redirection) character to put text into a file. Even simpler is the \u2018touch\u2019 command. mkdir ~/cli cd ~/cli touch newFile ls -ltra # look at the time listed for the file you just created cat newFile # it's empty! sleep 60 # go grab some coffee touch newFile ls -ltra # same time? So \u2018touch\u2019 creates empty files, or updates the \u2018last modified\u2019 time. Note that the options on the \u2018ls\u2019 command you used here give you a Long listing, of All files, in Reverse Time order (l, a, r, t). Forced Removal (CAUTION!!!) \u00b6 When you\u2019re on the command line, there\u2019s no \u2018Recycle Bin\u2019. Since we\u2019ve expanded a whole directory tree, we need to be able to quickly remove a directory without clearing each subdirectory and using \u2018rmdir\u2019. cd mkdir -p rmtest/dir1/dir2 # the -p option creates all the directories at once rmdir rmtest # gives an error since rmdir can only remove directories that are empty rm -rf rmtest # will remove the directory and EVERYTHING in it Here -r = recursively remove sub-directories, -f means force. Obviously, be careful with \u2018rm -rf\u2019 , there is no going back , if you delete something with rm, rmdir its gone! There is no Recycle Bin on the Command-Line! Piping and Redirection \u00b6 Pipes (\u2018|\u2019) allow commands to hand output to other commands, and redirection characters (\u2018>\u2019 and \u2018\u00bb\u2019) allow you to put output into files. echo 'first' > test.txt cat test.txt # outputs the contents of the file to the terminal echo 'second' > test.txt cat test.txt echo 'third' >> test.txt cat test.txt The \u2018>\u2019 character redirects output of a command that would normally go to the screen instead into a specified file. \u2018>\u2019 overwrites the file, \u2018\u00bb\u2019 appends to the file. The \u2018cut\u2019 command pieces of lines from a file line by line. This command cuts characters 1 to 3, from every line, from file \u2018test.txt\u2019 cut -c 1 -3 test.txt same thing, piping output of one command into input of another cat test.txt | cut -c 1 -3 This pipes (i.e., sends the output of) cat to cut to sort (-r means reverse order sort), and then grep searches for pattern (\u2018s\u2019) matches (i.e. for any line where an \u2018s\u2019 appears anywhere on the line.) cat test.txt | cut -c 1 -3 | sort -r cat test.txt | cut -c 1 -3 | sort -r | grep s This is a great way to build up a set of operations while inspecting the output of each step in turn. We\u2019ll do more of this in a bit. Compression and Archives \u00b6 As file sizes get large, you\u2019ll often see compressed files, or whole compressed folders. Note that any good bioinformatics software should be able to work with compressed file formats. gzip test.txt cat test.txt.gz To uncompress a file gunzip -c test.txt.gz The \u2018-c\u2019 leaves the original file alone, but dumps expanded output to screen gunzip test.txt.gz # now the file should change back to uncompressed test.txt Tape archives, or .tar files, are one way to compress entire folders and all contained folders into one file. When they\u2019re further compressed they\u2019re called \u2018tarballs\u2019. We can use wget (web get). wget -L -O PhiX_Illumina_RTA.tar.gz http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/PhiX/Illumina/RTA/PhiX_Illumina_RTA.tar.gz The .tar.gz and .tgz are commonly used extensions for compressed tar files, when gzip compression is used. The application tar is used to uncompress .tar files tar -xzvf PhiX_Illumina_RTA.tar.gz Here -x = extract, -z = use gzip/gunzip, -v = verbose (show each file in archive), -f filename Note that, unlike Windows, linux does not depend on file extensions to determine file behavior. So you could name a tarball \u2018fish.puppy\u2019 and the extract command above should work just fine. The only thing that should be different is that tab-completion doesn\u2019t work within the \u2018tar\u2019 command if it doesn\u2019t see the \u2018correct\u2019 file extension. BASH Wildcard Characters \u00b6 We can use \u2018wildcard characters\u2019 when we want to specify or operate on sets of files all at once. ls ?hiX/Illumina list files in Illumina sub-directory of any directory ending in \u2018hiX\u2019 ls PhiX/Illumina/RTA/Sequence/*/*.fa list all files ending in \u2018.fa\u2019 a few directories down. So, \u2018?\u2019 fills in for zero or one character, \u2018*\u2019 fills in for zero or more characters. The \u2018find\u2019 command can be used to locate files using a similar form. find . -name \"*.f*\" find . -name \"*.f?\" how is this different from the previous ls commands? Quick Note About the Quote(s) \u00b6 The quote characters \u201c and \u2018 are different. In general, single quotes preserve the literal meaning of all characters between them. On the other hand, double quotes allow the shell to see what\u2019s between them and make substitutions when appropriate. For example: VRBL = someText echo '$VRBL' echo \" $VRBL \" However, some commands try to be \u2018smarter\u2019 about this behavior, so it\u2019s a little hard to predict what will happen in all cases. It\u2019s safest to experiment first when planning a command that depends on quoting \u2026 list filenames first, instead of changing them, etc. Finally, the \u2018backtick\u2019 characters ` (same key - unSHIFTED - as the tilde ~) causes the shell to interpret what\u2019s between them as a command, and return the result. counts the number of lines in file and stores result in the LINES variable \u00b6 LINES = ` cat PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome.1.bt2 | wc -l ` echo $LINES Symbolic Links \u00b6 Since copying or even moving large files (like sequence data) around your filesystem may be impractical, we can use links to reference \u2018distant\u2019 files without duplicating the data in the files. Symbolic links are disposable pointers that refer to other files, but behave like the referenced files in commands. I.e., they are essentially \u2018Shortcuts\u2019 (to use a Windows term) to a file or directory. The \u2018ln\u2019 command creates a link. You should, by default, always create a symbolic link using the -s option. ln -s PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa . ls -ltrhaF # notice the symbolic link pointing at its target grep -c \">\" genome.fa STDOUT & STDERR \u00b6 Programs can write to two separate output streams, \u2018standard out\u2019 (STDOUT), and \u2018standard error\u2019 (STDERR). The former is generally for direct output of a program, while the latter is supposed to be used for reporting problems. I\u2019ve seen some bioinformatics tools use STDERR to report summary statistics about the output, but this is probably bad practice. Default behavior in a lot of cases is to dump both STDOUT and STDERR to the screen, unless you specify otherwise. In order to nail down what goes where, and record it for posterity: wc -c genome.fa 1 > chars.txt 2 > any.err the 1 st output, STDOUT, goes to \u2018chars.txt\u2019 the 2 nd output, STDERR, goes to \u2018any.err\u2019 cat chars.txt Contains the character count of the file genome.fa cat any.err Empty since no errors occured. Saving STDOUT is pretty routine (you want your results, yes?), but remember that explicitly saving STDERR is important on a remote server, since you may not directly see the \u2018screen\u2019 when you\u2019re running jobs. The sed command \u00b6 Let\u2019s take a look at the \u2018sed\u2019 command. NOTE: On Macs use \u2018gsed\u2019. sed (short for stream editor) is a command that allows you to manipulate character data in various ways. One useful thing it can do is substitution. Let\u2019s download a simple file to work on: wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/region.bed -O region.bed Take a look at the file: cat region.bed Now, let\u2019s make all the uppercase \u201cCHR\u201ds into lowercase: cat region.bed | sed 's/CHR/chr/' What happened? Only the first CHR changed. That is because we need to add the \u201cg\u201d option: cat region.bed | sed 's/CHR/chr/g' We can also do the the substitution without regards to case: cat region.bed | sed 's/chr/chr/gi' Let\u2019s break down the argument to sed (within the single quotes)\u2026 The \u201cs\u201d means \u201csubstitute\u201d, the word between the 1 st and 2 nd forward slashes (i.e. /) is the word the substitute for, the word between the 2 nd and 3 rd slashes is the word to substitute with, and finally the \u201cgi\u201d at the end are flags for global substitution (i.e. substituting along an entire line instead of just the first occurence on a line), and for case insenstivity (i.e. it will ignore the case of the letters when doing the substitution). Note that this doesn\u2019t change the file itself, it is simply piping the output of the cat command to sed and outputting to the screen. If you wanted to change the file itself, you could use the \u201c-i\u201d option to sed: cat region.bed sed -i 's/chr/chr/gi' region.bed Now if you look at the file, the lines have changed. cat region.bed Another useful use of sed is for capturing certain lines from a file. You can select certain lines from a file: sed '4q;d' region.bed This will just select the 4 th line from the file. You can also extract a range of lines from a file: sed -n '10,20p' region.bed This gets the 10 th through 20 th lines from the file. CHALLENGE: See if you can find a way to use sed to remove all the \u201cCHR\u201ds from the file. More pipes \u00b6 Now, let\u2019s delve into pipes a little more. Pipes are a very powerful way to look at and manipulate complex data using a series of simple programs. Let\u2019s look at some fastq files. Get a few small fastq files: wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/C61.subset.fq.gz -O C61.subset.fq.gz wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I561.subset.fq.gz -O I561.subset.fq.gz wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I894.subset.fq.gz -O I894.subset.fq.gz Since the files are gzipped files we need to use \u201czcat\u201d to look at them. zcat is just like cat except for gzipped files: zcat C61.subset.fq.gz | head Fastq records are 4 lines per sequence, a header line, the sequence, a plus sign (which is historical), and then the quality encoding for the sequence. Notice that each header line has the barcode for that read at the end of the line. Let\u2019s count the number of each barcode. In order to do that we need to just capture the header lines from this file. We can use \u201csed\u201d to do that: zcat C61.subset.fq.gz | sed -n '1~4p' | head By default sed prints every line. In this case we are giving the \u201c-n\u201d option to sed which will not print every line. Instead, we are giving it the argument \u201c1~4p\u201d, which means to print the first line, then skip 4 lines and print again, and then continue to do that. Now that we have a way to get just the headers, we need to isolate the part of the header that is the barcode. There are multiple ways to do this\u2026 we will use the cut command: zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | head So we are using the \u201c-d\u201d option to cut with \u201c:\u201d as the argument to that option, meaning that we will be using the delimiter \u201c:\u201d to split the input. Then we use the \u201c-f\u201d option with argument \u201c10\u201d, meaning that we want the 10 th field after the split. In this case, that is the barcode. Finally, as before, we need to sort the data and then use \u201cuniq -c\u201d to count. Then put it all together and run it on the entire dataset (This will take about a minute to run): zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | sort | uniq -c Now you have a list of how many reads were categorized into each barcode. Here is a sed tutorial for more exercises. CHALLENGE: Find the distribution of the first 5 bases of all the reads in C61_S67_L006_R1_001.fq.gz. I.e., count the number of times the first 5 bases of every read occurs across all reads. Loops \u00b6 Loops are useful for quickly telling the shell to perform one operation after another, in series. For example: for i in { 1 ..21 } ; do echo $i >> a ; done # put multiple lines of code on one line, each line terminated by ';' cat a <1 through 21 on separate lines> \u00b6 The general form is: for name in { list } ; do commands done The list can be a sequence of numbers or letters, or a group of files specified with wildcard characters: for i in { 3 ,2,1,liftoff } ; do echo $i ; done # needs more excitement! for i in { 3 ,2,1, \"liftoff!\" } ; do echo $i ; done # exclamation point will confuse the shell unless quoted A \u201cwhile\u201d loop is more convenient than a \u201cfor\u201d loop \u2026 if you don\u2019t readily know how many iterations of the loop you want: while { condition } ; do commands done Now, let\u2019s do some bioinformatics-y things with loops and pipes. First, let\u2019s write a command to get the nucleotide count of the first 10,000 reads in a file. Use zcat and sed to get only the read lines of a file, and then only take the first 10,000: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | less Use grep\u2019s \u201c-o\u201d option to get each nucleotide on a separate line (take a look at the man page for grep to understand how this works): zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | less Finally, use sort and uniq to get the counts: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c 264012 A 243434 C 215045 G 278 N 277231 T And, voila, we have the per nucleotide count for these reads! We just did this for one file, but what if we wanted to do it for all of our files? We certainly don\u2019t want to type the command by hand dozens of times. So we\u2019ll use a while loop. You can pipe a command into a while loop and it will iterate through each line of the input. First, get a listing of all your files: ls -1 *.fq.gz Pipe that into a while loop and read in the lines into a variable called \u201cx\u201d. We use \u201c$x\u201d to get the value of the variable in that iteration of the loop: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; done Add the command we created above into the loop, placing $x where the filename would be and semi-colons inbetween commands: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c ; done When this runs it will print the name of every single file being processed and the nucleotide count for the reads from those files. Now, let\u2019s say you wanted to write the output of each command to a separate file. We would redirect the output to a filename, but we need to create a different file name for each command and we want the file name to reflect its contents, i.e. the output file name should be based on the input file name. So we use \u201cparameter expansion\u201d, which is fancy way of saying substitution: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; done This will put the output of the counting command into a file whose name is the prefix of the input file plus \u201c.nucl_count.txt\u201d. It will do this for every input file. Manipulation of a FASTA File \u00b6 Let\u2019s copy the phiX-174 genome (using the \u2018cp\u2019 command) to our current directory so we can play with it: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa phix.fa Similarly we can also use the move command here, but then ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa will no longer be there: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ mv ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa phix.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ This functionality of mv is why it is used to rename files. Note how we copied the \u2018genome.fa\u2019 file to a different name: \u2018phix.fa\u2019 wc -l phix.fa count the number of lines in the file using \u2018wc\u2019 (word count) and parameter \u2018-l\u2019 (lines). We can use the \u2018grep\u2019 command to search for matches to patterns. \u2018grep\u2019 comes from \u2018globally search for a regular expression and print\u2019. grep -c '>' phix.fa #Only one FASTA sequence entry, since only one header line (\u2018>gi|somethingsomething\u2026\u2019) cat phix.fa This may not be useful for anything larger than a virus! Let\u2019s look at the start codon and the two following codons: grep --color \"ATG......\" phix.fa #\u2019.\u2019 characters are the single-character wildcards for grep. So \u201cATG\u2026\u2026\u201d matches any set of 9 characters that starts with ATG. #Use the \u2013color \u2018-o\u2019 option to only print the pattern matches, one per line grep -o \"ATG......\" phix.fa #Use the \u2018cut\u2019 command with \u2018-c\u2019 to select characters 4-6, the second codon grep --color -o \"ATG......\" phix.fa | cut -c4-6 \u2018sort\u2019 the second codon sequences ( default order is same as ASCII table ; see \u2018man ascii\u2019 ) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort #Combine successive identical sequences, but count them using the \u2018uniq\u2019 command with the \u2018-c\u2019 option grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c #Finally sort using reverse numeric order (\u2018-rn\u2019) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c | sort -rn \u2026 which gives us the most common codons first This may not be a particularly useful thing to do with a genomic FASTA file, but it illustrates the process by which one can build up a string of operations, using pipes, in order to ask quantitative questions about sequence content. More generally than that, this process allows one to ask questions about files and file contents and the operating system, and verify at each step that the process so far is working as expected. The command line is, in this sense, really a modular workflow management system. Shell Scripts, File Permissions \u00b6 Often it\u2019s useful to define a whole string of commands to run on some input, so that (1) you can be sure you\u2019re running the same commands on all data, and (2) so you don\u2019t have to type the same commands in over and over! Let\u2019s use the \u2018nano\u2019 text editor program that\u2019s pretty reliably installed on most linux systems. nano test.sh insert cli_figure7 \u00b6 nano now occupies the whole screen; see commands at the bottom. Let\u2019s type in a few commands. First we need to put the following line at the top of the file: #!/bin/bash The \u201c#!\u201d at the beginning of a script tells the shell what language to use to interpret the rest of the script. In our case, we will be writing \u201cbash\u201d commands, so we specify the full path of the bash executable after the \u201c#!\u201d. Then, add some commands: #!/bin/bash echo \"Start script...\" pwd ls -l sleep 10 echo \"End script.\" Hit Cntl-O and then enter to save the file, and then Cntl-X to exit nano. Though there are ways to run the commands in test.sh right now, it\u2019s generally useful to give yourself (and others) \u2018execute\u2019 permissions for test.sh, really making it a shell script. Note the characters in the first (left-most) field of the file listing: ls -lh test.sh -rw-rw-r-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first \u2018-\u2018 becomes a \u2018d\u2019 if the \u2018file\u2019 is actually a directory. The next three characters represent read, write, and execute permissions for the file owner (you), followed by three characters for users in the owner\u2019s group, followed by three characters for all other users. Run the \u2018chmod\u2019 command to change permissions for the \u2018test.sh\u2019 file, adding execute permissions (\u2018+x\u2019) for the user (you) and your group (\u2018ug\u2019): chmod ug+x test.sh ls -lh test.sh -rwxr-xr-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first 10 characters of the output represent the file and permissions. The first character is the file type, the next three sets of three represent the file permissions for the user, group, and everyone respectively. r = read w = write x = execute So let\u2019s run this script. We have to provide a relative reference to the script \u2018./\u2019 because its not our our \u201cPATH\u201d.: #you can do either ./test.sh #or you can run it like this bash test.sh And you should see all the commands in the file run in sequential order in the terminal. Command Line Arguments for Shell Scripts \u00b6 Now let\u2019s modify our script to use command line arguments, which are arguments that can come after the script name (when executing) to be part of the input inside the script. This allows us to use the same script with different inputs. In order to do so, we add variables $1, $2, $3, etc\u2026. in the script where we want our input to be. So, for example, use nano to modify your test.sh script to look like this: #!/bin/bash echo \"Start script...\" PWD = ` pwd ` echo \"The present working directory is $PWD \" ls -l $1 sleep $2 wc -l $3 echo \"End script.\" Now, rerun the script using command line arguments like this: ./test.sh genome.fa 15 PhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt Note that each argument is separated by a space, so $1 becomes \u201cgenome.fa\u201d, $2 becomes \u201c15\u201d, and $3 becomes \u201cPhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\u201d. Then the commands are run using those values. Now rerun the script with some other values: ./test.sh .. 5 genome.fa Now, $1 becomes \u201c..\u201d, $2 is \u201c5\u201d, and $3 is \u201cgenome.fa\u201d. Pipes and Loops inside scripts \u00b6 Open a new text file using the text editor \u201cnano\u201d: nano get_nucl_counts.sh Copy and Paste the following into the file: #!/bin/bash zcat $1 | sed -n '2~4p' | head - $2 | grep -o . | sort | uniq -c Save the file and exit. Change the permissions on the file to make it executable: chmod a+x get_nucl_counts.sh Now, we can run this script giving it different arguments every time. The first argument (i.e. the first text after the script name when it is run) will get put into the variable \u201c \\(1\u201d. The second argument (delimited by spaces) will get put into \u201c\\) 2\u201d. In this case, \u201c \\(1\u201d is the file name, and \u201c\\) 2\u201d is the number of reads we want to count. So, then we can run the script over and over again using different values and the command will run based on those values: ./get_nucl_counts.sh I561.subset.fq.gz 1000 ./get_nucl_counts.sh I561.subset.fq.gz 10000 ./get_nucl_counts.sh C61.subset.fq.gz 555 We can also put loops into a script. We\u2019ll take the loop we created earlier and put it into a file, breaking it up for readability and using backslashes for line continuation: nano get_nucl_counts_loop.sh Put this in the file and save it: #!/bin/bash ls -1 *.fq.gz | \\ while read x ; do \\ echo $x is being processed... ; \\ zcat $x | sed -n '2~4p' | head - $1 | \\ grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; \\ done Make it executable: chmod a+x get_nucl_counts_loop.sh And now we can execute the entire loop using the script. Note that there is only one argument now, the number of reads to use: ./get_nucl_counts_loop.sh 100 FINALLY - a good summary for rounding up the session \u00b6 TEN SIMPLE RULES FOR GETTING STARTED WITH COMMAND-LINE BIOINFORMATICS","title":"Basic Unix"},{"location":"lab_session/basic_unix/#unix-for-bioinformatics","text":"","title":"Unix for Bioinformatics"},{"location":"lab_session/basic_unix/#outline","text":"What is the command line? Directory Structure Syntax of a Command Options of a Command Command Line Basics (ls, pwd, Ctrl-C, man, alias, ls -lthra) Getting Around (cd) Absolute and Relative Paths Tab Completion History Repeats Itself (history, head, tail, ) Editing Yourself (Ctrl-A, Ctrl-E, Ctrl-K, Ctrl-W) Create and Destroy (echo, cat, rm, rmdir) Transferring Files (scp) Piping and Redirection (|, >, \u00bb, cut, sort, grep) Compressions and Archives (tar, gzip, gunzip) Forced Removal (rm -r) BASH Wildcard Characters (?, *, find, environment variables($), quotes/ticks) Manipulation of a FASTA file (cp, mv, wc -l/-c) Symbolic Links (ln -s) STDOUT and STDERR (>1, >2) Paste Command (paste, for loops) Shell Scripts and File Permissions (chmod, nano, ./)","title":"Outline:"},{"location":"lab_session/basic_unix/#a-simple-unix-cheat-sheet","text":"Download Unix Cheat Sheet","title":"A Simple Unix cheat sheet"},{"location":"lab_session/basic_unix/#what-is-unix","text":"UNIX is an operating system which was first developed in the 1960s, and has been under constant development ever since. By operating system, we mean the suite of programs which make the computer work. It is a stable, multi-user, multi-tasking system for servers, desktops and laptops. UNIX systems also have a graphical user interface (GUI) similar to Microsoft Windows which provides an easy to use environment. However, knowledge of UNIX is required for operations which aren't covered by a graphical program, or for when there is no windows interface available, for example, in a telnet session.","title":"What is UNIX?"},{"location":"lab_session/basic_unix/#types-of-unix","text":"There are many different versions of UNIX, although they share common similarities. The most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X. Here in the School, we use Solaris on our servers and workstations, and Fedora Linux on the servers and desktop PCs.","title":"Types of UNIX"},{"location":"lab_session/basic_unix/#the-unix-operating-system","text":"The UNIX operating system is made up of three parts; the kernel, the shell and the programs.","title":"The UNIX operating system"},{"location":"lab_session/basic_unix/#the-kernel","text":"The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore and communications in response to system calls. As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile (which has the effect of removing the file myfile). The shell searches the filestore for the file containing the program rm, and then requests the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished running, the shell then returns the UNIX prompt % to the user, indicating that it is waiting for further commands.","title":"The kernel"},{"location":"lab_session/basic_unix/#the-shell","text":"The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt (% on our systems). The adept user can customise his/her own shell, and users can use different shells on the same machine. This shell is an all-text display (most of the time your mouse doesn\u2019t work) and is accessed using an application called the \"terminal\" which usually looks like a black window with white letters or a white window with black letters by default. The tcsh shell has certain features to help the user inputting commands. Filename Completion - By typing part of the name of a command, filename or directory and pressing the [Tab] key, the tcsh shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will beep, prompting you to type a few more letters before pressing the tab key again. History - The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or type history for a list of previous commands.","title":"The shell"},{"location":"lab_session/basic_unix/#files-and-processes","text":"Everything in UNIX is either a file or a process. A process is an executing program identified by a unique PID (process identifier). A file is a collection of data. They are created by users using text editors, running compilers etc. Examples of files: a document (report, essay etc.) the text of a program written in some high-level programming language instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file); a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files.","title":"Files and processes"},{"location":"lab_session/basic_unix/#the-directory-structure","text":"All the files are grouped together in the directory structure. The file-system is arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root (written as a slash / ) Absolute path: always starts with \u201d/\u201d - the root folder /home/ubuntu/workspace the folder (or file) \u201cworkspace\u201d in the folder \u201cubuntu\u201d or \" student#1 \" in the folder \u201chome\u201d in the folder from the root. Relative path: always relative to our current location. a single dot (.) refers to the current directory two dots (..) refers to the directory one level up","title":"The Directory Structure"},{"location":"lab_session/basic_unix/#_1","text":"Usually, /home is where the user accounts reside, ie. users\u2019 \u2018home\u2019 directories. For example, for a user that has a username of \u201c student#1 \u201d: their home directory is /home/student1. It is the directory that a user starts in after starting a new shell or logging into a remote server. The tilde (~) is a short form of a user\u2019s home directory.","title":""},{"location":"lab_session/basic_unix/#starting-an-unix-terminal","text":"","title":"Starting an UNIX terminal"},{"location":"lab_session/basic_unix/#_2","text":"After opening or logging into a terminal, system messages are often displayed, followed by the \u201cprompt\u201d. A prompt is a short text message at the start of the command line and ends with a $ in bash shell, commands are typed after the prompt. The prompt typically follows the form username@server:current_directory $ . If your screen looks like the one below, i.e. your see your a bunch of messages and then your ubuntu student instance number followed by \u201c @172 .31.84.105:~$\u201d at the beginning of the line, then you are successfully logged in.","title":""},{"location":"lab_session/basic_unix/#unix-basics","text":"First some basics - how to look at your surroundings","title":"Unix Basics"},{"location":"lab_session/basic_unix/#present-working-directory-where-am-i","text":"pwd see the words separated by /. this is called the path. In unix, the location of each folder or file is shown like this.. this is more like the address or the way to find a folder or file. for example, my Desktop, Documents and Downloads folder usually are located in my home directory so in that case, it path to the folders will be /home/ubuntu/Desktop /home/ubuntu/Documents /home/ubuntu/Downloads","title":"present working directory \u2026 where am I?"},{"location":"lab_session/basic_unix/#syntax-of-a-command","text":"A command plus the required parameters/arguments The separator used in issuing a command is space, number of spaces does not matter.","title":"Syntax of a command"},{"location":"lab_session/basic_unix/#now-lets-try-some-basic-and-simple-commands","text":"list files here\u2026 you should see just the folders that we have created for you here and nothing else. ls list files somewhere else, like /tmp/ ls /tmp","title":"Now let's try some basic and simple commands"},{"location":"lab_session/basic_unix/#tip","text":"In unix one of the first things that\u2019s good to know is how to escape once you\u2019ve started something you don\u2019t want. Use Ctrl-c (shows as \u2018^C\u2019 in the terminal) to exit (kill) a command. In some cases, a different key sequence is required (Ctrl-d). Note that anything including and after a \u201c#\u201d symbol is ignored, i.e. a comment. So in all the commands below, you do not have to type anything including and past a \u201c#\u201d.","title":"TIP!"},{"location":"lab_session/basic_unix/#options","text":"Each command can act as a basic tool, or you can add \u2018options\u2019 or \u2018flags\u2019 that modify the default behavior of the tool. These flags come in the form of \u2018-v\u2019 \u2026 or, when it\u2019s a more descriptive word, two dashes: \u2018--verbose\u2019 \u2026 that\u2019s a common (but not universal) one that tells a tool that you want it to give you output with more detail. Sometimes, options require specifying amounts or strings, like \u2018-o results.txt\u2019 or \u2018--output results.txt\u2019 \u2026 or \u2018-n 4\u2019 or \u2018--numCPUs 4\u2019. Let\u2019s try some, and see what the man page for the \u2018list files\u2019 command \u2018ls\u2019 is like. ls -R Lists directories and files recursively. This will be a very long output, so use Ctrl-C to break out of it. Sometimes you have to press Ctrl-C many times to get the terminal to recognize it. In order to know which options do what, you can use the manual pages. To look up a command in the manual pages type \u201cman\u201d and then the command name. So to look up the options for \u201cls\u201d, type: man ls Navigate this page using the up and down arrow keys, PageUp and PageDown, and then use q to quit out of the manual. In this manual page, find the following options, quit the page, and then try those commands. You could even open another terminal, log in again, and run manual commands in that terminal. ls -l /usr/bin/ #long format, gives permission values, owner, group, size, modification time, and name","title":"Options"},{"location":"lab_session/basic_unix/#exercise","text":"Feel free to see manual pages for these basic commands Commands and their Meanings man ls ls -a man mkdir Now see the difference between these three commands cd cd ~ cd .. change to parent directory Also these commands ls -l ls -a ls -l -a ls -la ls -ltrha ls -ltrha --color Quick aside: what if I want to use same options repeatedly? and be lazy? You can create a shortcut to another command using \u2018alias\u2019. alias ll = 'ls -lah' ll","title":"Exercise:"},{"location":"lab_session/basic_unix/#getting-around","text":"The filesystem you\u2019re working on is like the branching root system of a tree. The top level, right at the root of the tree, is called the \u2018root\u2019 directory, specified by \u2018/\u2019 \u2026 which is the divider for directory addresses, or \u2018paths\u2019. Now let's see a little about the commands you checked in the exercise above. We move around using the \u2018change directory\u2019 command, \u2018cd\u2019. The command pwd return the present working directory. cd # no effect? that's because by itself it sends you home (to ~) cd / # go to root of tree's root system cd home # go to where everyone's homes are pwd cd username # use your actual home, not \"username\" pwd cd / pwd cd ~ # a shortcut to home, from anywhere pwd cd . # '.' always means *this* directory pwd cd .. # '..' always means *one directory up* pwd","title":"Getting Around"},{"location":"lab_session/basic_unix/#absolute-and-relative-paths","text":"You can think of paths like addresses. You can tell your friend how to go to a particular store from where they are currently (a \u2018relative\u2019 path), or from the main Interstate Highway that everyone uses (in this case, the root of the filesystem, \u2018/\u2019 \u2026 this is an \u2018absolute\u2019 path). Both are valid. But absolute paths can\u2019t be confused, because they always start off from the same place, and are unique. Relative paths, on the other hand, could be totally wrong for your friend if you assume they\u2019re somewhere they\u2019re not. With this in mind, let\u2019s try a few more: cd /usr/bin # let's start in /usr/bin #relative (start here, take one step up, then down through lib and gcc) cd ../lib/init/ pwd #absolute (start at root, take steps) cd /usr/lib/init/ pwd Now, because it can be a real pain to type out, or remember these long paths, we need to discuss \u2026","title":"Absolute and Relative Paths"},{"location":"lab_session/basic_unix/#tab-completion","text":"Using tab-completion is a must on the command line. A single <tab> auto-completes file or directory names when there\u2019s only one name that could be completed correctly. If multiple files could satisfy the tab-completion, then nothing will happen after the first <tab> . In this case, press <tab> a second time to list all the possible completing names. Note that if you\u2019ve already made a mistake that means that no files will ever be completed correctly from its current state, then <tab> \u2019s will do nothing. touch updates the timestamp on a file, here we use it to create three empty files. cd # go to your home directory mkdir ~/tmp cd ~/tmp touch one seven september ls o tab with no enter should complete to \u2018one\u2019, then enter ls s tab with no enter completes up to \u2018se\u2019 since that\u2019s in common between seven and september. tab again and no enter, this second tab should cause listing of seven and september. type \u2018v\u2019 then tab and no enter now it\u2019s unique to seven, and should complete to seven. enter runs \u2018ls seven\u2019 command. It cannot be overstated how useful tab completion is. You should get used to using it constantly. Watch experienced users type and they maniacally hit tab once or twice in between almost every character. You don\u2019t have to go that far, of course, but get used to constantly getting feedback from hitting tab and you will save yourself a huge amount of typing and trying to remember weird directory and filenames.","title":"Tab Completion"},{"location":"lab_session/basic_unix/#time-to-shift-gears-and-pick-up-some-speed-now","text":"","title":"TIME TO SHIFT GEARS AND PICK UP SOME SPEED NOW!"},{"location":"lab_session/basic_unix/#history-repeats-itself","text":"Linux remembers everything you\u2019ve done (at least in the current shell session), which allows you to pull steps from your history, potentially modify them, and redo them. This can obviously save a lot of time and typing. The \u2018head\u2019 command views the first 10 (by default) lines of a file. The \u2018tail\u2019 commands views the last 10 (by default) lines of a file. Type \u2018man head\u2019 or \u2018man tail\u2019 to consult their manuals. <up arrow> # last command <up> # next-to-last command <down> # last command, again <down> # current command, empty or otherwise history # usually too much for one screen, so ... history | head # we discuss pipes (the vertical bar) below history | tail history | less # use 'q' to exit less ls -l pwd history | tail !560 # re-executes 560th command (yours will have different numbers; choose the one that recreates your really important result!)","title":"History Repeats Itself"},{"location":"lab_session/basic_unix/#editing-yourself","text":"Here are some more ways to make editing previous commands, or novel commands that you\u2019re building up, easier: <up><up> # go to some previous command, just to have something to work on <ctrl-a> # go to the beginning of the line <ctrl-e> # go to the end of the line #now use left and right to move to a single word (surrounded by whitespace: spaces or tabs) <ctrl-k> # delete from here to end of line <ctrl-w> # delete from here to beginning of preceeding word blah blah blah<ctrl-w><ctrl-w> # leaves you with only one 'blah' You can also search your history from the command line: <ctrl-r>fir # should find most recent command containing 'fir' string: echo 'first' > test.txt <enter> # to run command <ctrl-c> # get out of recursive search <ctr-r> # repeat <ctrl-r> to find successively older string matches","title":"Editing Yourself"},{"location":"lab_session/basic_unix/#create-and-destroy","text":"We already learned one command that will create a file, touch. Now let\u2019s look at create and removing files and directories. cd # home again mkdir ~/tmp2 cd ~/tmp2 echo 'Hello, world!' > first.txt echo text then redirect ( \u2018>\u2019 ) to a file. cat first.txt # 'cat' means 'concatenate', or just spit the contents of the file to the screen","title":"Create and Destroy"},{"location":"lab_session/basic_unix/#why-concatenate-try-this","text":"cat first.txt first.txt first.txt > second.txt cat second.txt OK, let\u2019s destroy what we just created: cd ../ rmdir tmp2 # 'rmdir' meands 'remove directory', but this shouldn't work! rm tmp2/first.txt rm tmp2/second.txt # clear directory first rmdir tmp2 # should succeed now So, \u2018mkdir\u2019 and \u2018rmdir\u2019 are used to create and destroy (empty) directories. \u2018rm\u2019 to remove files. To create a file can be as simple as using \u2018echo\u2019 and the \u2018>\u2019 (redirection) character to put text into a file. Even simpler is the \u2018touch\u2019 command. mkdir ~/cli cd ~/cli touch newFile ls -ltra # look at the time listed for the file you just created cat newFile # it's empty! sleep 60 # go grab some coffee touch newFile ls -ltra # same time? So \u2018touch\u2019 creates empty files, or updates the \u2018last modified\u2019 time. Note that the options on the \u2018ls\u2019 command you used here give you a Long listing, of All files, in Reverse Time order (l, a, r, t).","title":"why \u2018concatenate\u2019? try this:"},{"location":"lab_session/basic_unix/#forced-removal-caution","text":"When you\u2019re on the command line, there\u2019s no \u2018Recycle Bin\u2019. Since we\u2019ve expanded a whole directory tree, we need to be able to quickly remove a directory without clearing each subdirectory and using \u2018rmdir\u2019. cd mkdir -p rmtest/dir1/dir2 # the -p option creates all the directories at once rmdir rmtest # gives an error since rmdir can only remove directories that are empty rm -rf rmtest # will remove the directory and EVERYTHING in it Here -r = recursively remove sub-directories, -f means force. Obviously, be careful with \u2018rm -rf\u2019 , there is no going back , if you delete something with rm, rmdir its gone! There is no Recycle Bin on the Command-Line!","title":"Forced Removal (CAUTION!!!)"},{"location":"lab_session/basic_unix/#piping-and-redirection","text":"Pipes (\u2018|\u2019) allow commands to hand output to other commands, and redirection characters (\u2018>\u2019 and \u2018\u00bb\u2019) allow you to put output into files. echo 'first' > test.txt cat test.txt # outputs the contents of the file to the terminal echo 'second' > test.txt cat test.txt echo 'third' >> test.txt cat test.txt The \u2018>\u2019 character redirects output of a command that would normally go to the screen instead into a specified file. \u2018>\u2019 overwrites the file, \u2018\u00bb\u2019 appends to the file. The \u2018cut\u2019 command pieces of lines from a file line by line. This command cuts characters 1 to 3, from every line, from file \u2018test.txt\u2019 cut -c 1 -3 test.txt same thing, piping output of one command into input of another cat test.txt | cut -c 1 -3 This pipes (i.e., sends the output of) cat to cut to sort (-r means reverse order sort), and then grep searches for pattern (\u2018s\u2019) matches (i.e. for any line where an \u2018s\u2019 appears anywhere on the line.) cat test.txt | cut -c 1 -3 | sort -r cat test.txt | cut -c 1 -3 | sort -r | grep s This is a great way to build up a set of operations while inspecting the output of each step in turn. We\u2019ll do more of this in a bit.","title":"Piping and Redirection"},{"location":"lab_session/basic_unix/#compression-and-archives","text":"As file sizes get large, you\u2019ll often see compressed files, or whole compressed folders. Note that any good bioinformatics software should be able to work with compressed file formats. gzip test.txt cat test.txt.gz To uncompress a file gunzip -c test.txt.gz The \u2018-c\u2019 leaves the original file alone, but dumps expanded output to screen gunzip test.txt.gz # now the file should change back to uncompressed test.txt Tape archives, or .tar files, are one way to compress entire folders and all contained folders into one file. When they\u2019re further compressed they\u2019re called \u2018tarballs\u2019. We can use wget (web get). wget -L -O PhiX_Illumina_RTA.tar.gz http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/PhiX/Illumina/RTA/PhiX_Illumina_RTA.tar.gz The .tar.gz and .tgz are commonly used extensions for compressed tar files, when gzip compression is used. The application tar is used to uncompress .tar files tar -xzvf PhiX_Illumina_RTA.tar.gz Here -x = extract, -z = use gzip/gunzip, -v = verbose (show each file in archive), -f filename Note that, unlike Windows, linux does not depend on file extensions to determine file behavior. So you could name a tarball \u2018fish.puppy\u2019 and the extract command above should work just fine. The only thing that should be different is that tab-completion doesn\u2019t work within the \u2018tar\u2019 command if it doesn\u2019t see the \u2018correct\u2019 file extension.","title":"Compression and Archives"},{"location":"lab_session/basic_unix/#bash-wildcard-characters","text":"We can use \u2018wildcard characters\u2019 when we want to specify or operate on sets of files all at once. ls ?hiX/Illumina list files in Illumina sub-directory of any directory ending in \u2018hiX\u2019 ls PhiX/Illumina/RTA/Sequence/*/*.fa list all files ending in \u2018.fa\u2019 a few directories down. So, \u2018?\u2019 fills in for zero or one character, \u2018*\u2019 fills in for zero or more characters. The \u2018find\u2019 command can be used to locate files using a similar form. find . -name \"*.f*\" find . -name \"*.f?\" how is this different from the previous ls commands?","title":"BASH Wildcard Characters"},{"location":"lab_session/basic_unix/#quick-note-about-the-quotes","text":"The quote characters \u201c and \u2018 are different. In general, single quotes preserve the literal meaning of all characters between them. On the other hand, double quotes allow the shell to see what\u2019s between them and make substitutions when appropriate. For example: VRBL = someText echo '$VRBL' echo \" $VRBL \" However, some commands try to be \u2018smarter\u2019 about this behavior, so it\u2019s a little hard to predict what will happen in all cases. It\u2019s safest to experiment first when planning a command that depends on quoting \u2026 list filenames first, instead of changing them, etc. Finally, the \u2018backtick\u2019 characters ` (same key - unSHIFTED - as the tilde ~) causes the shell to interpret what\u2019s between them as a command, and return the result.","title":"Quick Note About the Quote(s)"},{"location":"lab_session/basic_unix/#counts-the-number-of-lines-in-file-and-stores-result-in-the-lines-variable","text":"LINES = ` cat PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome.1.bt2 | wc -l ` echo $LINES","title":"counts the number of lines in file and stores result in the LINES variable"},{"location":"lab_session/basic_unix/#symbolic-links","text":"Since copying or even moving large files (like sequence data) around your filesystem may be impractical, we can use links to reference \u2018distant\u2019 files without duplicating the data in the files. Symbolic links are disposable pointers that refer to other files, but behave like the referenced files in commands. I.e., they are essentially \u2018Shortcuts\u2019 (to use a Windows term) to a file or directory. The \u2018ln\u2019 command creates a link. You should, by default, always create a symbolic link using the -s option. ln -s PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa . ls -ltrhaF # notice the symbolic link pointing at its target grep -c \">\" genome.fa","title":"Symbolic Links"},{"location":"lab_session/basic_unix/#stdout-stderr","text":"Programs can write to two separate output streams, \u2018standard out\u2019 (STDOUT), and \u2018standard error\u2019 (STDERR). The former is generally for direct output of a program, while the latter is supposed to be used for reporting problems. I\u2019ve seen some bioinformatics tools use STDERR to report summary statistics about the output, but this is probably bad practice. Default behavior in a lot of cases is to dump both STDOUT and STDERR to the screen, unless you specify otherwise. In order to nail down what goes where, and record it for posterity: wc -c genome.fa 1 > chars.txt 2 > any.err the 1 st output, STDOUT, goes to \u2018chars.txt\u2019 the 2 nd output, STDERR, goes to \u2018any.err\u2019 cat chars.txt Contains the character count of the file genome.fa cat any.err Empty since no errors occured. Saving STDOUT is pretty routine (you want your results, yes?), but remember that explicitly saving STDERR is important on a remote server, since you may not directly see the \u2018screen\u2019 when you\u2019re running jobs.","title":"STDOUT &amp; STDERR"},{"location":"lab_session/basic_unix/#the-sed-command","text":"Let\u2019s take a look at the \u2018sed\u2019 command. NOTE: On Macs use \u2018gsed\u2019. sed (short for stream editor) is a command that allows you to manipulate character data in various ways. One useful thing it can do is substitution. Let\u2019s download a simple file to work on: wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/region.bed -O region.bed Take a look at the file: cat region.bed Now, let\u2019s make all the uppercase \u201cCHR\u201ds into lowercase: cat region.bed | sed 's/CHR/chr/' What happened? Only the first CHR changed. That is because we need to add the \u201cg\u201d option: cat region.bed | sed 's/CHR/chr/g' We can also do the the substitution without regards to case: cat region.bed | sed 's/chr/chr/gi' Let\u2019s break down the argument to sed (within the single quotes)\u2026 The \u201cs\u201d means \u201csubstitute\u201d, the word between the 1 st and 2 nd forward slashes (i.e. /) is the word the substitute for, the word between the 2 nd and 3 rd slashes is the word to substitute with, and finally the \u201cgi\u201d at the end are flags for global substitution (i.e. substituting along an entire line instead of just the first occurence on a line), and for case insenstivity (i.e. it will ignore the case of the letters when doing the substitution). Note that this doesn\u2019t change the file itself, it is simply piping the output of the cat command to sed and outputting to the screen. If you wanted to change the file itself, you could use the \u201c-i\u201d option to sed: cat region.bed sed -i 's/chr/chr/gi' region.bed Now if you look at the file, the lines have changed. cat region.bed Another useful use of sed is for capturing certain lines from a file. You can select certain lines from a file: sed '4q;d' region.bed This will just select the 4 th line from the file. You can also extract a range of lines from a file: sed -n '10,20p' region.bed This gets the 10 th through 20 th lines from the file. CHALLENGE: See if you can find a way to use sed to remove all the \u201cCHR\u201ds from the file.","title":"The sed command"},{"location":"lab_session/basic_unix/#more-pipes","text":"Now, let\u2019s delve into pipes a little more. Pipes are a very powerful way to look at and manipulate complex data using a series of simple programs. Let\u2019s look at some fastq files. Get a few small fastq files: wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/C61.subset.fq.gz -O C61.subset.fq.gz wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I561.subset.fq.gz -O I561.subset.fq.gz wget https://course-cg-5534.s3.amazonaws.com/unix_exercise/I894.subset.fq.gz -O I894.subset.fq.gz Since the files are gzipped files we need to use \u201czcat\u201d to look at them. zcat is just like cat except for gzipped files: zcat C61.subset.fq.gz | head Fastq records are 4 lines per sequence, a header line, the sequence, a plus sign (which is historical), and then the quality encoding for the sequence. Notice that each header line has the barcode for that read at the end of the line. Let\u2019s count the number of each barcode. In order to do that we need to just capture the header lines from this file. We can use \u201csed\u201d to do that: zcat C61.subset.fq.gz | sed -n '1~4p' | head By default sed prints every line. In this case we are giving the \u201c-n\u201d option to sed which will not print every line. Instead, we are giving it the argument \u201c1~4p\u201d, which means to print the first line, then skip 4 lines and print again, and then continue to do that. Now that we have a way to get just the headers, we need to isolate the part of the header that is the barcode. There are multiple ways to do this\u2026 we will use the cut command: zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | head So we are using the \u201c-d\u201d option to cut with \u201c:\u201d as the argument to that option, meaning that we will be using the delimiter \u201c:\u201d to split the input. Then we use the \u201c-f\u201d option with argument \u201c10\u201d, meaning that we want the 10 th field after the split. In this case, that is the barcode. Finally, as before, we need to sort the data and then use \u201cuniq -c\u201d to count. Then put it all together and run it on the entire dataset (This will take about a minute to run): zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | sort | uniq -c Now you have a list of how many reads were categorized into each barcode. Here is a sed tutorial for more exercises. CHALLENGE: Find the distribution of the first 5 bases of all the reads in C61_S67_L006_R1_001.fq.gz. I.e., count the number of times the first 5 bases of every read occurs across all reads.","title":"More pipes"},{"location":"lab_session/basic_unix/#loops","text":"Loops are useful for quickly telling the shell to perform one operation after another, in series. For example: for i in { 1 ..21 } ; do echo $i >> a ; done # put multiple lines of code on one line, each line terminated by ';' cat a","title":"Loops"},{"location":"lab_session/basic_unix/#1-through-21-on-separate-lines","text":"The general form is: for name in { list } ; do commands done The list can be a sequence of numbers or letters, or a group of files specified with wildcard characters: for i in { 3 ,2,1,liftoff } ; do echo $i ; done # needs more excitement! for i in { 3 ,2,1, \"liftoff!\" } ; do echo $i ; done # exclamation point will confuse the shell unless quoted A \u201cwhile\u201d loop is more convenient than a \u201cfor\u201d loop \u2026 if you don\u2019t readily know how many iterations of the loop you want: while { condition } ; do commands done Now, let\u2019s do some bioinformatics-y things with loops and pipes. First, let\u2019s write a command to get the nucleotide count of the first 10,000 reads in a file. Use zcat and sed to get only the read lines of a file, and then only take the first 10,000: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | less Use grep\u2019s \u201c-o\u201d option to get each nucleotide on a separate line (take a look at the man page for grep to understand how this works): zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | less Finally, use sort and uniq to get the counts: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c 264012 A 243434 C 215045 G 278 N 277231 T And, voila, we have the per nucleotide count for these reads! We just did this for one file, but what if we wanted to do it for all of our files? We certainly don\u2019t want to type the command by hand dozens of times. So we\u2019ll use a while loop. You can pipe a command into a while loop and it will iterate through each line of the input. First, get a listing of all your files: ls -1 *.fq.gz Pipe that into a while loop and read in the lines into a variable called \u201cx\u201d. We use \u201c$x\u201d to get the value of the variable in that iteration of the loop: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; done Add the command we created above into the loop, placing $x where the filename would be and semi-colons inbetween commands: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c ; done When this runs it will print the name of every single file being processed and the nucleotide count for the reads from those files. Now, let\u2019s say you wanted to write the output of each command to a separate file. We would redirect the output to a filename, but we need to create a different file name for each command and we want the file name to reflect its contents, i.e. the output file name should be based on the input file name. So we use \u201cparameter expansion\u201d, which is fancy way of saying substitution: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; done This will put the output of the counting command into a file whose name is the prefix of the input file plus \u201c.nucl_count.txt\u201d. It will do this for every input file.","title":"&lt;1 through 21 on separate lines&gt;"},{"location":"lab_session/basic_unix/#manipulation-of-a-fasta-file","text":"Let\u2019s copy the phiX-174 genome (using the \u2018cp\u2019 command) to our current directory so we can play with it: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa phix.fa Similarly we can also use the move command here, but then ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa will no longer be there: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ mv ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa phix.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ This functionality of mv is why it is used to rename files. Note how we copied the \u2018genome.fa\u2019 file to a different name: \u2018phix.fa\u2019 wc -l phix.fa count the number of lines in the file using \u2018wc\u2019 (word count) and parameter \u2018-l\u2019 (lines). We can use the \u2018grep\u2019 command to search for matches to patterns. \u2018grep\u2019 comes from \u2018globally search for a regular expression and print\u2019. grep -c '>' phix.fa #Only one FASTA sequence entry, since only one header line (\u2018>gi|somethingsomething\u2026\u2019) cat phix.fa This may not be useful for anything larger than a virus! Let\u2019s look at the start codon and the two following codons: grep --color \"ATG......\" phix.fa #\u2019.\u2019 characters are the single-character wildcards for grep. So \u201cATG\u2026\u2026\u201d matches any set of 9 characters that starts with ATG. #Use the \u2013color \u2018-o\u2019 option to only print the pattern matches, one per line grep -o \"ATG......\" phix.fa #Use the \u2018cut\u2019 command with \u2018-c\u2019 to select characters 4-6, the second codon grep --color -o \"ATG......\" phix.fa | cut -c4-6 \u2018sort\u2019 the second codon sequences ( default order is same as ASCII table ; see \u2018man ascii\u2019 ) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort #Combine successive identical sequences, but count them using the \u2018uniq\u2019 command with the \u2018-c\u2019 option grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c #Finally sort using reverse numeric order (\u2018-rn\u2019) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c | sort -rn \u2026 which gives us the most common codons first This may not be a particularly useful thing to do with a genomic FASTA file, but it illustrates the process by which one can build up a string of operations, using pipes, in order to ask quantitative questions about sequence content. More generally than that, this process allows one to ask questions about files and file contents and the operating system, and verify at each step that the process so far is working as expected. The command line is, in this sense, really a modular workflow management system.","title":"Manipulation of a FASTA File"},{"location":"lab_session/basic_unix/#shell-scripts-file-permissions","text":"Often it\u2019s useful to define a whole string of commands to run on some input, so that (1) you can be sure you\u2019re running the same commands on all data, and (2) so you don\u2019t have to type the same commands in over and over! Let\u2019s use the \u2018nano\u2019 text editor program that\u2019s pretty reliably installed on most linux systems. nano test.sh","title":"Shell Scripts, File Permissions"},{"location":"lab_session/basic_unix/#insert-cli_figure7","text":"nano now occupies the whole screen; see commands at the bottom. Let\u2019s type in a few commands. First we need to put the following line at the top of the file: #!/bin/bash The \u201c#!\u201d at the beginning of a script tells the shell what language to use to interpret the rest of the script. In our case, we will be writing \u201cbash\u201d commands, so we specify the full path of the bash executable after the \u201c#!\u201d. Then, add some commands: #!/bin/bash echo \"Start script...\" pwd ls -l sleep 10 echo \"End script.\" Hit Cntl-O and then enter to save the file, and then Cntl-X to exit nano. Though there are ways to run the commands in test.sh right now, it\u2019s generally useful to give yourself (and others) \u2018execute\u2019 permissions for test.sh, really making it a shell script. Note the characters in the first (left-most) field of the file listing: ls -lh test.sh -rw-rw-r-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first \u2018-\u2018 becomes a \u2018d\u2019 if the \u2018file\u2019 is actually a directory. The next three characters represent read, write, and execute permissions for the file owner (you), followed by three characters for users in the owner\u2019s group, followed by three characters for all other users. Run the \u2018chmod\u2019 command to change permissions for the \u2018test.sh\u2019 file, adding execute permissions (\u2018+x\u2019) for the user (you) and your group (\u2018ug\u2019): chmod ug+x test.sh ls -lh test.sh -rwxr-xr-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first 10 characters of the output represent the file and permissions. The first character is the file type, the next three sets of three represent the file permissions for the user, group, and everyone respectively. r = read w = write x = execute So let\u2019s run this script. We have to provide a relative reference to the script \u2018./\u2019 because its not our our \u201cPATH\u201d.: #you can do either ./test.sh #or you can run it like this bash test.sh And you should see all the commands in the file run in sequential order in the terminal.","title":"insert cli_figure7"},{"location":"lab_session/basic_unix/#command-line-arguments-for-shell-scripts","text":"Now let\u2019s modify our script to use command line arguments, which are arguments that can come after the script name (when executing) to be part of the input inside the script. This allows us to use the same script with different inputs. In order to do so, we add variables $1, $2, $3, etc\u2026. in the script where we want our input to be. So, for example, use nano to modify your test.sh script to look like this: #!/bin/bash echo \"Start script...\" PWD = ` pwd ` echo \"The present working directory is $PWD \" ls -l $1 sleep $2 wc -l $3 echo \"End script.\" Now, rerun the script using command line arguments like this: ./test.sh genome.fa 15 PhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt Note that each argument is separated by a space, so $1 becomes \u201cgenome.fa\u201d, $2 becomes \u201c15\u201d, and $3 becomes \u201cPhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\u201d. Then the commands are run using those values. Now rerun the script with some other values: ./test.sh .. 5 genome.fa Now, $1 becomes \u201c..\u201d, $2 is \u201c5\u201d, and $3 is \u201cgenome.fa\u201d.","title":"Command Line Arguments for Shell Scripts"},{"location":"lab_session/basic_unix/#pipes-and-loops-inside-scripts","text":"Open a new text file using the text editor \u201cnano\u201d: nano get_nucl_counts.sh Copy and Paste the following into the file: #!/bin/bash zcat $1 | sed -n '2~4p' | head - $2 | grep -o . | sort | uniq -c Save the file and exit. Change the permissions on the file to make it executable: chmod a+x get_nucl_counts.sh Now, we can run this script giving it different arguments every time. The first argument (i.e. the first text after the script name when it is run) will get put into the variable \u201c \\(1\u201d. The second argument (delimited by spaces) will get put into \u201c\\) 2\u201d. In this case, \u201c \\(1\u201d is the file name, and \u201c\\) 2\u201d is the number of reads we want to count. So, then we can run the script over and over again using different values and the command will run based on those values: ./get_nucl_counts.sh I561.subset.fq.gz 1000 ./get_nucl_counts.sh I561.subset.fq.gz 10000 ./get_nucl_counts.sh C61.subset.fq.gz 555 We can also put loops into a script. We\u2019ll take the loop we created earlier and put it into a file, breaking it up for readability and using backslashes for line continuation: nano get_nucl_counts_loop.sh Put this in the file and save it: #!/bin/bash ls -1 *.fq.gz | \\ while read x ; do \\ echo $x is being processed... ; \\ zcat $x | sed -n '2~4p' | head - $1 | \\ grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; \\ done Make it executable: chmod a+x get_nucl_counts_loop.sh And now we can execute the entire loop using the script. Note that there is only one argument now, the number of reads to use: ./get_nucl_counts_loop.sh 100","title":"Pipes and Loops inside scripts"},{"location":"lab_session/basic_unix/#finally-a-good-summary-for-rounding-up-the-session","text":"TEN SIMPLE RULES FOR GETTING STARTED WITH COMMAND-LINE BIOINFORMATICS","title":"FINALLY - a good summary for rounding up the session"},{"location":"lab_session/basics_awk/","text":"AWK crash course for Bioinformatics \u00b6 An introduction on how to analyze table data without MS Excel \u00b6 What is AWK? \u00b6 AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs. It is a standard feature of most Unix-like operating systems. source: Wikipedia Why awk? \u00b6 You can replace a pipeline of 'stuff | grep | sed | cut...' with a single call to awk. For a simple script, most of the timelag is in loading these apps into memory, and it's much faster to do it all with one. This is ideal for something like an openbox pipe menu where you want to generate something on the fly. You can use awk to make a neat one-liner for some quick job in the terminal, or build an awk section into a shell script. Simple AWK commands \u00b6 An AWK program consists of a sequence of pattern-action statements and optional function definitions. It processes text files. AWK is a line oriented language. It divides a file into lines called records. Each line is broken up into a sequence of fields. The fields are accessed by special variables: $1 reads the first field, $2 the second and so on. The $0 variable refers to the whole record. The structure of an AWK program has the following form: pattern { action } The pattern is a test that is performed on each of the records. If the condition is met then the action is performed. Either pattern or action can be omitted, but not both. The default pattern matches each line and the default action is to print the record. awk -f program-file [ file-list ] awk program [ file-list ] An AWK program can be run in two basic ways: a) the program is read from a separate file; the name of the program follows the -f option, b) the program is specified on the command line enclosed by quote characters. Print all the lines from a file \u00b6 By default, awk prints all lines of a file , so to print every line of above created file use below command awk '{print}' file Note: In awk command \u2018{print}\u2019 is used print all fields along with their values. Print only specific field like 2 nd & 3 rd \u00b6 In awk command, we use $ (dollar) symbol followed by field number to prints field values. awk -F \",\" '{print $2, $3}' file In the above command we have used the option -F \u201c,\u201d which specifies that comma (,) is the field separator in the file. This is usually good practice when dealing with tables where the separators between each column could be single or multiple white-space (\" \") or tab (\"\\t\") or a colon (\":\") or a semicolon (\";\"). AWK one-liners \u00b6 AWK one-liners are simple one-shot programs run from the command line. Let us have the following text file: words.txt We want to print all words included in the words.txt file that are longer than five characters. wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/words.txt -O words.txt awk 'length($1) > 5 {print $0}' words.txt storeroom existence ministerial falcon bookworm bookcase The AWK program is placed between two single quote characters. The first is the pattern; we specify that the length of the record is greater that five. The length function returns the length of the string. The $1 variable refers to the first field of the record; in our case there is only one field per record. The action is placed between curly brackets. awk 'length($1) > 5' words.txt storeroom existence ministerial falcon bookworm bookcase As we have specified earlier, the action can be omitted. In such a case a default action is performed \u2014 printing of the whole record. awk 'length($1) == 3' words.txt cup sky top war We print all words that have three characters. awk '!(length($1) == 3)' words.txt storeroom tree store book cloud existence ministerial falcon town bookworm bookcase With the ! operator, we can negate the condition; we print all lines that do not have three characters. awk '(length($1) == 3) || (length($1) == 4)' words.txt tree cup book town sky top war Next we apply conditions on numbers. We have a file with scores of students - scores.txt. wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/scores.txt -O scores.txt awk '$2 >= 90 { print $0 }' scores.txt Lucia 95 Joe 92 Sophia 90 We print all students with scores 90+. awk '$2 >= 90 { print }' scores.txt Lucia 95 Joe 92 Sophia 90 If we omit an argument for the print function, the $0 is assumed. awk '$2 >= 90' scores.txt Lucia 95 Joe 92 Sophia 90 A missing { action } means print the matching line. awk '{ if ($2 >= 90) print }' scores.txt Lucia 95 Joe 92 Sophia 90 Instead of a pattern, we can also use an if condition in the action. awk '{sum += $2} END { printf(\"The average score is %.2f\\n\", sum/NR) }' scores.txt The average score is 77.56 This command calculates the average score. In the action block, we calculate the sum of scores. In the END block, we print the average score. We format the output with the built-in printf function. The %.2f is a format specifier; each specifier begins with the % character. The .2 is the precision -- the number of digits after the decimal point. The f expects a floating point value. The \\n is not a part of the specifier; it is a newline character. It prints a newline after the string is shown on the terminal. AWK working with pipes \u00b6 AWK can receive input and send output to other commands via the pipe. echo -e \"1 2 3 5\\n2 2 3 8\" | awk '{print $(NF)}' 5 8 In this case, AWK receives output from the echo command. It prints the values of last column. awk -F: '$7 ~ /bash/ {print $1}' /etc/passwd | wc -l 3 Here, the AWK program sends data to the wc command via the pipe. In the AWK program, we find out those users who use bash. Their names are passed to the wc command which counts them. In our case, there are three users using bash. AWK for Bioinformatics - looking at transcriptome data \u00b6 You can find a lot of online tutorials, but here we will try out a few steps which show how a bioinformatician analyses a GTF file using awk. GTF is a special file format that contains information about different regions of the genome and their associated annotations. More on that here - Ensembl File Formats-GTF . wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/transcriptome.gtf -O transcriptome.gtf head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F \"\\t\" '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Chaining AWK calls \u00b6 We will start with the AWK call that we were using before, and we will append a pipe | so it can be used as input for the next AWK call, this time using a space and a semicolon as the delimiter to define what a column is: awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head | less -S gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Now that we see what the third column looks like, we can filter for protein-coding genes awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 == \"gene_type \\\"protein_coding\\\"\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENSG00000237683.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1\" ; level 3 ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENSG00000235249.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29\" ; level 2 ; havana_gene \"OTTHUMG00000002860.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENSG00000185097.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16\" ; level 2 ; havana_gene \"OTTHUMG00000002581.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENSG00000269831.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1\" ; level 3 ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENSG00000269308.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2\" ; level 3 ; gene_id \"ENSG00000187634.6\" ; transcript_id \"ENSG00000187634.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"SAMD11\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"SAMD11\" ; level 2 ; havana_gene \"OTTHUMG00000040719.8\" ; gene_id \"ENSG00000268179.1\" ; transcript_id \"ENSG00000268179.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.1\" ; level 3 ; gene_id \"ENSG00000188976.6\" ; transcript_id \"ENSG00000188976.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NOC2L\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NOC2L\" ; level 2 ; havana_gene \"OTTHUMG00000040720.1\" ; gene_id \"ENSG00000187961.9\" ; transcript_id \"ENSG00000187961.9\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"KLHL17\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"KLHL17\" ; level 2 ; havana_gene \"OTTHUMG00000040721.6\" ; I added a space and a backslash (not to be confused with the regular slash /) after the first and second pipes to split the code into two lines; this makes it easier to read and it highlights that we are taking two separate steps. The double quotes around protein_coding are escaped (also with a backslash \\\") because they are already contained inside double quotes. To avoid the backslashing drama we can use the partial matching operator ~ instead of the total equality operator ==. awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S The output is the same as before: those lines that contain a protein_coding somewhere in their third column make the partial matching rule true, and they get printed (which is the default behavior when there are no curly braces). Now we have all the protein-coding genes, but how do we get to the genes that only have one exon? Well, we have to revisit our initial AWK call: we selected lines that corresponded to genes, but we actually want lines that correspond to exons. That\u2019s an easy fix, we just change the word \u201cgene\u201d for the word \u201cexon\u201d. Everything else stays the same. awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENST00000335137.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5-001\" ; exon_number 1 ; exon_id \"ENSE00002319515.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30547.1\" ; havana_gene \"OTTHUMG00000001094.1\" ; havana_transcript \"OTTHUMT00000003223.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 1 ; exon_id \"ENSE00002221580.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 2 ; exon_id \"ENSE00002314092.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENST00000426406.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29-001\" ; exon_number 1 ; exon_id \"ENSE00002316283.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41220.1\" ; havana_gene \"OTTHUMG00000002860.1\" ; havana_transcript \"OTTHUMT00000007999.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENST00000332831.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16-001\" ; exon_number 1 ; exon_id \"ENSE00002324228.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41221.1\" ; havana_gene \"OTTHUMG00000002581.1\" ; havana_transcript \"OTTHUMT00000007334.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 1 ; exon_id \"ENSE00003063549.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 2 ; exon_id \"ENSE00003084653.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 3 ; exon_id \"ENSE00003138540.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 1 ; exon_id \"ENSE00003079649.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 2 ; exon_id \"ENSE00003048391.1\" ; level 3 ; tag \"basic\" ; Cleaning up the output \u00b6 Before we try to count how many exons belong to the same protein-coding gene, let\u2019s simplify the output so we only get the gene names (which are in column 5). awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\" {print $5}' | \\ head gene_name \"OR4F5\" gene_name \"AL627309.1\" gene_name \"AL627309.1\" gene_name \"OR4F29\" gene_name \"OR4F16\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL645608.2\" gene_name \"AL645608.2\" This is sort of what we want. We could chain another AWK call using -F \" \", and pick the second column (which would get rid of the gene_name). Feel free to try that approach if you are curious. We can also take a shortcut by using the tr -d command, which deletes whatever characters appear in double quotes. For example, to remove every vowel from a sentence: echo \"This unix thing is cool\" | tr -d \"aeiou\" # Ths nx thng s cl Let\u2019s try deleting all the semicolons and quotes before the second AWK call: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" '$6 == \"protein_coding\" {print $10}' | \\ head OR4F5 AL627309.1 AL627309.1 OR4F29 OR4F16 AL669831.1 AL669831.1 AL669831.1 AL645608.2 AL645608.2 Run bash awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | tr -d \";\\\"\" | head to understand what the input to the second AWK call looks like. It\u2019s just words separated by spaces; the sixth word corresponds to the gene type, and the tenth word to the gene name. Counting genes \u00b6 There is one more concept we need to introduce before we start counting. AWK uses a special rule called END, which is only true once the input is over. See an example: echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { print $1 } END { print \"Done with letters!\" } ' a a b b b c Done with letters! The -e option tells echo to convert each \\n into a new line, which is a convenient way of printing multiple lines from a single character string. In AWK, any amount of whitespace is allowed between the initial and the final quote '. I separated the first rule from the END rule to make them easier to read. Now we are ready for counting. echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { counter[$1] += 1 } END { for (letter in counter){ print letter, counter[letter] } } ' a 2 b 3 c 1 Wow, what is all that madness? Instead of printing each letter, we manipulate a variable that we called counter. This variable is special because it is followed by brackets [ ], which makes it an associative array, a fancy way of calling a variable that stores key-value pairs. In this case we chose the values of the first column \\(1 to be the keys of the counter variable, which means there are 3 keys (\u201ca\u201d, \u201cb\u201d and \u201cc\u201d). The values are initialized to 0. For every line in the input, we add a 1 to the value in the array whose key is equal to \\(1. We use the addition operator +=, a shortcut for counter[\\) 1] = counter[\\) 1] + 1. When all the lines are read, the END rule becomes true, and the code between the curly braces { } is executed. The structure for (key in associate_array) { some_code } is called a for loop, and it executes some_code as many times as there are keys in the array. letter is the name that we chose for the variable that cycles through all the keys in counter, and counter[letter] gives the value stored in counter for each letter (which we we calculated in the previous curly brace chunk). Now we can apply this to the real example: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" ' $6 == \"protein_coding\" { gene_counter[$10] += 1 } END { for (gene_name in gene_counter){ print gene_name, gene_counter[gene_name] } }' > number_of_exons_by_gene.txt head number_of_exons_by_gene.txt CAPN6 24 ARL14EPL 3 DACH1 38 IFNA13 1 HSP90AB1 36 CAPN7 52 DACH2 84 IFNA14 1 LARS 188 CAPN8 78 If you are using the real transcriptome, it takes less than a minute to count up one million exons. Pretty impressive. We saved the output to a file, so now we can use AWK to see how many genes are made up of a single exon. awk '$2 == 1' number_of_exons_by_gene.txt | wc -l # 1362 I will suggest to follow the original tutorial if you need to refer to these steps later on for your own data: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"AWK"},{"location":"lab_session/basics_awk/#awk-crash-course-for-bioinformatics","text":"","title":"AWK crash course for Bioinformatics"},{"location":"lab_session/basics_awk/#an-introduction-on-how-to-analyze-table-data-without-ms-excel","text":"","title":"An introduction on how to analyze table data without MS Excel"},{"location":"lab_session/basics_awk/#what-is-awk","text":"AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs. It is a standard feature of most Unix-like operating systems. source: Wikipedia","title":"What is AWK?"},{"location":"lab_session/basics_awk/#why-awk","text":"You can replace a pipeline of 'stuff | grep | sed | cut...' with a single call to awk. For a simple script, most of the timelag is in loading these apps into memory, and it's much faster to do it all with one. This is ideal for something like an openbox pipe menu where you want to generate something on the fly. You can use awk to make a neat one-liner for some quick job in the terminal, or build an awk section into a shell script.","title":"Why awk?"},{"location":"lab_session/basics_awk/#simple-awk-commands","text":"An AWK program consists of a sequence of pattern-action statements and optional function definitions. It processes text files. AWK is a line oriented language. It divides a file into lines called records. Each line is broken up into a sequence of fields. The fields are accessed by special variables: $1 reads the first field, $2 the second and so on. The $0 variable refers to the whole record. The structure of an AWK program has the following form: pattern { action } The pattern is a test that is performed on each of the records. If the condition is met then the action is performed. Either pattern or action can be omitted, but not both. The default pattern matches each line and the default action is to print the record. awk -f program-file [ file-list ] awk program [ file-list ] An AWK program can be run in two basic ways: a) the program is read from a separate file; the name of the program follows the -f option, b) the program is specified on the command line enclosed by quote characters.","title":"Simple AWK commands"},{"location":"lab_session/basics_awk/#print-all-the-lines-from-a-file","text":"By default, awk prints all lines of a file , so to print every line of above created file use below command awk '{print}' file Note: In awk command \u2018{print}\u2019 is used print all fields along with their values.","title":"Print all the lines from a file"},{"location":"lab_session/basics_awk/#print-only-specific-field-like-2nd-3rd","text":"In awk command, we use $ (dollar) symbol followed by field number to prints field values. awk -F \",\" '{print $2, $3}' file In the above command we have used the option -F \u201c,\u201d which specifies that comma (,) is the field separator in the file. This is usually good practice when dealing with tables where the separators between each column could be single or multiple white-space (\" \") or tab (\"\\t\") or a colon (\":\") or a semicolon (\";\").","title":"Print only specific field like 2nd &amp; 3rd"},{"location":"lab_session/basics_awk/#awk-one-liners","text":"AWK one-liners are simple one-shot programs run from the command line. Let us have the following text file: words.txt We want to print all words included in the words.txt file that are longer than five characters. wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/words.txt -O words.txt awk 'length($1) > 5 {print $0}' words.txt storeroom existence ministerial falcon bookworm bookcase The AWK program is placed between two single quote characters. The first is the pattern; we specify that the length of the record is greater that five. The length function returns the length of the string. The $1 variable refers to the first field of the record; in our case there is only one field per record. The action is placed between curly brackets. awk 'length($1) > 5' words.txt storeroom existence ministerial falcon bookworm bookcase As we have specified earlier, the action can be omitted. In such a case a default action is performed \u2014 printing of the whole record. awk 'length($1) == 3' words.txt cup sky top war We print all words that have three characters. awk '!(length($1) == 3)' words.txt storeroom tree store book cloud existence ministerial falcon town bookworm bookcase With the ! operator, we can negate the condition; we print all lines that do not have three characters. awk '(length($1) == 3) || (length($1) == 4)' words.txt tree cup book town sky top war Next we apply conditions on numbers. We have a file with scores of students - scores.txt. wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/scores.txt -O scores.txt awk '$2 >= 90 { print $0 }' scores.txt Lucia 95 Joe 92 Sophia 90 We print all students with scores 90+. awk '$2 >= 90 { print }' scores.txt Lucia 95 Joe 92 Sophia 90 If we omit an argument for the print function, the $0 is assumed. awk '$2 >= 90' scores.txt Lucia 95 Joe 92 Sophia 90 A missing { action } means print the matching line. awk '{ if ($2 >= 90) print }' scores.txt Lucia 95 Joe 92 Sophia 90 Instead of a pattern, we can also use an if condition in the action. awk '{sum += $2} END { printf(\"The average score is %.2f\\n\", sum/NR) }' scores.txt The average score is 77.56 This command calculates the average score. In the action block, we calculate the sum of scores. In the END block, we print the average score. We format the output with the built-in printf function. The %.2f is a format specifier; each specifier begins with the % character. The .2 is the precision -- the number of digits after the decimal point. The f expects a floating point value. The \\n is not a part of the specifier; it is a newline character. It prints a newline after the string is shown on the terminal.","title":"AWK one-liners"},{"location":"lab_session/basics_awk/#awk-working-with-pipes","text":"AWK can receive input and send output to other commands via the pipe. echo -e \"1 2 3 5\\n2 2 3 8\" | awk '{print $(NF)}' 5 8 In this case, AWK receives output from the echo command. It prints the values of last column. awk -F: '$7 ~ /bash/ {print $1}' /etc/passwd | wc -l 3 Here, the AWK program sends data to the wc command via the pipe. In the AWK program, we find out those users who use bash. Their names are passed to the wc command which counts them. In our case, there are three users using bash.","title":"AWK working with pipes"},{"location":"lab_session/basics_awk/#awk-for-bioinformatics-looking-at-transcriptome-data","text":"You can find a lot of online tutorials, but here we will try out a few steps which show how a bioinformatician analyses a GTF file using awk. GTF is a special file format that contains information about different regions of the genome and their associated annotations. More on that here - Ensembl File Formats-GTF . wget https://course-cg-5534.s3.amazonaws.com/awk_exercise/transcriptome.gtf -O transcriptome.gtf head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F \"\\t\" '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\"","title":"AWK for Bioinformatics - looking at transcriptome data"},{"location":"lab_session/basics_awk/#chaining-awk-calls","text":"We will start with the AWK call that we were using before, and we will append a pipe | so it can be used as input for the next AWK call, this time using a space and a semicolon as the delimiter to define what a column is: awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head | less -S gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Now that we see what the third column looks like, we can filter for protein-coding genes awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 == \"gene_type \\\"protein_coding\\\"\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENSG00000237683.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1\" ; level 3 ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENSG00000235249.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29\" ; level 2 ; havana_gene \"OTTHUMG00000002860.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENSG00000185097.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16\" ; level 2 ; havana_gene \"OTTHUMG00000002581.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENSG00000269831.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1\" ; level 3 ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENSG00000269308.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2\" ; level 3 ; gene_id \"ENSG00000187634.6\" ; transcript_id \"ENSG00000187634.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"SAMD11\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"SAMD11\" ; level 2 ; havana_gene \"OTTHUMG00000040719.8\" ; gene_id \"ENSG00000268179.1\" ; transcript_id \"ENSG00000268179.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.1\" ; level 3 ; gene_id \"ENSG00000188976.6\" ; transcript_id \"ENSG00000188976.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NOC2L\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NOC2L\" ; level 2 ; havana_gene \"OTTHUMG00000040720.1\" ; gene_id \"ENSG00000187961.9\" ; transcript_id \"ENSG00000187961.9\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"KLHL17\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"KLHL17\" ; level 2 ; havana_gene \"OTTHUMG00000040721.6\" ; I added a space and a backslash (not to be confused with the regular slash /) after the first and second pipes to split the code into two lines; this makes it easier to read and it highlights that we are taking two separate steps. The double quotes around protein_coding are escaped (also with a backslash \\\") because they are already contained inside double quotes. To avoid the backslashing drama we can use the partial matching operator ~ instead of the total equality operator ==. awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S The output is the same as before: those lines that contain a protein_coding somewhere in their third column make the partial matching rule true, and they get printed (which is the default behavior when there are no curly braces). Now we have all the protein-coding genes, but how do we get to the genes that only have one exon? Well, we have to revisit our initial AWK call: we selected lines that corresponded to genes, but we actually want lines that correspond to exons. That\u2019s an easy fix, we just change the word \u201cgene\u201d for the word \u201cexon\u201d. Everything else stays the same. awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENST00000335137.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5-001\" ; exon_number 1 ; exon_id \"ENSE00002319515.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30547.1\" ; havana_gene \"OTTHUMG00000001094.1\" ; havana_transcript \"OTTHUMT00000003223.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 1 ; exon_id \"ENSE00002221580.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 2 ; exon_id \"ENSE00002314092.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENST00000426406.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29-001\" ; exon_number 1 ; exon_id \"ENSE00002316283.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41220.1\" ; havana_gene \"OTTHUMG00000002860.1\" ; havana_transcript \"OTTHUMT00000007999.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENST00000332831.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16-001\" ; exon_number 1 ; exon_id \"ENSE00002324228.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41221.1\" ; havana_gene \"OTTHUMG00000002581.1\" ; havana_transcript \"OTTHUMT00000007334.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 1 ; exon_id \"ENSE00003063549.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 2 ; exon_id \"ENSE00003084653.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 3 ; exon_id \"ENSE00003138540.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 1 ; exon_id \"ENSE00003079649.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 2 ; exon_id \"ENSE00003048391.1\" ; level 3 ; tag \"basic\" ;","title":"Chaining AWK calls"},{"location":"lab_session/basics_awk/#cleaning-up-the-output","text":"Before we try to count how many exons belong to the same protein-coding gene, let\u2019s simplify the output so we only get the gene names (which are in column 5). awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\" {print $5}' | \\ head gene_name \"OR4F5\" gene_name \"AL627309.1\" gene_name \"AL627309.1\" gene_name \"OR4F29\" gene_name \"OR4F16\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL645608.2\" gene_name \"AL645608.2\" This is sort of what we want. We could chain another AWK call using -F \" \", and pick the second column (which would get rid of the gene_name). Feel free to try that approach if you are curious. We can also take a shortcut by using the tr -d command, which deletes whatever characters appear in double quotes. For example, to remove every vowel from a sentence: echo \"This unix thing is cool\" | tr -d \"aeiou\" # Ths nx thng s cl Let\u2019s try deleting all the semicolons and quotes before the second AWK call: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" '$6 == \"protein_coding\" {print $10}' | \\ head OR4F5 AL627309.1 AL627309.1 OR4F29 OR4F16 AL669831.1 AL669831.1 AL669831.1 AL645608.2 AL645608.2 Run bash awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | tr -d \";\\\"\" | head to understand what the input to the second AWK call looks like. It\u2019s just words separated by spaces; the sixth word corresponds to the gene type, and the tenth word to the gene name.","title":"Cleaning up the output"},{"location":"lab_session/basics_awk/#counting-genes","text":"There is one more concept we need to introduce before we start counting. AWK uses a special rule called END, which is only true once the input is over. See an example: echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { print $1 } END { print \"Done with letters!\" } ' a a b b b c Done with letters! The -e option tells echo to convert each \\n into a new line, which is a convenient way of printing multiple lines from a single character string. In AWK, any amount of whitespace is allowed between the initial and the final quote '. I separated the first rule from the END rule to make them easier to read. Now we are ready for counting. echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { counter[$1] += 1 } END { for (letter in counter){ print letter, counter[letter] } } ' a 2 b 3 c 1 Wow, what is all that madness? Instead of printing each letter, we manipulate a variable that we called counter. This variable is special because it is followed by brackets [ ], which makes it an associative array, a fancy way of calling a variable that stores key-value pairs. In this case we chose the values of the first column \\(1 to be the keys of the counter variable, which means there are 3 keys (\u201ca\u201d, \u201cb\u201d and \u201cc\u201d). The values are initialized to 0. For every line in the input, we add a 1 to the value in the array whose key is equal to \\(1. We use the addition operator +=, a shortcut for counter[\\) 1] = counter[\\) 1] + 1. When all the lines are read, the END rule becomes true, and the code between the curly braces { } is executed. The structure for (key in associate_array) { some_code } is called a for loop, and it executes some_code as many times as there are keys in the array. letter is the name that we chose for the variable that cycles through all the keys in counter, and counter[letter] gives the value stored in counter for each letter (which we we calculated in the previous curly brace chunk). Now we can apply this to the real example: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" ' $6 == \"protein_coding\" { gene_counter[$10] += 1 } END { for (gene_name in gene_counter){ print gene_name, gene_counter[gene_name] } }' > number_of_exons_by_gene.txt head number_of_exons_by_gene.txt CAPN6 24 ARL14EPL 3 DACH1 38 IFNA13 1 HSP90AB1 36 CAPN7 52 DACH2 84 IFNA14 1 LARS 188 CAPN8 78 If you are using the real transcriptome, it takes less than a minute to count up one million exons. Pretty impressive. We saved the output to a file, so now we can use AWK to see how many genes are made up of a single exon. awk '$2 == 1' number_of_exons_by_gene.txt | wc -l # 1362 I will suggest to follow the original tutorial if you need to refer to these steps later on for your own data: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"Counting genes"},{"location":"lab_session/human_genome/","text":"Human genome reference files \u00b6 The human genome is work in progress. Different versions exist (assemblies) and is often a source of confusion in genomics. Quick use of published data is a bit more challenging if a differnt genome build was applied than what is used in your lab. Also, there may even be differences in the latest versions, e.g. the GRCh37 and the hg19 assemblies from NCBI and USCS, respectively, had different mitochondrial genome. In genomics, the reference genome offers a scaffold upon which new data can be mapped, which is a much more efficient way rather than building a genome from scratch. Avery nice overview of the human genome versions can be found here . During this course version GRCh38, from the Genome Reference Consortium, with modifications fromt he 1000 genomes consortium will be used. It includes extra decoy and HLA sequences. The files are already available on AWS but can be downloaded from here . As several of the analysis steps wold take many hours we will use a smaller version of the reference genome which contains chr6 and chr17. These were choosen because the datat that will be analysed contain interesting varaiants on these chromosomes. - Connect to your AWS instance # Remember, source the settings, paths and variables file source .bashrc # Check the CHRS variable. echo $CHRS # This variable should return \"chr6_and_chr17\", if not, reach of for assistance. # Go to the directory for the reference genome files cd ~/workspace/inputs/references/genome # List the files ls -halt # Check what the reference genome looks like. When you have the reference genome open just type \"1000\" and press enter. You will then jump 1000 lines. less -SN ref_genome.fa #less -SN will be used during the course. Check the man page. Give it some time to understand how it it structured, it is very helpful to be able to read the help-pages fast when working on the command line. man less # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\" # View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from the end of the filefile tail -n 10 ref_genome.fa # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"? # Google \"IUPAC nucleic acid codes\" Split the long fasta by chromosome \u00b6 cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/ # View contents. tree Index the fasta files \u00b6 Indexing is widely used in bioinformatics workflows to improve performance. Typically it is applied to large data files with many records to improve the ability of tools to rapidly access specific locations of the file. For example, if we have alignments against the entire genome and we want to visualize those alignments for a single gene on chr17 in a genome viewer such as IGV, we don\u2019t want the viewer to have to scan through the entire file. Indexing allows us to jump right to the correct place in the file and pull out just the information we need without reading much of the file. # first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome ls -halt rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # What information does the index fail store? # google \"fasta index fai format\" or something similar # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. head ref_genome.dict # less can also be applied #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa Human genome transcriptome reference files \u00b6 # Make sure CHRS environment variable is set. echo $CHRS cd ~/workspace/inputs/references/transcriptome # List files in the directory ls -halt # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. # When the file is displayed usin \"less\" type \"/\" and then a something you want to highlight e.g. \"chr6\". This is very useful when searching for specific infomration less -SN ref_transcriptome.gtf Explore the contents of the transcriptome reference files \u00b6 #How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c Below we will prepare files for the bioinformatic tools (HISAT/Kallisto) used to analyse RNA-sequencing data. Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome \u00b6 cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome Create a reference transcriptome index for use with Kallisto \u00b6 cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Human Genome"},{"location":"lab_session/human_genome/#human-genome-reference-files","text":"The human genome is work in progress. Different versions exist (assemblies) and is often a source of confusion in genomics. Quick use of published data is a bit more challenging if a differnt genome build was applied than what is used in your lab. Also, there may even be differences in the latest versions, e.g. the GRCh37 and the hg19 assemblies from NCBI and USCS, respectively, had different mitochondrial genome. In genomics, the reference genome offers a scaffold upon which new data can be mapped, which is a much more efficient way rather than building a genome from scratch. Avery nice overview of the human genome versions can be found here . During this course version GRCh38, from the Genome Reference Consortium, with modifications fromt he 1000 genomes consortium will be used. It includes extra decoy and HLA sequences. The files are already available on AWS but can be downloaded from here . As several of the analysis steps wold take many hours we will use a smaller version of the reference genome which contains chr6 and chr17. These were choosen because the datat that will be analysed contain interesting varaiants on these chromosomes. - Connect to your AWS instance # Remember, source the settings, paths and variables file source .bashrc # Check the CHRS variable. echo $CHRS # This variable should return \"chr6_and_chr17\", if not, reach of for assistance. # Go to the directory for the reference genome files cd ~/workspace/inputs/references/genome # List the files ls -halt # Check what the reference genome looks like. When you have the reference genome open just type \"1000\" and press enter. You will then jump 1000 lines. less -SN ref_genome.fa #less -SN will be used during the course. Check the man page. Give it some time to understand how it it structured, it is very helpful to be able to read the help-pages fast when working on the command line. man less # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\" # View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from the end of the filefile tail -n 10 ref_genome.fa # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"? # Google \"IUPAC nucleic acid codes\"","title":"Human genome reference files"},{"location":"lab_session/human_genome/#split-the-long-fasta-by-chromosome","text":"cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/ # View contents. tree","title":"Split the long fasta by chromosome"},{"location":"lab_session/human_genome/#index-the-fasta-files","text":"Indexing is widely used in bioinformatics workflows to improve performance. Typically it is applied to large data files with many records to improve the ability of tools to rapidly access specific locations of the file. For example, if we have alignments against the entire genome and we want to visualize those alignments for a single gene on chr17 in a genome viewer such as IGV, we don\u2019t want the viewer to have to scan through the entire file. Indexing allows us to jump right to the correct place in the file and pull out just the information we need without reading much of the file. # first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome ls -halt rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # What information does the index fail store? # google \"fasta index fai format\" or something similar # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. head ref_genome.dict # less can also be applied #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa","title":"Index the fasta files"},{"location":"lab_session/human_genome/#human-genome-transcriptome-reference-files","text":"# Make sure CHRS environment variable is set. echo $CHRS cd ~/workspace/inputs/references/transcriptome # List files in the directory ls -halt # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. # When the file is displayed usin \"less\" type \"/\" and then a something you want to highlight e.g. \"chr6\". This is very useful when searching for specific infomration less -SN ref_transcriptome.gtf","title":"Human genome transcriptome reference files"},{"location":"lab_session/human_genome/#explore-the-contents-of-the-transcriptome-reference-files","text":"#How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c Below we will prepare files for the bioinformatic tools (HISAT/Kallisto) used to analyse RNA-sequencing data.","title":"Explore the contents of the transcriptome reference files"},{"location":"lab_session/human_genome/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","text":"cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome"},{"location":"lab_session/human_genome/#create-a-reference-transcriptome-index-for-use-with-kallisto","text":"cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Create a reference transcriptome index for use with Kallisto"},{"location":"lab_session/intro_igv/","text":"Introduction to IGV \u00b6 For various resons there is often a need to manually inspect sequencing data. For this purpose, we will use IGV , the Integrative Genomics Viewer. IGV is in a nutshell an easy-to-use and interactive tool for the visual exploration of genomic data (not just sequening data). In IGV it is possible to integrate multiple data types in a straight forward manner. Download the IGV version that includes java. #First download IGV to your local computer. #Using a mac, execute the igv.sh bash script (obs - change to the correct path) /PATH_GOES_HERE/IGV_DIRECTORY/igv.sh #Go to your favourite directory and download a bam file for the IGV intro: scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/igv_intro_bams/* . HCC1143 cell line \u00b6 The cell line sequencincg data that will be used to explore the features of IGV comes from a 52 year old caucasian woman with breast cancer. Additional information can be found at the ATCC (American Type Culture Collection) website. The bam-file contains only reads on chromosome 21:19,000,000-20,000,000 in order to reduce file sizes. Getting familiar with IGV \u00b6 IGV comes pre-loaded with the genome build HG19. During this introduction we will use hg19 . Remote data files (tracks) can be loaded from the IGV server. Select \"File\" & \"Load from Server...\": Ensembl Genes GC Percentage dbSNP (latest version) Repeat masker Load the bam file Select \"File\" & \"Load from File...\": Locate \"HCC1143.normal.21.19M-20M.bam\" on the hard drive and press \"Open\". Familiarize the different sections of IGV \u00b6 The top genome ruler. The top panel data tracks. The mid panel sequencing data tracks. The bottom panel data- and annotation tracks. Note the gene model. line with arrows: the direction/strand of the gene the thin boxes: untranslated regions the thick boxes: coding exons Investigate read alignments \u00b6 Go to chr21:19,479,237-19,479,814. Right click on the aligned reads in the mid panel sequencing data tracks. Test to: Right click on a read and sort alignments by start location. group alignments by pair orientation. Experiment with the settings. Click on an individual read and assess the information. Enable the ruler, it simplifies investigating variants. Push the button to the upper right, see figure below: Investigate a SNP \u00b6 Go to chr21:19479237-19479814 Note a heterozygous variant at chr21:19479321. Zoom in to observe the individual base changes. Sort aglinment according to base. Color alignments according to strand. Try to answer the following: Reflect on mapping and base qualities, is this a high quality SNP? How does the shaded non-reference bases help? How does read strand information help? Investigate a homopolymer region with variability \u00b6 Go to chr21:19518412-19518497. Group alignments by read strand. Sort alignments by base. Try to answer the following: Is this a region with variability that can be trusted, motivate? Go to chr21:19518470 Sort alignments by base. Try to answer the following: Is this a region with variability that can be trusted, motivate? GC precentage and coverge \u00b6 Go to chr21:19611925-19631555. In the menue bar, go to View > Preferences > Alignments tab > set the visibility range threshold to 20 kb. Use Collapsed view for the read-pairs. Group alignments by \"none\". Sort alignments by start position. Color alignments by -> insert size and pair orientation. Is GC precentage and coverage correlated? SNPs on different alleles \u00b6 Go to chr21:19666833-19667007. Sort by base at position chr21:19666901 Try to answer the following: Are these valid SNPs? Explan what you are observing. Problematic region \u00b6 Load a the repeat masker remote data track. Select \"File\" & \"Load from Server...\": Repeat masker Go to chr21:19800320-19818162. Try to answer the following: Why are some reads white? Explan what you are observing. Deleted region \u00b6 Go to chr21:19324469-19331468. Select expanded view. View reads as pairs. Color alignments by insert size and pair orientation. Sort reads by insert size. Try to answer the following: Click on a red read pair and a grey read pair, can you identify information supporting the deletion in the red pairs? Is the deletion hetero- or homozygous? Mis-alignment \u00b6 Go to chr21:19102154-19103108. Group alignment by \"Chromosome of mate\". Zoom out in a stepwise manner and compare the region of interest with surrounding regions. Try to answer the following: Click on one of the brown pairs, where does the mate map? What is the reason for an increase in poorly mapped reads? Download the tumor and normal bam files processed today \u00b6 Go to your favourite directory # Tumor bam and bai files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.ba* ./ # Normal bam and bai files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.ba* . # The in solution hybridization based capture definition files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE::~/workspace/inputs/references/exome/probe_regions.bed ./ scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/references/exome/exome_regions.bed ./ Open the files in IGV \u00b6 Start a new IGV session Select File -> New Session Select hg38 Load the bam files Exome_Norm_sorted_mrkdup_bqsr.bam Exome_Tumor_sorted_mrkdup_bqsr.bam Load the cature definion files exome_regions.bed probe_regions.bed Change viewing preferences In the menue bar, go to View > Preferences > Alignments tab > set the visibility range threshold to 25 kb. Use Expanded view for the read-pairs. Group alignments by \u201cnone\u201d. Sort alignments by start position. Color alignments by -> no color. Type TP53 in the search box and press enter. Try to answer the following: Do you observe any variants? If yes, what kinds? Are they coding/non-coding? Any somatic variants? Try to assess the consequence of a selected variant. Why do you think there is a difference between the probe- and exome regions? Assess the coverge pattern relatively the probe regions. Try to explain the pattern.","title":"Introduction to IGV"},{"location":"lab_session/intro_igv/#introduction-to-igv","text":"For various resons there is often a need to manually inspect sequencing data. For this purpose, we will use IGV , the Integrative Genomics Viewer. IGV is in a nutshell an easy-to-use and interactive tool for the visual exploration of genomic data (not just sequening data). In IGV it is possible to integrate multiple data types in a straight forward manner. Download the IGV version that includes java. #First download IGV to your local computer. #Using a mac, execute the igv.sh bash script (obs - change to the correct path) /PATH_GOES_HERE/IGV_DIRECTORY/igv.sh #Go to your favourite directory and download a bam file for the IGV intro: scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/igv_intro_bams/* .","title":"Introduction to IGV"},{"location":"lab_session/intro_igv/#hcc1143-cell-line","text":"The cell line sequencincg data that will be used to explore the features of IGV comes from a 52 year old caucasian woman with breast cancer. Additional information can be found at the ATCC (American Type Culture Collection) website. The bam-file contains only reads on chromosome 21:19,000,000-20,000,000 in order to reduce file sizes.","title":"HCC1143 cell line"},{"location":"lab_session/intro_igv/#getting-familiar-with-igv","text":"IGV comes pre-loaded with the genome build HG19. During this introduction we will use hg19 . Remote data files (tracks) can be loaded from the IGV server. Select \"File\" & \"Load from Server...\": Ensembl Genes GC Percentage dbSNP (latest version) Repeat masker Load the bam file Select \"File\" & \"Load from File...\": Locate \"HCC1143.normal.21.19M-20M.bam\" on the hard drive and press \"Open\".","title":"Getting familiar with IGV"},{"location":"lab_session/intro_igv/#familiarize-the-different-sections-of-igv","text":"The top genome ruler. The top panel data tracks. The mid panel sequencing data tracks. The bottom panel data- and annotation tracks. Note the gene model. line with arrows: the direction/strand of the gene the thin boxes: untranslated regions the thick boxes: coding exons","title":"Familiarize the different sections of IGV"},{"location":"lab_session/intro_igv/#investigate-read-alignments","text":"Go to chr21:19,479,237-19,479,814. Right click on the aligned reads in the mid panel sequencing data tracks. Test to: Right click on a read and sort alignments by start location. group alignments by pair orientation. Experiment with the settings. Click on an individual read and assess the information. Enable the ruler, it simplifies investigating variants. Push the button to the upper right, see figure below:","title":"Investigate read alignments"},{"location":"lab_session/intro_igv/#investigate-a-snp","text":"Go to chr21:19479237-19479814 Note a heterozygous variant at chr21:19479321. Zoom in to observe the individual base changes. Sort aglinment according to base. Color alignments according to strand. Try to answer the following: Reflect on mapping and base qualities, is this a high quality SNP? How does the shaded non-reference bases help? How does read strand information help?","title":"Investigate a SNP"},{"location":"lab_session/intro_igv/#investigate-a-homopolymer-region-with-variability","text":"Go to chr21:19518412-19518497. Group alignments by read strand. Sort alignments by base. Try to answer the following: Is this a region with variability that can be trusted, motivate? Go to chr21:19518470 Sort alignments by base. Try to answer the following: Is this a region with variability that can be trusted, motivate?","title":"Investigate a homopolymer region with variability"},{"location":"lab_session/intro_igv/#gc-precentage-and-coverge","text":"Go to chr21:19611925-19631555. In the menue bar, go to View > Preferences > Alignments tab > set the visibility range threshold to 20 kb. Use Collapsed view for the read-pairs. Group alignments by \"none\". Sort alignments by start position. Color alignments by -> insert size and pair orientation. Is GC precentage and coverage correlated?","title":"GC precentage and coverge"},{"location":"lab_session/intro_igv/#snps-on-different-alleles","text":"Go to chr21:19666833-19667007. Sort by base at position chr21:19666901 Try to answer the following: Are these valid SNPs? Explan what you are observing.","title":"SNPs on different alleles"},{"location":"lab_session/intro_igv/#problematic-region","text":"Load a the repeat masker remote data track. Select \"File\" & \"Load from Server...\": Repeat masker Go to chr21:19800320-19818162. Try to answer the following: Why are some reads white? Explan what you are observing.","title":"Problematic region"},{"location":"lab_session/intro_igv/#deleted-region","text":"Go to chr21:19324469-19331468. Select expanded view. View reads as pairs. Color alignments by insert size and pair orientation. Sort reads by insert size. Try to answer the following: Click on a red read pair and a grey read pair, can you identify information supporting the deletion in the red pairs? Is the deletion hetero- or homozygous?","title":"Deleted region"},{"location":"lab_session/intro_igv/#mis-alignment","text":"Go to chr21:19102154-19103108. Group alignment by \"Chromosome of mate\". Zoom out in a stepwise manner and compare the region of interest with surrounding regions. Try to answer the following: Click on one of the brown pairs, where does the mate map? What is the reason for an increase in poorly mapped reads?","title":"Mis-alignment"},{"location":"lab_session/intro_igv/#download-the-tumor-and-normal-bam-files-processed-today","text":"Go to your favourite directory # Tumor bam and bai files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.ba* ./ # Normal bam and bai files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.ba* . # The in solution hybridization based capture definition files scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE::~/workspace/inputs/references/exome/probe_regions.bed ./ scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/references/exome/exome_regions.bed ./","title":"Download the tumor and normal bam files processed today"},{"location":"lab_session/intro_igv/#open-the-files-in-igv","text":"Start a new IGV session Select File -> New Session Select hg38 Load the bam files Exome_Norm_sorted_mrkdup_bqsr.bam Exome_Tumor_sorted_mrkdup_bqsr.bam Load the cature definion files exome_regions.bed probe_regions.bed Change viewing preferences In the menue bar, go to View > Preferences > Alignments tab > set the visibility range threshold to 25 kb. Use Expanded view for the read-pairs. Group alignments by \u201cnone\u201d. Sort alignments by start position. Color alignments by -> no color. Type TP53 in the search box and press enter. Try to answer the following: Do you observe any variants? If yes, what kinds? Are they coding/non-coding? Any somatic variants? Try to assess the consequence of a selected variant. Why do you think there is a difference between the probe- and exome regions? Assess the coverge pattern relatively the probe regions. Try to explain the pattern.","title":"Open the files in IGV"},{"location":"lab_session/processing_of_dna/","text":"Bioinformatic processing of DNA data \u00b6 cd ~/workspace/inputs/data/fastq # list all files tree # view the exome normal sample data files tree Exome_Norm # view the RNA-seq tumor sample data files. tree RNAseq_Tumor Investigate the raw data files (fastq-files) \u00b6 cd ~/workspace/inputs/data/fastq/Exome_Tumor # show the first ten lines of the Exome Tumor fastq files # note this is how paird-end data is stored. # Read 1 zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head # Read 2 zcat Exome_Tumor/Exome_Tumor_R2.fastq.gz | head # This wikipedia file gives a very nice overview of fastq-files and Illumina base qualities # https://en.wikipedia.org/wiki/FASTQ_format zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 8 # what do R1 and R2 refer to? What is the length of each read? zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 2 | tail -n 1 | wc # how many lines are there in the Exome_Tumor file zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | wc -l # There are: 33,326,620 # how many paired reads or fragments are there then? expr 33326620 / 4 # There are: 8,331,655 paired end reads # how many total bases of data are in the Exome Tumor data set? echo \"8331655 * (101 * 2)\" | bc # There are: 1,682,994,310 bases of data # how many total bases when expressed as \"gigabases\" (specify 2 decimal points using `scale`) echo \"scale=2; (8331655 * (101 * 2))/1000000000\" | bc # There are: 1.68 Gbp of data # what is the average coverage we expect to achieve with this much data for the exome region targeted? # first determine the size of our exome regions (answer = 6683920). cat /workspace/inputs/references/exome/exome_regions.bed | perl -ne 'chomp; @l=split(\"\\t\", $_); $size += $l[2]-$l[1]; if (eof){print \"size = $size\\n\"}' # now determine the average coverage of these positions by our bases of data echo \"scale=2; (8331655 * (101 * 2))/6683920\" | bc # Average covered expected = 251.79x # what is the fundamental assumption of this calculation that is at least partially not true? What effect will this have on the observed coverage? Run fastqc to check the quality of the fastq-files \u00b6 Have a look here for a short tutorial on the fastqc output. cd ~/workspace/inputs/data/fastq fastqc Exome_Norm/Exome_Norm*.fastq.gz fastqc Exome_Tumor/Exome_Tumor*.fastq.gz tree fastqc RNAseq_Norm/RNAseq_Norm*.fastq.gz fastqc RNAseq_Tumor/RNAseq_Tumor*.fastq.gz tree Run MultiQC \u00b6 We will run mutliQC to provide a combined report of the fastQC data. More info on MultiQC is available here . cd ~/workspace/inputs #Dont do if directory exist #mkdir qc cd qc multiqc ~/workspace/inputs/data/fastq/ tree # Download MultiQC output to the local computer, open the .html in you favourite browser. scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/qc/multiqc* . Run BWA \u00b6 # Run bwa mem using the above information cd ~/workspace/align # Runtime: ~ 4min bwa mem -t 2 -Y -R \"@RG\\tID:Exome_Norm\\tPL:ILLUMINA\\tPU:C1TD1ACXX-CGATGT.7\\tLB:exome_norm_lib1\\tSM:HCC1395BL_DNA\" -o ~/workspace/align/Exome_Norm.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R2.fastq.gz # Runtime: ~ 4min bwa mem -t 2 -Y -R \"@RG\\tID:Exome_Tumor\\tPL:ILLUMINA\\tPU:C1TD1ACXX-ATCACG.7\\tLB:exome_tumor_lib1\\tSM:HCC1395_DNA\" -o ~/workspace/align/Exome_Tumor.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R2.fastq.gz Convert sam to bam format \u00b6 cd ~/workspace/align # Runtime: ~30s samtools view -@ 2 -h -b -o Exome_Norm.bam Exome_Norm.sam # Runtime: ~45s samtools view -@ 2 -h -b -o Exome_Tumor.bam Exome_Tumor.sam # Have a look at the bam header and try to figure out what type of information that is in the bam file: samtools view -H Exome_Tumor.bam | less -SN # Have a look at the bam file data and try to figure out what type of information that is in the bam file: samtools view Exome_Tumor.bam | less -SN Detailed information on the sam format can be found HERE . Sort bam files \u00b6 cd ~/workspace/align # Runtime: ~ 4min java -Xmx12g -jar $PICARD SortSam I = Exome_Norm.bam O = Exome_Norm_namesorted.bam SO = queryname java -Xmx12g -jar $PICARD SortSam I = Exome_Tumor.bam O = Exome_Tumor_namesorted.bam SO = queryname Mark duplicates \u00b6 During library preparation, PCR is performed. Removing the PCR duplicates is necessary for substantial removal of PCR artefacts. Picard MarkDuplicates use the read names and alignment positions to identify PCR duplicates. cd ~/workspace/align java -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Norm_namesorted.bam -O Exome_Norm_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Norm_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT java -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Tumor_namesorted.bam -O Exome_Tumor_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Tumor_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT Position sort bam file \u00b6 For indexing and subsequent steps a position-sorted bam is required. Therefore, we will sort bam file by coordinate. cd ~/workspace/align java -Xmx12g -jar $PICARD SortSam -I Exome_Norm_namesorted_mrkdup.bam -O Exome_Norm_sorted_mrkdup.bam -SO coordinate java -Xmx12g -jar $PICARD SortSam -I Exome_Tumor_namesorted_mrkdup.bam -O Exome_Tumor_sorted_mrkdup.bam -SO coordinate Create bam index for use with GATK, IGV, etc \u00b6 In order to efficiently load and search a bam file, downstream applications typically require an index cd ~/workspace/align java -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Norm_sorted_mrkdup.bam java -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Tumor_sorted_mrkdup.bam Indel Realignment \u00b6 Depending on the downstream appoarch something called Indel realignment may be needed. During alignment of the DNA data, each individual read is aligned to the reference sequence individually. This may lead to misaligned reads in e.g. repetitive regions or due to somatic/germline variation. If needed, add the realignment-step here, see docs here . In this course the HaplotypeCaller/MuTect2 will be applied to identify germline/somatic variantion. These two variant callers do realignment internally and therefore it is not needed. See the following blog for a detailed discussion of this issue. See here for latest versions of all GATK tutorials. Perform Base Quality Score Recalibration \u00b6 BQSR stands for Base Quality Score Recalibration. In a nutshell, it is a data pre-processing step that detects systematic errors made by the sequencing machine when it estimates the accuracy of each base call. Calculate BQSR Table \u00b6 First, calculate the BQSR table. NOTE: For exome data, we should modify the below to use the --intervals ( -L ) option. This excludes off-target sequences and sequences that may be poorly mapped, which have a higher error rate. Including them could lead to a skewed model and bad recalibration. cd ~/workspace/align gatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS gatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS Apply BQSR \u00b6 Now, apply the BQSR table to bam files. cd ~/workspace/align gatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 gatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 Create an index for these new bams cd ~/workspace/align java -Xmx12g -jar $PICARD BuildBamIndex I = Exome_Norm_sorted_mrkdup_bqsr.bam java -Xmx12g -jar $PICARD BuildBamIndex I = Exome_Tumor_sorted_mrkdup_bqsr.bam Clean up un-needed sam/bam files \u00b6 Keep final sorted, duplicated marked, bqsr bam/bai/table files and mrkdup.txt files. Delete everything else. cd ~/workspace/align mkdir final mv *_sorted_mrkdup_bqsr.* final/ mv *.txt final/ rm *.sam rm *.bam rm *.bai mv final/* . rmdir final/ Run Samtools flagstat \u00b6 Samtools flagstat provides QC-metrics after aligning the reads, e.g. the fraction of read-pairs that map to different chromosomes (which should be limited). cd ~/workspace/align/ samtools flagstat ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam > ~/workspace/align/Exome_Norm_flagstat.txt samtools flagstat ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam > ~/workspace/align/Exome_Tumor_flagstat.txt Run various picard CollectMetrics tools \u00b6 Picard is a widely used QC tool in genomics and it is named after a famous Star Trek Captain . Description of the output of picard can be found here . cd ~/workspace/align/ java -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_insert_size_metrics.txt -H ~/workspace/align/Exome_Norm_insert_size_metrics.pdf java -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_insert_size_metrics.txt -H ~/workspace/align/Exome_Tumor_insert_size_metrics.pdf java -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa java -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_exome_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa #Picard CollectHsMetrics #Exome Only java -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list java -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list Run FastQC \u00b6 cd ~/workspace/align fastqc -t 8 Exome_Norm_sorted_mrkdup_bqsr.bam fastqc -t 8 Exome_Tumor_sorted_mrkdup_bqsr.bam tree Run MultiQC to produce a final report \u00b6 cd ~/workspace/align #Dont do if directory exist #mkdir post_align_qc cd post_align_qc multiqc ~/workspace/align/ tree # Download MultiQC output to the local computer, open the .html in you favourite browser. #Discuss the MultiQC output with a fellow student scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/post_align_qc/multiqc* .","title":"Processing of DNA sequencing data"},{"location":"lab_session/processing_of_dna/#bioinformatic-processing-of-dna-data","text":"cd ~/workspace/inputs/data/fastq # list all files tree # view the exome normal sample data files tree Exome_Norm # view the RNA-seq tumor sample data files. tree RNAseq_Tumor","title":"Bioinformatic processing of DNA data"},{"location":"lab_session/processing_of_dna/#investigate-the-raw-data-files-fastq-files","text":"cd ~/workspace/inputs/data/fastq/Exome_Tumor # show the first ten lines of the Exome Tumor fastq files # note this is how paird-end data is stored. # Read 1 zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head # Read 2 zcat Exome_Tumor/Exome_Tumor_R2.fastq.gz | head # This wikipedia file gives a very nice overview of fastq-files and Illumina base qualities # https://en.wikipedia.org/wiki/FASTQ_format zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 8 # what do R1 and R2 refer to? What is the length of each read? zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | head -n 2 | tail -n 1 | wc # how many lines are there in the Exome_Tumor file zcat Exome_Tumor/Exome_Tumor_R1.fastq.gz | wc -l # There are: 33,326,620 # how many paired reads or fragments are there then? expr 33326620 / 4 # There are: 8,331,655 paired end reads # how many total bases of data are in the Exome Tumor data set? echo \"8331655 * (101 * 2)\" | bc # There are: 1,682,994,310 bases of data # how many total bases when expressed as \"gigabases\" (specify 2 decimal points using `scale`) echo \"scale=2; (8331655 * (101 * 2))/1000000000\" | bc # There are: 1.68 Gbp of data # what is the average coverage we expect to achieve with this much data for the exome region targeted? # first determine the size of our exome regions (answer = 6683920). cat /workspace/inputs/references/exome/exome_regions.bed | perl -ne 'chomp; @l=split(\"\\t\", $_); $size += $l[2]-$l[1]; if (eof){print \"size = $size\\n\"}' # now determine the average coverage of these positions by our bases of data echo \"scale=2; (8331655 * (101 * 2))/6683920\" | bc # Average covered expected = 251.79x # what is the fundamental assumption of this calculation that is at least partially not true? What effect will this have on the observed coverage?","title":"Investigate the raw data files (fastq-files)"},{"location":"lab_session/processing_of_dna/#run-fastqc-to-check-the-quality-of-the-fastq-files","text":"Have a look here for a short tutorial on the fastqc output. cd ~/workspace/inputs/data/fastq fastqc Exome_Norm/Exome_Norm*.fastq.gz fastqc Exome_Tumor/Exome_Tumor*.fastq.gz tree fastqc RNAseq_Norm/RNAseq_Norm*.fastq.gz fastqc RNAseq_Tumor/RNAseq_Tumor*.fastq.gz tree","title":"Run fastqc to check the quality of the fastq-files"},{"location":"lab_session/processing_of_dna/#run-multiqc","text":"We will run mutliQC to provide a combined report of the fastQC data. More info on MultiQC is available here . cd ~/workspace/inputs #Dont do if directory exist #mkdir qc cd qc multiqc ~/workspace/inputs/data/fastq/ tree # Download MultiQC output to the local computer, open the .html in you favourite browser. scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/qc/multiqc* .","title":"Run MultiQC"},{"location":"lab_session/processing_of_dna/#run-bwa","text":"# Run bwa mem using the above information cd ~/workspace/align # Runtime: ~ 4min bwa mem -t 2 -Y -R \"@RG\\tID:Exome_Norm\\tPL:ILLUMINA\\tPU:C1TD1ACXX-CGATGT.7\\tLB:exome_norm_lib1\\tSM:HCC1395BL_DNA\" -o ~/workspace/align/Exome_Norm.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Norm/Exome_Norm_R2.fastq.gz # Runtime: ~ 4min bwa mem -t 2 -Y -R \"@RG\\tID:Exome_Tumor\\tPL:ILLUMINA\\tPU:C1TD1ACXX-ATCACG.7\\tLB:exome_tumor_lib1\\tSM:HCC1395_DNA\" -o ~/workspace/align/Exome_Tumor.sam ~/workspace/inputs/references/genome/ref_genome.fa ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/Exome_Tumor/Exome_Tumor_R2.fastq.gz","title":"Run BWA"},{"location":"lab_session/processing_of_dna/#convert-sam-to-bam-format","text":"cd ~/workspace/align # Runtime: ~30s samtools view -@ 2 -h -b -o Exome_Norm.bam Exome_Norm.sam # Runtime: ~45s samtools view -@ 2 -h -b -o Exome_Tumor.bam Exome_Tumor.sam # Have a look at the bam header and try to figure out what type of information that is in the bam file: samtools view -H Exome_Tumor.bam | less -SN # Have a look at the bam file data and try to figure out what type of information that is in the bam file: samtools view Exome_Tumor.bam | less -SN Detailed information on the sam format can be found HERE .","title":"Convert sam to bam format"},{"location":"lab_session/processing_of_dna/#sort-bam-files","text":"cd ~/workspace/align # Runtime: ~ 4min java -Xmx12g -jar $PICARD SortSam I = Exome_Norm.bam O = Exome_Norm_namesorted.bam SO = queryname java -Xmx12g -jar $PICARD SortSam I = Exome_Tumor.bam O = Exome_Tumor_namesorted.bam SO = queryname","title":"Sort bam files"},{"location":"lab_session/processing_of_dna/#mark-duplicates","text":"During library preparation, PCR is performed. Removing the PCR duplicates is necessary for substantial removal of PCR artefacts. Picard MarkDuplicates use the read names and alignment positions to identify PCR duplicates. cd ~/workspace/align java -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Norm_namesorted.bam -O Exome_Norm_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Norm_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT java -Xmx12g -jar $PICARD MarkDuplicates -I Exome_Tumor_namesorted.bam -O Exome_Tumor_namesorted_mrkdup.bam -ASSUME_SORT_ORDER queryname -METRICS_FILE Exome_Tumor_mrkdup_metrics.txt -QUIET true -COMPRESSION_LEVEL 0 -VALIDATION_STRINGENCY LENIENT","title":"Mark duplicates"},{"location":"lab_session/processing_of_dna/#position-sort-bam-file","text":"For indexing and subsequent steps a position-sorted bam is required. Therefore, we will sort bam file by coordinate. cd ~/workspace/align java -Xmx12g -jar $PICARD SortSam -I Exome_Norm_namesorted_mrkdup.bam -O Exome_Norm_sorted_mrkdup.bam -SO coordinate java -Xmx12g -jar $PICARD SortSam -I Exome_Tumor_namesorted_mrkdup.bam -O Exome_Tumor_sorted_mrkdup.bam -SO coordinate","title":"Position sort bam file"},{"location":"lab_session/processing_of_dna/#create-bam-index-for-use-with-gatk-igv-etc","text":"In order to efficiently load and search a bam file, downstream applications typically require an index cd ~/workspace/align java -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Norm_sorted_mrkdup.bam java -Xmx12g -jar $PICARD BuildBamIndex -I Exome_Tumor_sorted_mrkdup.bam","title":"Create bam index for use with GATK, IGV, etc"},{"location":"lab_session/processing_of_dna/#indel-realignment","text":"Depending on the downstream appoarch something called Indel realignment may be needed. During alignment of the DNA data, each individual read is aligned to the reference sequence individually. This may lead to misaligned reads in e.g. repetitive regions or due to somatic/germline variation. If needed, add the realignment-step here, see docs here . In this course the HaplotypeCaller/MuTect2 will be applied to identify germline/somatic variantion. These two variant callers do realignment internally and therefore it is not needed. See the following blog for a detailed discussion of this issue. See here for latest versions of all GATK tutorials.","title":"Indel Realignment"},{"location":"lab_session/processing_of_dna/#perform-base-quality-score-recalibration","text":"BQSR stands for Base Quality Score Recalibration. In a nutshell, it is a data pre-processing step that detects systematic errors made by the sequencing machine when it estimates the accuracy of each base call.","title":"Perform Base Quality Score Recalibration"},{"location":"lab_session/processing_of_dna/#calculate-bqsr-table","text":"First, calculate the BQSR table. NOTE: For exome data, we should modify the below to use the --intervals ( -L ) option. This excludes off-target sequences and sequences that may be poorly mapped, which have a higher error rate. Including them could lead to a skewed model and bad recalibration. cd ~/workspace/align gatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS gatk --java-options '-Xmx12g' BaseRecalibrator -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz --known-sites ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --preserve-qscores-less-than 6 --disable-bam-index-caching $GATK_REGIONS","title":"Calculate BQSR Table"},{"location":"lab_session/processing_of_dna/#apply-bqsr","text":"Now, apply the BQSR table to bam files. cd ~/workspace/align gatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Norm_sorted_mrkdup.bam -O ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 gatk --java-options '-Xmx12g' ApplyBQSR -R ~/workspace/inputs/references/genome/ref_genome.fa -I ~/workspace/align/Exome_Tumor_sorted_mrkdup.bam -O ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam --bqsr-recal-file ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.table --preserve-qscores-less-than 6 --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 Create an index for these new bams cd ~/workspace/align java -Xmx12g -jar $PICARD BuildBamIndex I = Exome_Norm_sorted_mrkdup_bqsr.bam java -Xmx12g -jar $PICARD BuildBamIndex I = Exome_Tumor_sorted_mrkdup_bqsr.bam","title":"Apply BQSR"},{"location":"lab_session/processing_of_dna/#clean-up-un-needed-sambam-files","text":"Keep final sorted, duplicated marked, bqsr bam/bai/table files and mrkdup.txt files. Delete everything else. cd ~/workspace/align mkdir final mv *_sorted_mrkdup_bqsr.* final/ mv *.txt final/ rm *.sam rm *.bam rm *.bai mv final/* . rmdir final/","title":"Clean up un-needed sam/bam files"},{"location":"lab_session/processing_of_dna/#run-samtools-flagstat","text":"Samtools flagstat provides QC-metrics after aligning the reads, e.g. the fraction of read-pairs that map to different chromosomes (which should be limited). cd ~/workspace/align/ samtools flagstat ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam > ~/workspace/align/Exome_Norm_flagstat.txt samtools flagstat ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam > ~/workspace/align/Exome_Tumor_flagstat.txt","title":"Run Samtools flagstat"},{"location":"lab_session/processing_of_dna/#run-various-picard-collectmetrics-tools","text":"Picard is a widely used QC tool in genomics and it is named after a famous Star Trek Captain . Description of the output of picard can be found here . cd ~/workspace/align/ java -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_insert_size_metrics.txt -H ~/workspace/align/Exome_Norm_insert_size_metrics.pdf java -Xmx12g -jar $PICARD CollectInsertSizeMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_insert_size_metrics.txt -H ~/workspace/align/Exome_Tumor_insert_size_metrics.pdf java -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa java -Xmx12g -jar $PICARD CollectAlignmentSummaryMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_exome_alignment_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa #Picard CollectHsMetrics #Exome Only java -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Norm_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list java -Xmx12g -jar $PICARD CollectHsMetrics -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam -O ~/workspace/align/Exome_Tumor_hs_metrics.txt -R ~/workspace/inputs/references/genome/ref_genome.fa -BI ~/workspace/inputs/references/exome/probe_regions.bed.interval_list -TI ~/workspace/inputs/references/exome/exome_regions.bed.interval_list","title":"Run various picard CollectMetrics tools"},{"location":"lab_session/processing_of_dna/#run-fastqc","text":"cd ~/workspace/align fastqc -t 8 Exome_Norm_sorted_mrkdup_bqsr.bam fastqc -t 8 Exome_Tumor_sorted_mrkdup_bqsr.bam tree","title":"Run FastQC"},{"location":"lab_session/processing_of_dna/#run-multiqc-to-produce-a-final-report","text":"cd ~/workspace/align #Dont do if directory exist #mkdir post_align_qc cd post_align_qc multiqc ~/workspace/align/ tree # Download MultiQC output to the local computer, open the .html in you favourite browser. #Discuss the MultiQC output with a fellow student scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/post_align_qc/multiqc* .","title":"Run MultiQC to produce a final report"},{"location":"lab_session/processing_of_rnaseq/","text":"BIOINFORMATIC PROCESSING OF RNA DATA \u00b6 Adapter Trimming of the FASTQ files \u00b6 The purpose of adapter trimming is to remove sequences in our data that correspond to the Illumina sequence adapters. The most common adapter trimming scenario is the removal of adapter sequences that occur at the end of read sequences. This happens when a DNA (or cDNA) fragment is shorter than the read length. For example if we sequence our RNA-seq fragments to 150 base length and a fragment is only 140 bases long the read will end with 10 bases of adapter sequence. Since this adapter sequence does not correspond to the genome, it will not align. Too much adapter sequence can actually prevent reads from aligning at all. Adapter trimming may therefore sometime improve the overall alignment success rate for an RNA-seq data set. Adapter trimming involves a simplistic alignment itself and therefore can be computationally expensive. cd ~/workspace/inputs/references #Command already run #wget -c http://genomedata.org/rnaseq-tutorial/illumina_multiplex.fa #Have a look at the adapter sequences that we want to remove from the RNA data less -SN ~/workspace/inputs/references/illumina_multiplex.fa #Start with tumor RNAseq data cd ~/workspace/inputs/data/fastq/RNAseq_Tumor #Have a look to see what is in the folder ls -halt #pPerform trimming using flexbar #Before using a new tool, make a habit of looking the tool up, # google \"flexbar trimming adapters\" or something similar #As this step takes approxmately 25 min, we will use nohup. #Google \"nohup\" ... #If running with nohup the command will continue even if the connection breaks to the aws server #Also, notice that we are using the flexbar argument --threads 7 which means that we are using 7 cpu threads. To check how many you have at your disposal use the command: lscpu #We have 2 thread left which is good, not to hog all resources #To check the available RAM use the command: cat /proc/meminfo #When trimming the adapters run two simultaneous jobs using 7 cores/job and send both to the background if connection breaks. Redirect the standard output to a custom log file #nohup cmd > custom-out.log & nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane1_R1.fastq.gz --reads2 RNAseq_Tumor_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1 > RNAseq_Tumor_Lane1.log & #Start the second process nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane2_R1.fastq.gz --reads2 RNAseq_Tumor_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2 > RNAseq_Tumor_Lane2.log & #Use htop to monitor the processor load htop #quit htop by pressing \"q\" #have a look at one of the log files less -SN RNAseq_Tumor_Lane1.log ####### Wait for the processes to stop before continuing ####### #Start the #Normal RNAseq data cd ~/workspace/inputs/data/fastq/RNAseq_Norm nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane1_R1.fastq.gz --reads2 RNAseq_Norm_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1 > RNAseq_Normal_Lane1.log & flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane2_R1.fastq.gz --reads2 RNAseq_Norm_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2 > RNAseq_Normal_Lane2.log & ####### Wait for the processes to stop ####### As you can see, nohup is very useful for sending processes to the backgroud that will continue if connection breaks or if you need to log out. For repetition how to send stdout and stderr to a file while running nohup, have a look here . Run fastqc to check the quality of the fastq-files (as for DNA data) \u00b6 #Run fastqc and compre the trimmed and non-trimmed file cd ~/workspace/inputs/data/fastq/RNAseq_Tumor fastqc RNAseq_Tumor_Lane1_R1.fastq.gz fastqc RNAseq_Tumor_Lane1_1.fastq.gz # Download the output to your computer scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1_fastqc.html . scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1_fastqc.html . Compare the two html files side by side: - Any obivous difference? #Move the trimmed data mkdir -p ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed mkdir -p ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz #make a habit of always checking the result of any command ls -halt ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed ls -halt ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed Alignment \u00b6 We will use the aligner HISAT2 to perform spliced alignments to our reference genome. For efficiency, the output of HISAT (SAM format) will be piped directly to another program called sambamba to first convert to BAM format and then sort that BAM file. Before each command we will also create and assign a path for temporary directories. mkdir -p ~/workspace/rnaseq/alignments cd ~/workspace/rnaseq/alignments # Align tumor data # Runtime: ~15 min each run TUMOR_DATA_1_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626107.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626107 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.4 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam /dev/stdin & TUMOR_DATA_2_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626112.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626112 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.5 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam /dev/stdin & #Wait for the processes to finish, monitor using htop rmdir $TUMOR_DATA_2_TEMP $TUMOR_DATA_1_TEMP # Align normal data NORMAL_DATA_1_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895625992.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895625992 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.4 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam /dev/stdin & NORMAL_DATA_2_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626097.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626097 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.5 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam /dev/stdin & #Wait for the processes to finish, monitor using htop rmdir $NORMAL_DATA_2_TEMP $NORMAL_DATA_1_TEMP Merging BAMs \u00b6 Since we have multiple BAMs of each sample that just represent additional data for the same sequence library, we should combine them into a single BAM for convenience before proceeding. cd ~/workspace/rnaseq/alignments #Runtime: ~ 8m each merging command sambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam sambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam GTF (General Transfer Format) and creating files for downstream processing \u00b6 We will encounter the GTF file format during the exercises below, described in detail here . The GTF format is use to describe genes and other features of DNA, RNA and proteins. In the section below we will prepare files needed for RNAseq QC NOTE: for the sake of time we are only investigating genes on chr6 and chr17 #Go to this directory: cd ~/workspace/inputs/references/transcriptome #Have a peak into the transcriptome gtf file and try to make sense of it using the ensembl description above less -SN ref_transcriptome.gtf #check on which chromsomes from which we have transcripts cut -f1 ref_transcriptome.gtf | sort | uniq -c # Generating the necessary input files for picard CollectRnaSeqMetrics cd ~/workspace/inputs/references/transcriptome grep -i rrna ref_transcriptome.gtf > ref_ribosome.gtf #A bed file with Ribosomal RNA gff2bed < ~/workspace/inputs/references/transcriptome/ref_ribosome.gtf > ref_ribosome.bed #Check the ribosome bed file less -SN ref_ribosome.bed java -jar $PICARD BedToIntervalList I = ~/workspace/inputs/references/transcriptome/ref_ribosome.bed O = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict gtfToGenePred -genePredExt ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf ~/workspace/inputs/references/transcriptome/ref_flat.txt cat ref_flat.txt | awk '{print $12\"\\t\"$0}' | cut -d $'\\t' -f1-11 > ref_flat_final.txt mv ref_flat_final.txt ref_flat.txt Post-alignment QC \u00b6 cd ~/workspace/rnaseq/alignments #Run samtools flagstat #Runtime: ~2min, start and send to background nohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Norm.bam > ~/workspace/rnaseq/alignments/RNAseq_Norm_flagstat.txt 2 > flagstat_RNA_N_bam.out & nohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam > ~/workspace/rnaseq/alignments/RNAseq_Tumor_flagstat.txt 2 > flagstat_RNA_T_bam.out & #Runtime: ~12 min, start and send to background nohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam > fastqc_RNA_T_bam.out 2 > & 1 & nohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam > fastqc_RNA_N_bam.out 2 > & 1 & # Runtime: 26min nohup java -jar $PICARD CollectRnaSeqMetrics I = ~/workspace/rnaseq/alignments/RNAseq_Norm.bam O = ~/workspace/rnaseq/alignments/RNAseq_Norm.RNA_Metrics REF_FLAT = ~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND = SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list > collectRnaSeqMetrics_N.out 2 > & 1 & #Follow the progress of the program by looking in the nohup output file less -SN collectRnaSeqMetrics_N.out nohup java -jar $PICARD CollectRnaSeqMetrics I = ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam O = ~/workspace/rnaseq/alignments/RNAseq_Tumor.RNA_Metrics REF_FLAT = ~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND = SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list > collectRnaSeqMetrics_T.out 2 > & 1 & cd ~/workspace/rnaseq #mkdir post_align_qc cd post_align_qc multiqc ~/workspace/rnaseq/alignments/ #Finally, download multiqc files to your local computer # Download MultiQC output to the local computer, open the .html in you favourite browser. scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/rnaseq/post_align_qc/multiqc* . Familiarize yourself with the RNAseq MultiQC data Indexing BAMs \u00b6 In order to be able to view our BAM files in IGV, as usual we need to index them cd ~/workspace/rnaseq/alignments/ samtools index RNAseq_Norm.bam samtools index RNAseq_Tumor.bam Run a simplified \"reference only\" StringTie expression approach \u00b6 The StringTie developer's recommend to: perform reference guided transcript compilation (aka transcript assembly) on each individual sample. merge transcript predictions from all samples into a single model of the transcriptome. annotate this predicted transcriptome with known transcriptome information. estimate abundance for each of the transcripts in this final transcriptome model in each sample. In the final result we would have abundance for all the same transcripts across all samples. This includes a combination of predicted and known transcripts. It is sometimes convenient to have a more simplified workflow where we only have values for known transcripts. This is particularly true in species where we already have comprehensive high quality transcriptome annotations and there is less of a focus on de novo transcript discovery. The following workflow produces a \"reference-only\" transcriptome result in which we will perform abundance calculations on each lane of data individually. cd ~/workspace/rnaseq #mkdir ref-only-expression cd ref-only-expression nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam > knowntranscripts_T1_L1.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam > knowntranscripts_T1_L2.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam > knowntranscripts_N1_L1.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam > knowntranscripts_T2_L2.out 2 > & 1 & #Wait to continue submitting jobs until the scripts have finished Create a tidy expression matrix files for the StringTie results. This will be done at both the gene and transcript level and also will take into account the various expression measures produced: coverage, FPKM, and TPM. cd ~/workspace/rnaseq/ref-only-expression #Already run #wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/stringtie_expression_matrix.pl #chmod +x stringtie_expression_matrix.pl ./stringtie_expression_matrix.pl --expression_metric = TPM --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_tpm_all_samples.tsv --gene_matrix_file = gene_tpm_all_samples.tsv ./stringtie_expression_matrix.pl --expression_metric = FPKM --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_fpkm_all_samples.tsv --gene_matrix_file = gene_fpkm_all_samples.tsv ./stringtie_expression_matrix.pl --expression_metric = Coverage --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_coverage_all_samples.tsv --gene_matrix_file = gene_coverage_all_samples.tsv #Have a look at the output files head gene_coverage_all_samples.tsv transcript_coverage_all_samples.tsv gene_fpkm_all_samples.tsv transcript_fpkm_all_samples.tsv gene_tpm_all_samples.tsv transcript_tpm_all_samples.tsv Reference Free Expression Analysis with Kallisto \u00b6 Remember that in previous sections we have been using reference genome fasta sequences for the reference for alignment and subsequent steps. However, Kallisto works directly on target cDNA/transcript sequences. We have for stringtie used transcript annotations for genes on our subset of chromosomes (i.e. chr6 and chr17). The transcript models were downloaded from Ensembl in GTF format. This GTF contains a description of the coordinates of exons that make up each transcript but it does not contain the transcript sequences themselves which Kallisto is using. There are many places we could obtain such transcript sequences. For example, we could have download them directly in Fasta format from the Ensembl FTP site (or from UCSC or NCBI). cd ~/workspace/rnaseq/ #mkdir kallisto cd kallisto # first check that the GTF and Fasta file are present for Kallisto head ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf head ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa #Each transcript and its sequence head ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa # now check for the kallisto index is there ls -halt ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index # create a list of all transcript IDs for later use: cd ~/workspace/rnaseq/kallisto/ cat ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa | grep \">\" | perl -ne '$_ =~ s/\\>//; print $_' | sort | uniq > transcript_id_list.txt head -n 10 transcript_id_list.txt Generate abundance estimates for all samples using Kallisto \u00b6 As we did with StringTie we will generate transcript abundances for each of our demonstration samples using Kallisto. Here we are treating the two lanes for each sample as if they were independent samples. cd ~/workspace/rnaseq/kallisto/ #mkdir quants cd quants nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Norm_Lane1 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz > kallisto_quant_N_L1.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Norm_Lane2 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz > kallisto_quant_N_L2.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Tumor_Lane1 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz > kallisto_quant_T_L1.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Tumor_Lane2 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz > kallisto_quant_T_L2.out 2 > & 1 & #check nohup output less -SN kallisto_quant_T_L2.out #Create a single TSV file that has the TPM abundance estimates for all six samples. #First check the contets of an abudance output file from Kallisto less -SN ./RNAseq_Norm_Lane1/abundance.tsv #Merge all files paste */abundance.tsv | cut -f 1 ,2,5,10,15,20 > transcript_tpms_all_samples.tsv ls -1 */abundance.tsv | perl -ne 'chomp $_; if ($_ =~ /(\\S+)\\/abundance\\.tsv/){print \"\\t$1\"}' | perl -ne 'print \"target_id\\tlength$_\\n\"' > header.tsv cat header.tsv transcript_tpms_all_samples.tsv | grep -v \"tpm\" > transcript_tpms_all_samples.tsv2 mv transcript_tpms_all_samples.tsv2 transcript_tpms_all_samples.tsv rm -f header.tsv head transcript_tpms_all_samples.tsv #First create a gene version of the Kallisto TPM matrix, we will simply sum the TPM values for transcripts of the same gene. # Command already run #wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/kallisto_gene_matrix.pl #chmod +x kallisto_gene_matrix.pl ./kallisto_gene_matrix.pl --gtf_file = /home/ubuntu/workspace/inputs/references/transcriptome/ref_transcriptome.gtf --kallisto_transcript_matrix_in = transcript_tpms_all_samples.tsv --kallisto_transcript_matrix_out = gene_tpms_all_samples.tsv less -SN gene_tpms_all_samples.tsv Compare expression values between Kallisto and StringTie \u00b6 To compare the two approaches we can use the expression value for each Ensembl transcript. To do this comparison, we need to gather the expression estimates for each of our replicates from each approach. First - download the files to your local machine #Run these commands on your local machine scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/kallisto/transcript_tpms_all_samples.tsv . mv transcript_tpms_all_samples.tsv kallisto_transcript_tpms_all_samples.tsv scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/ref-only-expression/transcript_tpm_all_samples.tsv . mv transcript_tpm_all_samples.tsv stringtie_transcript_tpms_all_samples.tsv #Remember, one gene can have many transcripts. The ensembl gene ID for TP53 is ENSG00000141510. Check which ensembl transcripts that exist in the reference gtf. #RUN ON THE AWS SERVER grep ENST ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf | grep ENSG00000141510 | perl -nle '@a = split /\\t/; $_ =~ /transcript_id\\s\"(\\S*)\";/g; print $1;' | sort | uniq # start R # set the working directory to where the files were downloaded setwd ( \"DIR GOES HERE\" ) # load libraries library ( ggplot2 ) library ( reshape2 ) # read in data kallisto_transcript_tpm <- read.delim ( \"kallisto_transcript_tpms_all_samples.tsv\" ) head ( kallisto_transcript_tpm ) stringtie_transcript_tpm <- read.delim ( \"stringtie_transcript_tpms_all_samples.tsv\" ) head ( stringtie_transcript_tpm ) # minor reformatting kallisto_transcript_tpm <- kallisto_transcript_tpm [ ,-2 ] kallisto_transcript_tpm <- melt ( kallisto_transcript_tpm, id.vars = c ( \"target_id\" )) head ( kallisto_transcript_tpm ) stringtie_transcript_tpm <- melt ( stringtie_transcript_tpm, id.vars = c ( \"Transcript_ID\" )) head ( stringtie_transcript_tpm ) # merge the data kallisto_stringtie_tpm <- merge ( kallisto_transcript_tpm, stringtie_transcript_tpm, by.x = c ( \"target_id\" , \"variable\" ) , by.y = c ( \"Transcript_ID\" , \"variable\" ) , suffixes = c ( \".kallisto\" , \".stringtie\" )) #The numeric vector is read as a character vector, correcting .. kallisto_stringtie_tpm $value .stringtie = as.numeric ( kallisto_stringtie_tpm $value .stringtie ) str ( kallisto_stringtie_tpm ) #TP53 transcript ID vector from the transcript GTF TP53 = c ( \"ENST00000269305\" , \"ENST00000359597\" , \"ENST00000413465\" , \"ENST00000420246\" , \"ENST00000445888\" , \"ENST00000455263\" , \"ENST00000503591\" , \"ENST00000504290\" , \"ENST00000504937\" , \"ENST00000505014\" , \"ENST00000508793\" , \"ENST00000509690\" , \"ENST00000510385\" , \"ENST00000514944\" , \"ENST00000574684\" , \"ENST00000576024\" , \"ENST00000604348\" , \"ENST00000610292\" , \"ENST00000610538\" , \"ENST00000610623\" , \"ENST00000615910\" , \"ENST00000617185\" , \"ENST00000618944\" , \"ENST00000619186\" , \"ENST00000619485\" , \"ENST00000620739\" , \"ENST00000622645\" , \"ENST00000635293\" ) idx = match ( kallisto_stringtie_tpm $target_id , TP53 ) idx.1 = which ( is.na ( idx ) == FALSE ) idx.2 = idx [ idx.1 ] TP53 [ idx.2 ] == kallisto_stringtie_tpm $target_id [ idx.1 ] #Check TP53 expression values kallisto_stringtie_tpm [ idx.2, ] #To plot TP53, make a new column kallisto_stringtie_tpm $TP53 = \"Other transcript\" kallisto_stringtie_tpm $TP53 [ idx.2 ] = \"TP53\" kallisto_stringtie_tpm $TP53 = factor ( kallisto_stringtie_tpm $TP53 , levels = c ( \"Other transcript\" , \"TP53\" )) str ( kallisto_stringtie_tpm ) #To remove 0/NA values kallisto_stringtie_tpm $value .stringtie [ which ( is.na ( kallisto_stringtie_tpm $value .stringtie )== TRUE )] = 0 .001 #kallisto_stringtie_tpm$value.kallisto[which(is.na(kallisto_stringtie_tpm$value.kallisto)==TRUE)] = 0.001 kallisto_stringtie_tpm $value .stringtie = kallisto_stringtie_tpm $value .stringtie + 0 .1 kallisto_stringtie_tpm $value .kallisto = kallisto_stringtie_tpm $value .kallisto + 0 .1 ########### plot the result ###################### #pdf(file=\"transcript_stringtie_v_kallisto.pdf\", height=8, width=8) ggplot ( data = kallisto_stringtie_tpm, aes ( x = log2 ( value.kallisto ) , y = log2 ( value.stringtie ) , colour = TP53 )) + geom_point ( alpha = 1 ) + scale_colour_manual ( values = c ( \"forestgreen\" , \"firebrick\" , \"dodgerblue\" ) , name = \"Transcripts\" , drop = TRUE ) + geom_point ( data = subset ( kallisto_stringtie_tpm, TP53 == \"TP53\" ) , aes ( x = log2 ( value.kallisto ) , y = log2 ( value.stringtie ) , colour = TP53 ) , inherit.aes = FALSE ) + facet_wrap ( ~variable ) + theme_bw () #To plot density disbributions df = rbind ( data.frame ( variable = kallisto_stringtie_tpm $variable , value = kallisto_stringtie_tpm $value .kallisto, type = \"kallisto\" ) , data.frame ( variable = kallisto_stringtie_tpm $variable , value = kallisto_stringtie_tpm $value .stringtie, type = \"stringtie\" )) df $type = factor ( df $type , levels = c ( \"kallisto\" , \"stringtie\" )) head ( df ) ggplot ( data = df, aes ( x = log2 ( value ))) + geom_density () + facet_wrap ( type~variable, ncol = 4 ) + ggtitle ( \"Density distributions\" ) + xlab ( \"Log2 expression value\" ) + theme ( legend.position = \"none\" ) #dev.off() Gene Fusion detection \u00b6 Introduction \u00b6 In addition to providing information about gene expression, RNA-seq data can be used to discover transcripts which result from chromosomal translocations. Translocations and their resultant chimeric (AKA fusion) transcripts are important driver mutations in many cancers. A variety of specialized alignment and filtering strategies have been developed to identify fusion transcripts from RNA, but these programs suffer from low specificity (i.e. many false-positives) and poor correlation across methods. This tutorial uses the kallisto and pizzly tools for fusion detection from RNA-seq data. Kallisto quantifies transcript abundance through pseudoalignment . Pizzly aligns reads which kallisto has flagged as potentially spanning fusion junctions. Running the tutorial requires RNA fastq files, a reference transcriptome, and a gene annotation file- see below. Some of the tools used to detection gene fusions from RNAseq are Arriba, EricScript, FusionCatcher, Fusion-Inspector, fusion-report, Pizzly, Squid, Star-Fusion. All these tools have their own merits and demerits. They are often used in combinations or all together. Hence, we need to assess which one is suitable for our data. We will use Pizzly in this exercise. Setup \u00b6 Prerequisites- This module assumes you have completed the initial setup process for the course including: - installed Conda package manager, R, pizzly, and kallisto. - have fastq sequence files of normal and tumor RNA at ~/workspace/inputs/data/fastq/chr6_and_chr17/. Additional setup: Important: pizzly will not work with the most recent Ensembl human GTF annotation file. Download the version 87 GTF as shown in the below code block. We will subset the reference transcriptome fasta file. Fasta splitting programs which do not preserve the full Ensembl header such as gffread or gtf_to_fasta will not work with pizzly. Download Ensembl GTF and fasta and parse to include only chromosomes 6 and 17 ( 15 min ): # Get files from source #Already run #mkdir -p ~/workspace/inputs/reference/fusion #cd ~/workspace/inputs/reference/fusion/ #wget ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz #gunzip Homo_sapiens.GRCh38.cdna.all.fa.gz #wget ftp://ftp.ensembl.org/pub/release-87/gtf/homo_sapiens/Homo_sapiens.GRCh38.87.gtf.gz #gunzip -k Homo_sapiens.GRCh38.87.gtf.gz cd ~/workspace/inputs/reference/fusion/ ls -halt # Get annotations for only chromosomes 6 and 17 cat Homo_sapiens.GRCh38.87.gtf | grep --color = never -w \"^6\" > chr617.gtf cat Homo_sapiens.GRCh38.87.gtf | grep --color = never -w \"^17\" >> chr617.gtf head chr617.gtf tail chr617.gtf #### CODE BELOW ALREDY RUN - TAKES >2h # Parse transcriptome fasta, preserving full ensembl headers # Setup directory #mkdir -p ~/workspace/inputs/reference/fusion/per-feature #cd ~/workspace/inputs/reference/fusion/per-feature # Split fasta at each instance of a sequence header and write to new file #csplit -s -z ../Homo_sapiens.GRCh38.cdna.all.fa '/>/' '{*}' # If from chromosomes 6 or 17, rename files using the columns of the original ensemble header ##### (This step takes about 120 minutes. You can proceed with the next section in ~/workspace/inputs/data/fastq/chr6_and_chr17) #Run with Nohup in case connection is lost #for f in xx*; do awk -F \":\" 'NR==1 && $3==\"6\" || NR==1 && $3==\"17\"{print $2 \".\" $3 \".\" $4 \".\" $5}' $f | xargs -I{} mv $f {}.fa; done # Concatenate features from chromsomes 6 and 17 to a new reference fasta #cd ~/workspace/inputs/reference/fusion #cat ./per-feature/GRCh38.6.*.fa ./per-feature/GRCh38.17.*.fa > chr617.fa #rm -rf per-feature To get one read pair each for normal and tumor, merge the chr6_and_chr17 only RNA-seq fastqs. mkdir -p ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion cd ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion cat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz > RNAseq_Norm_R1.fastq.gz cat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz > RNAseq_Norm_R2.fastq.gz cat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz > RNAseq_Tumor_R1.fastq.gz cat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz > RNAseq_Tumor_R2.fastq.gz Subsample fastqs to allow fusion alignment to run quickly ( 5 min ): # Use seqtk to take subsamples of the 30% of the fastq read pairs seqtk sample -s100 RNAseq_Norm_R1.fastq.gz 0 .3 > subRNAseq_Norm_R1.fastq.gz seqtk sample -s100 RNAseq_Norm_R2.fastq.gz 0 .3 > subRNAseq_Norm_R2.fastq.gz seqtk sample -s100 RNAseq_Tumor_R1.fastq.gz 0 .3 > subRNAseq_Tumor_R1.fastq.gz seqtk sample -s100 RNAseq_Tumor_R2.fastq.gz 0 .3 > subRNAseq_Tumor_R2.fastq.gz Run Fusion Alignment \u00b6 Create kallisto index: #mkdir -p ~/workspace/rnaseq/fusion cd ~/workspace/rnaseq/fusion kallisto index -i index.617.idx -k 31 --make-unique ~/workspace/inputs/reference/fusion/chr617.fa Call fusions \u00b6 cd ~/workspace/rnaseq/fusion #Quantify potential fusions (**15 min**): nohup kallisto quant -i index.617.idx --fusion -o kquant-norm617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R2.fastq.gz 2 > kallisto_quant_N_bam.out & nohup kallisto quant -i index.617.idx --fusion -o kquant-tumor617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R2.fastq.gz 2 > kallisto_quant_T_bam.out & #Call fusions with pizzly: **120 mins** cd ~/workspace/rnaseq/fusion nohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-norm617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output norm-fuse617 kquant-norm617/fusion.txt 2 > pizzly_N_bam.out & nohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-tumor617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output tumor-fuse617 kquant-tumor617/fusion.txt 2 > pizzly_T_bam.out & If using 30% of reads with the above process, expect about 13,000 retained transcripts from normal and about 3,000 retained transcripts from tumor. See next section to investigate the output of the pizzly fusion calling. #To go the fusion directory cd ~/workspace/rnaseq/fusion # ALREADY RUN: Get R scripts for later use #wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/mod.grolar.R #wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/import_Pizzly.R scp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/*.json . scp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/mod20220117.grolar.R . #list files ls -halt # Open R on AWS and set working directory R setwd ( \"~/workspace/rnaseq/fusion/\" ) #Check the path getwd () # Load required packages library ( cowplot ) library ( jsonlite ) library ( dplyr ) library ( EnsDb.Hsapiens.v86 ) library ( ggplot2 ) library ( chimeraviz ) # Change to your own output location setwd ( \"~\" ) # Load data suffix = \"617.json\" print ( suffix ) JSON_files = list.files ( path = \"~\" , pattern = paste0 ( \"*\" ,suffix )) print ( JSON_files ) Ids = gsub ( suffix, \"\" , JSON_files ) print ( Ids ) # Assuming you have used GRCh38 gene models edb <- EnsDb.Hsapiens.v86 listColumns ( edb ) supportedFilters ( edb ) # Load the functoin https://github.com/MattBashton/grolar/blob/master/grolar.R source ( \"mod20220117.grolar.R\" ) # Suffix which apears after sample id in output file name # Use above funciton on all output files lapply ( Ids, function ( x ) GetFusionz_and_namez ( x, suffix = \"617.json\" ) ) # This function will flatten out the JSON giving you a list of gene A and gene B sorted by splitcount then paircount # Each fusion now has a unique identifier, sequence positions, and distance values for genes from the same chromosome: # Read the flattened files normal = read.table ( \"~/norm-fuse_fusions_filt_sorted.txt\" , header = T ) tumor = read.table ( \"~/tumor-fuse_fusions_filt_sorted.txt\" , header = T ) #Investigate the first five rows head ( normal, 5 ) head ( tumor, 5 ) # Common filtering tasks for fusion output include removing fusions from the tumor sample which are present in the normal, and removing fusions for which the is little support by pair and split read counts: normal $genepr = paste0 ( normal $geneA .name, \".\" ,normal $geneB .name ) tumor $genepr = paste0 ( tumor $geneA .name, \".\" ,tumor $geneB .name ) uniqueTumor = subset ( tumor, ! ( tumor $genepr %in% normal $genepr )) nrow ( uniqueTumor )== nrow ( tumor ) [ 1 ] FALSE nrow ( tumor ) -nrow ( uniqueTumor ) [ 1 ] 2 # There are two fusions (or at least fusion gene pairs) from the normal sample which are also present in the tumor. # Examine the output of- shared_src_tumor = subset ( tumor, ( tumor $genepr %in% normal $genepr )) shared_src_normal = subset ( normal, ( normal $genepr %in% tumor $genepr )) shared_src_tumor shared_src_normal # Filtering by counts: # Merge normal and tumor data normal $sample = \"normal\" tumor $sample = \"tumor\" allfusions = rbind ( normal,tumor ) # Compare counts of paired and split reads tapply ( allfusions $paircount , allfusions $sample , summary ) tapply ( allfusions $splitcount , allfusions $sample , summary ) # As a density plot p1 = ggplot ( allfusions, aes ( paircount, fill = sample )) + geom_density ( alpha = .4 ) + geom_vline ( xintercept = 2 ) + coord_fixed ( ratio = 15 ) p2 = ggplot ( allfusions, aes ( splitcount, fill = sample )) + geom_density ( alpha = .4 ) +coord_cartesian ( ylim = c ( 0 ,.2 )) + geom_vline ( xintercept = 5 ) + coord_fixed ( ratio = 200 ) plot_grid ( p1,p2, ncol = 2 , rel_heights = c ( 1 ,1 )) nrow ( allfusions ) allfusions = allfusions [ which ( allfusions $paircount > = 2 & allfusions $splitcount > = 5 ) , ] nrow ( allfusions ) #write.table(allfusions, \"allfusions.txt\") write.table ( allfusions, file = \"allfusions.txt\" ,row.names = F, col.names = T, quote = F, sep = \"\\t\" ) #Chimeraviz is an R package for visualizing fusions from RNA-seq data. The chimeraviz package has import functions built in for a variety of fusion-finder programs, but not for pizzly. We will have to load our own import function that you downloaded above: # Enter R, install and load chimeraviz R source ( \"https://bioconductor.org/biocLite.R\" ) biocLite ( \"chimeraviz\" ) # (if asked to update old packages, you can ignore- Update all/some/none? [a/s/n]:) library ( chimeraviz ) # Use the pizzly importer script to import fusion data source ( \"./import_Pizzly.R\" ) # You can view the function by calling it without variables #importPizzly fusions = importPizzly ( \"./allfusions.txt\" , \"hg38\" ) plot_circle ( fusions )","title":"RNAseq"},{"location":"lab_session/processing_of_rnaseq/#bioinformatic-processing-of-rna-data","text":"","title":"BIOINFORMATIC PROCESSING OF RNA DATA"},{"location":"lab_session/processing_of_rnaseq/#adapter-trimming-of-the-fastq-files","text":"The purpose of adapter trimming is to remove sequences in our data that correspond to the Illumina sequence adapters. The most common adapter trimming scenario is the removal of adapter sequences that occur at the end of read sequences. This happens when a DNA (or cDNA) fragment is shorter than the read length. For example if we sequence our RNA-seq fragments to 150 base length and a fragment is only 140 bases long the read will end with 10 bases of adapter sequence. Since this adapter sequence does not correspond to the genome, it will not align. Too much adapter sequence can actually prevent reads from aligning at all. Adapter trimming may therefore sometime improve the overall alignment success rate for an RNA-seq data set. Adapter trimming involves a simplistic alignment itself and therefore can be computationally expensive. cd ~/workspace/inputs/references #Command already run #wget -c http://genomedata.org/rnaseq-tutorial/illumina_multiplex.fa #Have a look at the adapter sequences that we want to remove from the RNA data less -SN ~/workspace/inputs/references/illumina_multiplex.fa #Start with tumor RNAseq data cd ~/workspace/inputs/data/fastq/RNAseq_Tumor #Have a look to see what is in the folder ls -halt #pPerform trimming using flexbar #Before using a new tool, make a habit of looking the tool up, # google \"flexbar trimming adapters\" or something similar #As this step takes approxmately 25 min, we will use nohup. #Google \"nohup\" ... #If running with nohup the command will continue even if the connection breaks to the aws server #Also, notice that we are using the flexbar argument --threads 7 which means that we are using 7 cpu threads. To check how many you have at your disposal use the command: lscpu #We have 2 thread left which is good, not to hog all resources #To check the available RAM use the command: cat /proc/meminfo #When trimming the adapters run two simultaneous jobs using 7 cores/job and send both to the background if connection breaks. Redirect the standard output to a custom log file #nohup cmd > custom-out.log & nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane1_R1.fastq.gz --reads2 RNAseq_Tumor_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1 > RNAseq_Tumor_Lane1.log & #Start the second process nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Tumor_Lane2_R1.fastq.gz --reads2 RNAseq_Tumor_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2 > RNAseq_Tumor_Lane2.log & #Use htop to monitor the processor load htop #quit htop by pressing \"q\" #have a look at one of the log files less -SN RNAseq_Tumor_Lane1.log ####### Wait for the processes to stop before continuing ####### #Start the #Normal RNAseq data cd ~/workspace/inputs/data/fastq/RNAseq_Norm nohup flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane1_R1.fastq.gz --reads2 RNAseq_Norm_Lane1_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1 > RNAseq_Normal_Lane1.log & flexbar --adapter-min-overlap 7 --adapter-trim-end RIGHT --adapters ~/workspace/inputs/references/illumina_multiplex.fa --pre-trim-left 13 --max-uncalled 300 --min-read-length 25 --threads 7 --zip-output GZ --reads RNAseq_Norm_Lane2_R1.fastq.gz --reads2 RNAseq_Norm_Lane2_R2.fastq.gz --target ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2 > RNAseq_Normal_Lane2.log & ####### Wait for the processes to stop ####### As you can see, nohup is very useful for sending processes to the backgroud that will continue if connection breaks or if you need to log out. For repetition how to send stdout and stderr to a file while running nohup, have a look here .","title":"Adapter Trimming of the FASTQ files"},{"location":"lab_session/processing_of_rnaseq/#run-fastqc-to-check-the-quality-of-the-fastq-files-as-for-dna-data","text":"#Run fastqc and compre the trimmed and non-trimmed file cd ~/workspace/inputs/data/fastq/RNAseq_Tumor fastqc RNAseq_Tumor_Lane1_R1.fastq.gz fastqc RNAseq_Tumor_Lane1_1.fastq.gz # Download the output to your computer scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1_fastqc.html . scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1_fastqc.html . Compare the two html files side by side: - Any obivous difference? #Move the trimmed data mkdir -p ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed mkdir -p ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_1.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz mv ~/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_2.fastq.gz ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz #make a habit of always checking the result of any command ls -halt ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed ls -halt ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed","title":"Run fastqc to check the quality of the fastq-files (as for DNA data)"},{"location":"lab_session/processing_of_rnaseq/#alignment","text":"We will use the aligner HISAT2 to perform spliced alignments to our reference genome. For efficiency, the output of HISAT (SAM format) will be piped directly to another program called sambamba to first convert to BAM format and then sort that BAM file. Before each command we will also create and assign a path for temporary directories. mkdir -p ~/workspace/rnaseq/alignments cd ~/workspace/rnaseq/alignments # Align tumor data # Runtime: ~15 min each run TUMOR_DATA_1_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626107.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626107 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.4 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam /dev/stdin & TUMOR_DATA_2_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626112.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626112 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-GCCAAT.5 --rg LB:rna_tumor_lib1 --rg SM:HCC1395_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Tumor/trimmed/RNAseq_Tumor_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $TUMOR_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam /dev/stdin & #Wait for the processes to finish, monitor using htop rmdir $TUMOR_DATA_2_TEMP $TUMOR_DATA_1_TEMP # Align normal data NORMAL_DATA_1_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895625992.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895625992 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.4 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane1_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_1_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam /dev/stdin & NORMAL_DATA_2_TEMP = ` mktemp -d ~/workspace/rnaseq/alignments/2895626097.XXXXXXXXXXXX ` nohup hisat2 -p 7 --dta -x ~/workspace/inputs/references/transcriptome/ref_genome --rg-id 2895626097 --rg PL:ILLUMINA --rg PU:H3MYFBBXX-CTTGTA.5 --rg LB:rna_norm_lib1 --rg SM:HCC1395BL_RNA --rna-strandness RF -1 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R1.fastq.gz -2 ~/workspace/inputs/data/fastq/RNAseq_Norm/trimmed/RNAseq_Norm_Lane2_R2.fastq.gz | sambamba view -S -f bam -l 0 /dev/stdin | sambamba sort -t 8 -m 24G --tmpdir $NORMAL_DATA_2_TEMP -o ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam /dev/stdin & #Wait for the processes to finish, monitor using htop rmdir $NORMAL_DATA_2_TEMP $NORMAL_DATA_1_TEMP","title":"Alignment"},{"location":"lab_session/processing_of_rnaseq/#merging-bams","text":"Since we have multiple BAMs of each sample that just represent additional data for the same sequence library, we should combine them into a single BAM for convenience before proceeding. cd ~/workspace/rnaseq/alignments #Runtime: ~ 8m each merging command sambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam sambamba merge -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam","title":"Merging BAMs"},{"location":"lab_session/processing_of_rnaseq/#gtf-general-transfer-format-and-creating-files-for-downstream-processing","text":"We will encounter the GTF file format during the exercises below, described in detail here . The GTF format is use to describe genes and other features of DNA, RNA and proteins. In the section below we will prepare files needed for RNAseq QC NOTE: for the sake of time we are only investigating genes on chr6 and chr17 #Go to this directory: cd ~/workspace/inputs/references/transcriptome #Have a peak into the transcriptome gtf file and try to make sense of it using the ensembl description above less -SN ref_transcriptome.gtf #check on which chromsomes from which we have transcripts cut -f1 ref_transcriptome.gtf | sort | uniq -c # Generating the necessary input files for picard CollectRnaSeqMetrics cd ~/workspace/inputs/references/transcriptome grep -i rrna ref_transcriptome.gtf > ref_ribosome.gtf #A bed file with Ribosomal RNA gff2bed < ~/workspace/inputs/references/transcriptome/ref_ribosome.gtf > ref_ribosome.bed #Check the ribosome bed file less -SN ref_ribosome.bed java -jar $PICARD BedToIntervalList I = ~/workspace/inputs/references/transcriptome/ref_ribosome.bed O = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict gtfToGenePred -genePredExt ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf ~/workspace/inputs/references/transcriptome/ref_flat.txt cat ref_flat.txt | awk '{print $12\"\\t\"$0}' | cut -d $'\\t' -f1-11 > ref_flat_final.txt mv ref_flat_final.txt ref_flat.txt","title":"GTF (General Transfer Format) and creating files for downstream processing"},{"location":"lab_session/processing_of_rnaseq/#post-alignment-qc","text":"cd ~/workspace/rnaseq/alignments #Run samtools flagstat #Runtime: ~2min, start and send to background nohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Norm.bam > ~/workspace/rnaseq/alignments/RNAseq_Norm_flagstat.txt 2 > flagstat_RNA_N_bam.out & nohup samtools flagstat ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam > ~/workspace/rnaseq/alignments/RNAseq_Tumor_flagstat.txt 2 > flagstat_RNA_T_bam.out & #Runtime: ~12 min, start and send to background nohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam > fastqc_RNA_T_bam.out 2 > & 1 & nohup fastqc -t 8 ~/workspace/rnaseq/alignments/RNAseq_Norm.bam > fastqc_RNA_N_bam.out 2 > & 1 & # Runtime: 26min nohup java -jar $PICARD CollectRnaSeqMetrics I = ~/workspace/rnaseq/alignments/RNAseq_Norm.bam O = ~/workspace/rnaseq/alignments/RNAseq_Norm.RNA_Metrics REF_FLAT = ~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND = SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list > collectRnaSeqMetrics_N.out 2 > & 1 & #Follow the progress of the program by looking in the nohup output file less -SN collectRnaSeqMetrics_N.out nohup java -jar $PICARD CollectRnaSeqMetrics I = ~/workspace/rnaseq/alignments/RNAseq_Tumor.bam O = ~/workspace/rnaseq/alignments/RNAseq_Tumor.RNA_Metrics REF_FLAT = ~/workspace/inputs/references/transcriptome/ref_flat.txt STRAND = SECOND_READ_TRANSCRIPTION_STRAND RIBOSOMAL_INTERVALS = ~/workspace/inputs/references/transcriptome/ref_ribosome.interval_list > collectRnaSeqMetrics_T.out 2 > & 1 & cd ~/workspace/rnaseq #mkdir post_align_qc cd post_align_qc multiqc ~/workspace/rnaseq/alignments/ #Finally, download multiqc files to your local computer # Download MultiQC output to the local computer, open the .html in you favourite browser. scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/rnaseq/post_align_qc/multiqc* . Familiarize yourself with the RNAseq MultiQC data","title":"Post-alignment QC"},{"location":"lab_session/processing_of_rnaseq/#indexing-bams","text":"In order to be able to view our BAM files in IGV, as usual we need to index them cd ~/workspace/rnaseq/alignments/ samtools index RNAseq_Norm.bam samtools index RNAseq_Tumor.bam","title":"Indexing BAMs"},{"location":"lab_session/processing_of_rnaseq/#run-a-simplified-reference-only-stringtie-expression-approach","text":"The StringTie developer's recommend to: perform reference guided transcript compilation (aka transcript assembly) on each individual sample. merge transcript predictions from all samples into a single model of the transcriptome. annotate this predicted transcriptome with known transcriptome information. estimate abundance for each of the transcripts in this final transcriptome model in each sample. In the final result we would have abundance for all the same transcripts across all samples. This includes a combination of predicted and known transcripts. It is sometimes convenient to have a more simplified workflow where we only have values for known transcripts. This is particularly true in species where we already have comprehensive high quality transcriptome annotations and there is less of a focus on de novo transcript discovery. The following workflow produces a \"reference-only\" transcriptome result in which we will perform abundance calculations on each lane of data individually. cd ~/workspace/rnaseq #mkdir ref-only-expression cd ref-only-expression nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane1.bam > knowntranscripts_T1_L1.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Tumor_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Tumor_Lane2.bam > knowntranscripts_T1_L2.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane1/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane1.bam > knowntranscripts_N1_L1.out 2 > & 1 & nohup stringtie -p 4 -e -G ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf -o ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/transcripts.gtf -A ~/workspace/rnaseq/ref-only-expression/RNAseq_Norm_Lane2/gene_abundances.tsv ~/workspace/rnaseq/alignments/RNAseq_Norm_Lane2.bam > knowntranscripts_T2_L2.out 2 > & 1 & #Wait to continue submitting jobs until the scripts have finished Create a tidy expression matrix files for the StringTie results. This will be done at both the gene and transcript level and also will take into account the various expression measures produced: coverage, FPKM, and TPM. cd ~/workspace/rnaseq/ref-only-expression #Already run #wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/stringtie_expression_matrix.pl #chmod +x stringtie_expression_matrix.pl ./stringtie_expression_matrix.pl --expression_metric = TPM --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_tpm_all_samples.tsv --gene_matrix_file = gene_tpm_all_samples.tsv ./stringtie_expression_matrix.pl --expression_metric = FPKM --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_fpkm_all_samples.tsv --gene_matrix_file = gene_fpkm_all_samples.tsv ./stringtie_expression_matrix.pl --expression_metric = Coverage --result_dirs = 'RNAseq_Norm_Lane1,RNAseq_Norm_Lane2,RNAseq_Tumor_Lane1,RNAseq_Tumor_Lane2' --transcript_matrix_file = transcript_coverage_all_samples.tsv --gene_matrix_file = gene_coverage_all_samples.tsv #Have a look at the output files head gene_coverage_all_samples.tsv transcript_coverage_all_samples.tsv gene_fpkm_all_samples.tsv transcript_fpkm_all_samples.tsv gene_tpm_all_samples.tsv transcript_tpm_all_samples.tsv","title":"Run a simplified \"reference only\" StringTie expression approach"},{"location":"lab_session/processing_of_rnaseq/#reference-free-expression-analysis-with-kallisto","text":"Remember that in previous sections we have been using reference genome fasta sequences for the reference for alignment and subsequent steps. However, Kallisto works directly on target cDNA/transcript sequences. We have for stringtie used transcript annotations for genes on our subset of chromosomes (i.e. chr6 and chr17). The transcript models were downloaded from Ensembl in GTF format. This GTF contains a description of the coordinates of exons that make up each transcript but it does not contain the transcript sequences themselves which Kallisto is using. There are many places we could obtain such transcript sequences. For example, we could have download them directly in Fasta format from the Ensembl FTP site (or from UCSC or NCBI). cd ~/workspace/rnaseq/ #mkdir kallisto cd kallisto # first check that the GTF and Fasta file are present for Kallisto head ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf head ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa #Each transcript and its sequence head ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa # now check for the kallisto index is there ls -halt ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index # create a list of all transcript IDs for later use: cd ~/workspace/rnaseq/kallisto/ cat ~/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_clean.fa | grep \">\" | perl -ne '$_ =~ s/\\>//; print $_' | sort | uniq > transcript_id_list.txt head -n 10 transcript_id_list.txt","title":"Reference Free Expression Analysis with Kallisto"},{"location":"lab_session/processing_of_rnaseq/#generate-abundance-estimates-for-all-samples-using-kallisto","text":"As we did with StringTie we will generate transcript abundances for each of our demonstration samples using Kallisto. Here we are treating the two lanes for each sample as if they were independent samples. cd ~/workspace/rnaseq/kallisto/ #mkdir quants cd quants nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Norm_Lane1 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz > kallisto_quant_N_L1.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Norm_Lane2 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz > kallisto_quant_N_L2.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Tumor_Lane1 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz > kallisto_quant_T_L1.out 2 > & 1 & nohup kallisto quant --index = /home/ubuntu/workspace/inputs/references/transcriptome/kallisto/ref_transcriptome_kallisto_index --output-dir = RNAseq_Tumor_Lane2 --threads = 8 --plaintext /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz /home/ubuntu/workspace/inputs/data/fastq/RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz > kallisto_quant_T_L2.out 2 > & 1 & #check nohup output less -SN kallisto_quant_T_L2.out #Create a single TSV file that has the TPM abundance estimates for all six samples. #First check the contets of an abudance output file from Kallisto less -SN ./RNAseq_Norm_Lane1/abundance.tsv #Merge all files paste */abundance.tsv | cut -f 1 ,2,5,10,15,20 > transcript_tpms_all_samples.tsv ls -1 */abundance.tsv | perl -ne 'chomp $_; if ($_ =~ /(\\S+)\\/abundance\\.tsv/){print \"\\t$1\"}' | perl -ne 'print \"target_id\\tlength$_\\n\"' > header.tsv cat header.tsv transcript_tpms_all_samples.tsv | grep -v \"tpm\" > transcript_tpms_all_samples.tsv2 mv transcript_tpms_all_samples.tsv2 transcript_tpms_all_samples.tsv rm -f header.tsv head transcript_tpms_all_samples.tsv #First create a gene version of the Kallisto TPM matrix, we will simply sum the TPM values for transcripts of the same gene. # Command already run #wget https://raw.githubusercontent.com/griffithlab/rnaseq_tutorial/master/scripts/kallisto_gene_matrix.pl #chmod +x kallisto_gene_matrix.pl ./kallisto_gene_matrix.pl --gtf_file = /home/ubuntu/workspace/inputs/references/transcriptome/ref_transcriptome.gtf --kallisto_transcript_matrix_in = transcript_tpms_all_samples.tsv --kallisto_transcript_matrix_out = gene_tpms_all_samples.tsv less -SN gene_tpms_all_samples.tsv","title":"Generate abundance estimates for all samples using Kallisto"},{"location":"lab_session/processing_of_rnaseq/#compare-expression-values-between-kallisto-and-stringtie","text":"To compare the two approaches we can use the expression value for each Ensembl transcript. To do this comparison, we need to gather the expression estimates for each of our replicates from each approach. First - download the files to your local machine #Run these commands on your local machine scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/kallisto/transcript_tpms_all_samples.tsv . mv transcript_tpms_all_samples.tsv kallisto_transcript_tpms_all_samples.tsv scp -ri ~/course_EC2_01.pem ubuntu@AWS_ADDRESS_HERE:/home/ubuntu/workspace/rnaseq/ref-only-expression/transcript_tpm_all_samples.tsv . mv transcript_tpm_all_samples.tsv stringtie_transcript_tpms_all_samples.tsv #Remember, one gene can have many transcripts. The ensembl gene ID for TP53 is ENSG00000141510. Check which ensembl transcripts that exist in the reference gtf. #RUN ON THE AWS SERVER grep ENST ~/workspace/inputs/references/transcriptome/ref_transcriptome.gtf | grep ENSG00000141510 | perl -nle '@a = split /\\t/; $_ =~ /transcript_id\\s\"(\\S*)\";/g; print $1;' | sort | uniq # start R # set the working directory to where the files were downloaded setwd ( \"DIR GOES HERE\" ) # load libraries library ( ggplot2 ) library ( reshape2 ) # read in data kallisto_transcript_tpm <- read.delim ( \"kallisto_transcript_tpms_all_samples.tsv\" ) head ( kallisto_transcript_tpm ) stringtie_transcript_tpm <- read.delim ( \"stringtie_transcript_tpms_all_samples.tsv\" ) head ( stringtie_transcript_tpm ) # minor reformatting kallisto_transcript_tpm <- kallisto_transcript_tpm [ ,-2 ] kallisto_transcript_tpm <- melt ( kallisto_transcript_tpm, id.vars = c ( \"target_id\" )) head ( kallisto_transcript_tpm ) stringtie_transcript_tpm <- melt ( stringtie_transcript_tpm, id.vars = c ( \"Transcript_ID\" )) head ( stringtie_transcript_tpm ) # merge the data kallisto_stringtie_tpm <- merge ( kallisto_transcript_tpm, stringtie_transcript_tpm, by.x = c ( \"target_id\" , \"variable\" ) , by.y = c ( \"Transcript_ID\" , \"variable\" ) , suffixes = c ( \".kallisto\" , \".stringtie\" )) #The numeric vector is read as a character vector, correcting .. kallisto_stringtie_tpm $value .stringtie = as.numeric ( kallisto_stringtie_tpm $value .stringtie ) str ( kallisto_stringtie_tpm ) #TP53 transcript ID vector from the transcript GTF TP53 = c ( \"ENST00000269305\" , \"ENST00000359597\" , \"ENST00000413465\" , \"ENST00000420246\" , \"ENST00000445888\" , \"ENST00000455263\" , \"ENST00000503591\" , \"ENST00000504290\" , \"ENST00000504937\" , \"ENST00000505014\" , \"ENST00000508793\" , \"ENST00000509690\" , \"ENST00000510385\" , \"ENST00000514944\" , \"ENST00000574684\" , \"ENST00000576024\" , \"ENST00000604348\" , \"ENST00000610292\" , \"ENST00000610538\" , \"ENST00000610623\" , \"ENST00000615910\" , \"ENST00000617185\" , \"ENST00000618944\" , \"ENST00000619186\" , \"ENST00000619485\" , \"ENST00000620739\" , \"ENST00000622645\" , \"ENST00000635293\" ) idx = match ( kallisto_stringtie_tpm $target_id , TP53 ) idx.1 = which ( is.na ( idx ) == FALSE ) idx.2 = idx [ idx.1 ] TP53 [ idx.2 ] == kallisto_stringtie_tpm $target_id [ idx.1 ] #Check TP53 expression values kallisto_stringtie_tpm [ idx.2, ] #To plot TP53, make a new column kallisto_stringtie_tpm $TP53 = \"Other transcript\" kallisto_stringtie_tpm $TP53 [ idx.2 ] = \"TP53\" kallisto_stringtie_tpm $TP53 = factor ( kallisto_stringtie_tpm $TP53 , levels = c ( \"Other transcript\" , \"TP53\" )) str ( kallisto_stringtie_tpm ) #To remove 0/NA values kallisto_stringtie_tpm $value .stringtie [ which ( is.na ( kallisto_stringtie_tpm $value .stringtie )== TRUE )] = 0 .001 #kallisto_stringtie_tpm$value.kallisto[which(is.na(kallisto_stringtie_tpm$value.kallisto)==TRUE)] = 0.001 kallisto_stringtie_tpm $value .stringtie = kallisto_stringtie_tpm $value .stringtie + 0 .1 kallisto_stringtie_tpm $value .kallisto = kallisto_stringtie_tpm $value .kallisto + 0 .1 ########### plot the result ###################### #pdf(file=\"transcript_stringtie_v_kallisto.pdf\", height=8, width=8) ggplot ( data = kallisto_stringtie_tpm, aes ( x = log2 ( value.kallisto ) , y = log2 ( value.stringtie ) , colour = TP53 )) + geom_point ( alpha = 1 ) + scale_colour_manual ( values = c ( \"forestgreen\" , \"firebrick\" , \"dodgerblue\" ) , name = \"Transcripts\" , drop = TRUE ) + geom_point ( data = subset ( kallisto_stringtie_tpm, TP53 == \"TP53\" ) , aes ( x = log2 ( value.kallisto ) , y = log2 ( value.stringtie ) , colour = TP53 ) , inherit.aes = FALSE ) + facet_wrap ( ~variable ) + theme_bw () #To plot density disbributions df = rbind ( data.frame ( variable = kallisto_stringtie_tpm $variable , value = kallisto_stringtie_tpm $value .kallisto, type = \"kallisto\" ) , data.frame ( variable = kallisto_stringtie_tpm $variable , value = kallisto_stringtie_tpm $value .stringtie, type = \"stringtie\" )) df $type = factor ( df $type , levels = c ( \"kallisto\" , \"stringtie\" )) head ( df ) ggplot ( data = df, aes ( x = log2 ( value ))) + geom_density () + facet_wrap ( type~variable, ncol = 4 ) + ggtitle ( \"Density distributions\" ) + xlab ( \"Log2 expression value\" ) + theme ( legend.position = \"none\" ) #dev.off()","title":"Compare expression values between Kallisto and StringTie"},{"location":"lab_session/processing_of_rnaseq/#gene-fusion-detection","text":"","title":"Gene Fusion detection"},{"location":"lab_session/processing_of_rnaseq/#introduction","text":"In addition to providing information about gene expression, RNA-seq data can be used to discover transcripts which result from chromosomal translocations. Translocations and their resultant chimeric (AKA fusion) transcripts are important driver mutations in many cancers. A variety of specialized alignment and filtering strategies have been developed to identify fusion transcripts from RNA, but these programs suffer from low specificity (i.e. many false-positives) and poor correlation across methods. This tutorial uses the kallisto and pizzly tools for fusion detection from RNA-seq data. Kallisto quantifies transcript abundance through pseudoalignment . Pizzly aligns reads which kallisto has flagged as potentially spanning fusion junctions. Running the tutorial requires RNA fastq files, a reference transcriptome, and a gene annotation file- see below. Some of the tools used to detection gene fusions from RNAseq are Arriba, EricScript, FusionCatcher, Fusion-Inspector, fusion-report, Pizzly, Squid, Star-Fusion. All these tools have their own merits and demerits. They are often used in combinations or all together. Hence, we need to assess which one is suitable for our data. We will use Pizzly in this exercise.","title":"Introduction"},{"location":"lab_session/processing_of_rnaseq/#setup","text":"Prerequisites- This module assumes you have completed the initial setup process for the course including: - installed Conda package manager, R, pizzly, and kallisto. - have fastq sequence files of normal and tumor RNA at ~/workspace/inputs/data/fastq/chr6_and_chr17/. Additional setup: Important: pizzly will not work with the most recent Ensembl human GTF annotation file. Download the version 87 GTF as shown in the below code block. We will subset the reference transcriptome fasta file. Fasta splitting programs which do not preserve the full Ensembl header such as gffread or gtf_to_fasta will not work with pizzly. Download Ensembl GTF and fasta and parse to include only chromosomes 6 and 17 ( 15 min ): # Get files from source #Already run #mkdir -p ~/workspace/inputs/reference/fusion #cd ~/workspace/inputs/reference/fusion/ #wget ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz #gunzip Homo_sapiens.GRCh38.cdna.all.fa.gz #wget ftp://ftp.ensembl.org/pub/release-87/gtf/homo_sapiens/Homo_sapiens.GRCh38.87.gtf.gz #gunzip -k Homo_sapiens.GRCh38.87.gtf.gz cd ~/workspace/inputs/reference/fusion/ ls -halt # Get annotations for only chromosomes 6 and 17 cat Homo_sapiens.GRCh38.87.gtf | grep --color = never -w \"^6\" > chr617.gtf cat Homo_sapiens.GRCh38.87.gtf | grep --color = never -w \"^17\" >> chr617.gtf head chr617.gtf tail chr617.gtf #### CODE BELOW ALREDY RUN - TAKES >2h # Parse transcriptome fasta, preserving full ensembl headers # Setup directory #mkdir -p ~/workspace/inputs/reference/fusion/per-feature #cd ~/workspace/inputs/reference/fusion/per-feature # Split fasta at each instance of a sequence header and write to new file #csplit -s -z ../Homo_sapiens.GRCh38.cdna.all.fa '/>/' '{*}' # If from chromosomes 6 or 17, rename files using the columns of the original ensemble header ##### (This step takes about 120 minutes. You can proceed with the next section in ~/workspace/inputs/data/fastq/chr6_and_chr17) #Run with Nohup in case connection is lost #for f in xx*; do awk -F \":\" 'NR==1 && $3==\"6\" || NR==1 && $3==\"17\"{print $2 \".\" $3 \".\" $4 \".\" $5}' $f | xargs -I{} mv $f {}.fa; done # Concatenate features from chromsomes 6 and 17 to a new reference fasta #cd ~/workspace/inputs/reference/fusion #cat ./per-feature/GRCh38.6.*.fa ./per-feature/GRCh38.17.*.fa > chr617.fa #rm -rf per-feature To get one read pair each for normal and tumor, merge the chr6_and_chr17 only RNA-seq fastqs. mkdir -p ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion cd ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion cat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R1.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R1.fastq.gz > RNAseq_Norm_R1.fastq.gz cat ../../RNAseq_Norm/RNAseq_Norm_Lane1_R2.fastq.gz ../../RNAseq_Norm/RNAseq_Norm_Lane2_R2.fastq.gz > RNAseq_Norm_R2.fastq.gz cat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R1.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R1.fastq.gz > RNAseq_Tumor_R1.fastq.gz cat ../../RNAseq_Tumor/RNAseq_Tumor_Lane1_R2.fastq.gz ../../RNAseq_Tumor/RNAseq_Tumor_Lane2_R2.fastq.gz > RNAseq_Tumor_R2.fastq.gz Subsample fastqs to allow fusion alignment to run quickly ( 5 min ): # Use seqtk to take subsamples of the 30% of the fastq read pairs seqtk sample -s100 RNAseq_Norm_R1.fastq.gz 0 .3 > subRNAseq_Norm_R1.fastq.gz seqtk sample -s100 RNAseq_Norm_R2.fastq.gz 0 .3 > subRNAseq_Norm_R2.fastq.gz seqtk sample -s100 RNAseq_Tumor_R1.fastq.gz 0 .3 > subRNAseq_Tumor_R1.fastq.gz seqtk sample -s100 RNAseq_Tumor_R2.fastq.gz 0 .3 > subRNAseq_Tumor_R2.fastq.gz","title":"Setup"},{"location":"lab_session/processing_of_rnaseq/#run-fusion-alignment","text":"Create kallisto index: #mkdir -p ~/workspace/rnaseq/fusion cd ~/workspace/rnaseq/fusion kallisto index -i index.617.idx -k 31 --make-unique ~/workspace/inputs/reference/fusion/chr617.fa","title":"Run Fusion Alignment"},{"location":"lab_session/processing_of_rnaseq/#call-fusions","text":"cd ~/workspace/rnaseq/fusion #Quantify potential fusions (**15 min**): nohup kallisto quant -i index.617.idx --fusion -o kquant-norm617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Norm_R2.fastq.gz 2 > kallisto_quant_N_bam.out & nohup kallisto quant -i index.617.idx --fusion -o kquant-tumor617 ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R1.fastq.gz ~/workspace/inputs/data/fastq/chr6_and_chr17/fusion/subRNAseq_Tumor_R2.fastq.gz 2 > kallisto_quant_T_bam.out & #Call fusions with pizzly: **120 mins** cd ~/workspace/rnaseq/fusion nohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-norm617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output norm-fuse617 kquant-norm617/fusion.txt 2 > pizzly_N_bam.out & nohup pizzly -k 31 --gtf ~/workspace/inputs/reference/fusion/chr617.gtf --cache index-tumor617.cache.txt --align-score 2 --insert-size 400 --fasta ~/workspace/inputs/reference/fusion/chr617.fa --output tumor-fuse617 kquant-tumor617/fusion.txt 2 > pizzly_T_bam.out & If using 30% of reads with the above process, expect about 13,000 retained transcripts from normal and about 3,000 retained transcripts from tumor. See next section to investigate the output of the pizzly fusion calling. #To go the fusion directory cd ~/workspace/rnaseq/fusion # ALREADY RUN: Get R scripts for later use #wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/mod.grolar.R #wget https://raw.githubusercontent.com/griffithlab/pmbio.org/master/assets/course_scripts/import_Pizzly.R scp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/*.json . scp -ri ~/course_EC2_01.pem ubuntu@ec2-34-203-226-9.compute-1.amazonaws.com:~/workspace/rnaseq/fusion/mod20220117.grolar.R . #list files ls -halt # Open R on AWS and set working directory R setwd ( \"~/workspace/rnaseq/fusion/\" ) #Check the path getwd () # Load required packages library ( cowplot ) library ( jsonlite ) library ( dplyr ) library ( EnsDb.Hsapiens.v86 ) library ( ggplot2 ) library ( chimeraviz ) # Change to your own output location setwd ( \"~\" ) # Load data suffix = \"617.json\" print ( suffix ) JSON_files = list.files ( path = \"~\" , pattern = paste0 ( \"*\" ,suffix )) print ( JSON_files ) Ids = gsub ( suffix, \"\" , JSON_files ) print ( Ids ) # Assuming you have used GRCh38 gene models edb <- EnsDb.Hsapiens.v86 listColumns ( edb ) supportedFilters ( edb ) # Load the functoin https://github.com/MattBashton/grolar/blob/master/grolar.R source ( \"mod20220117.grolar.R\" ) # Suffix which apears after sample id in output file name # Use above funciton on all output files lapply ( Ids, function ( x ) GetFusionz_and_namez ( x, suffix = \"617.json\" ) ) # This function will flatten out the JSON giving you a list of gene A and gene B sorted by splitcount then paircount # Each fusion now has a unique identifier, sequence positions, and distance values for genes from the same chromosome: # Read the flattened files normal = read.table ( \"~/norm-fuse_fusions_filt_sorted.txt\" , header = T ) tumor = read.table ( \"~/tumor-fuse_fusions_filt_sorted.txt\" , header = T ) #Investigate the first five rows head ( normal, 5 ) head ( tumor, 5 ) # Common filtering tasks for fusion output include removing fusions from the tumor sample which are present in the normal, and removing fusions for which the is little support by pair and split read counts: normal $genepr = paste0 ( normal $geneA .name, \".\" ,normal $geneB .name ) tumor $genepr = paste0 ( tumor $geneA .name, \".\" ,tumor $geneB .name ) uniqueTumor = subset ( tumor, ! ( tumor $genepr %in% normal $genepr )) nrow ( uniqueTumor )== nrow ( tumor ) [ 1 ] FALSE nrow ( tumor ) -nrow ( uniqueTumor ) [ 1 ] 2 # There are two fusions (or at least fusion gene pairs) from the normal sample which are also present in the tumor. # Examine the output of- shared_src_tumor = subset ( tumor, ( tumor $genepr %in% normal $genepr )) shared_src_normal = subset ( normal, ( normal $genepr %in% tumor $genepr )) shared_src_tumor shared_src_normal # Filtering by counts: # Merge normal and tumor data normal $sample = \"normal\" tumor $sample = \"tumor\" allfusions = rbind ( normal,tumor ) # Compare counts of paired and split reads tapply ( allfusions $paircount , allfusions $sample , summary ) tapply ( allfusions $splitcount , allfusions $sample , summary ) # As a density plot p1 = ggplot ( allfusions, aes ( paircount, fill = sample )) + geom_density ( alpha = .4 ) + geom_vline ( xintercept = 2 ) + coord_fixed ( ratio = 15 ) p2 = ggplot ( allfusions, aes ( splitcount, fill = sample )) + geom_density ( alpha = .4 ) +coord_cartesian ( ylim = c ( 0 ,.2 )) + geom_vline ( xintercept = 5 ) + coord_fixed ( ratio = 200 ) plot_grid ( p1,p2, ncol = 2 , rel_heights = c ( 1 ,1 )) nrow ( allfusions ) allfusions = allfusions [ which ( allfusions $paircount > = 2 & allfusions $splitcount > = 5 ) , ] nrow ( allfusions ) #write.table(allfusions, \"allfusions.txt\") write.table ( allfusions, file = \"allfusions.txt\" ,row.names = F, col.names = T, quote = F, sep = \"\\t\" ) #Chimeraviz is an R package for visualizing fusions from RNA-seq data. The chimeraviz package has import functions built in for a variety of fusion-finder programs, but not for pizzly. We will have to load our own import function that you downloaded above: # Enter R, install and load chimeraviz R source ( \"https://bioconductor.org/biocLite.R\" ) biocLite ( \"chimeraviz\" ) # (if asked to update old packages, you can ignore- Update all/some/none? [a/s/n]:) library ( chimeraviz ) # Use the pizzly importer script to import fusion data source ( \"./import_Pizzly.R\" ) # You can view the function by calling it without variables #importPizzly fusions = importPizzly ( \"./allfusions.txt\" , \"hg38\" ) plot_circle ( fusions )","title":"Call fusions"},{"location":"lab_session/real_life_example_cli/","text":"Real life example of how to use command line features \u00b6 This is an example of how to use various UNIX/bash/command line features for finding files, assigning variables, parsing and modifying text strings etc. The purpose in the example is to run programs that creates bam files from fastq files and then calculates some quality control metrics. This is done as a loop over multiple samples, where the same commands are applied to every sample. At the end, a table of the resulting metrics of interest is created. This is not to be run as part of the lab, just a demonstration of a real life example when multiple various command line features are utilized. cd /path/to/project_dir # move to project directory ref = /path/to/human_g1k_v37_decoy.fasta # assign path to file as variable bwaRef = /path/to/bwa/human_g1k_v37_decoy.fasta # assign path to file as variable tmpDir = /scratch/tmp/rebber # assign path to directory as variable inbox = /path/to/fastq_dir # assign path to directory as variable # find fastq files of interest, # extract the sample names from found paths with basename, # sort and assign results to array (one sample per array entry): samples =( $( find $inbox -maxdepth 1 -mindepth 1 -newermt 2021 -01-11 \\ ! -newermt 2021 -01-12 -name \"PB-P-HD*\" | xargs -I {} basename {} | sort ) ) echo ${ samples [@] } # display all entries of array echo ${# samples [@] } # display number of entries in array mkdir -p bams # create output directory # for loop over the samples: for samp in ${ samples [@] } ; do # get correct design files: # extract info from sample name (7th field, everything before \"2020\"): design = $( echo $samp | cut -f 7 -d \"-\" | sed 's/2020.*//g' ) # --> e.g. PB-P-HD181117-N-7minfrag-KH20201002-PS20201005 --> PS # assign variables depending on the value of the $design variable: if [[ $design == \"PB\" ]] ; then baits = /path/to/PB.baits.interval_list targets = /path/to/PB.targets.interval_list elif [[ $design == \"PS\" ]] ; then baits = /path/to/PS.baits.interval_list targets = /path/to/PS.targets.interval_list else echo \"Unknown design $design , skipping sample $samp \" # print message continue # skip the rest of the loop for this value of $samp fi # skewer (trimming of fastq files) threads = 6 # set variable to use for this tool dep_jids = \"\" # reset list of jobs to depend on (from previous iterations) mkdir -p skewer/ $samp #create output dir # find the \"first in pair\" fastq files for this sample: fq1s =( $( find $inbox / $samp -name \"*_1.fastq.gz\" | sort ) ) for fq1 in ${ fq1s [@] } ; do # loop over the found fastq files # get the basename of the current fastq file, # sed replaces the pattern (to nothing), # basename removes the path to the file: fqbase = $( echo $fq1 | sed 's/_1.fastq.gz//g' | xargs -I {} basename {} ) # find the corresponding second in pair fastq file, # for the current first in pair fastq file: fq2 = $( find $inbox / $samp -name ${ fqbase } _2.fastq.gz ) # assign output path and base name: outPrefix = skewer/ $samp /skewer. $fqbase # use sbatch for submitting a job to the slurm queue, set options for it, # then use \"here document\" to create script on the fly, # that is submitted to slurm sbatch -o $outPrefix .slurm.log -n $threads -J skewer_ $fqbase < ( cat << EOF #!/bin/bash # initiate bash script with shebang # activate a bash profile which activates access to e.g. a tool set: . /path/to/.bash_profile skewer -z -t $threads --quiet -o $outPrefix $fq1 $fq2 # run the desired program EOF ) # list jobs with squeue command, # get job ID of previously submitted job: jid = $( squeue -o \"%j %i\" | grep skewer_ $fqbase | cut -f 2 -d \" \" ) dep_jids = \" ${ dep_jids } : $jid \" # append to list of job IDs from this sample done # end of loop over fastq files # assign variable with job IDs that the next job is going to be dependent on: dep = \"--dependency=afterok $dep_jids \" # mapping threads = 12 fastqDir = skewer/ $samp outBam = bams/raw/ $samp .mapped.bam # submit to slurm queue, with dependency on the previous jobs, # now a pre-written script which takes positional arguments sbatch -o $outBam .slurm.log -n $threads -J ${ samp } _fastq2mappedBam $dep \\ /path/to/scripts/fastq2mappedBam.sh $fastqDir $outBam $ref $bwaRef \\ $threads $tmpDir # get jobid of submitted job jid = $( squeue -o \"%j %i\" | grep ${ samp } _fastq2mappedBam | cut -f 2 -d \" \" ) dep = \"--dependency=afterok: $jid \" # dependency # HsMetrics threads = 1 inbam = bams/raw/ ${ samp } .mapped.bam outdir1 = metrics/HsMetrics/ $design mkdir -p $outdir1 sbatch -o $outdir1 / $samp . $design .hsmetrics.slurm.log -J ${ samp } _HsMetrics \\ -n $threads $dep /path/to/scripts/runHsmetrics.sh $inbam $outdir1 \\ $targets $baits $design done # end of loop over samples squeue | less -SN # check the slurm queue ls -lhtr skewer/* # list content of skewer output dir # find the log files and view them with less: find skewer/ -newermt 2021 -01-11 -name \"*slurm.log\" | xargs less -SN # grep for any error message in the log files, # remove irrelevant hits, and view the remaining results: grep -iP \"(error|fail|exception|traceback|cancel|slurm|retry|No such file or directory)\" \\ $( find skewer/* -newermt 2021 -01-11 -name \"*.slurm.log\" | sort ) | \\ grep -v \"error ratio allowed\" | less -SN # more checks of output files can be done... # make a table of columns 34 (MEAN_TARGET_COVERAGE) and 13 (FOLD_ENRICHMENT) # for all the samples # the relevant data is on line 8, with the header on line 7 # use awk to extract the relevant lines and columns # NR: row number in total; FNR: row number in file # column -t aligns the columns nicely awk -F \"\\t\" -v OFS = \"\\t\" 'NR==7 || FNR==8 {print FILENAME, $34, $13}' \\ metrics/HsMetrics/*/*txt | column -t Resulting QC metrics table: metrics/HsMetrics/PB/PB-P-HD181110-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt MEAN_TARGET_COVERAGE FOLD_ENRICHMENT metrics/HsMetrics/PB/PB-P-HD181110-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt 36390.640776 6267.968261 metrics/HsMetrics/PB/PB-P-HD181115-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt 34498.125895 6251.155829 metrics/HsMetrics/PS/PB-P-HD181110-N-4minfrag-KH20201002-PS20201005.PS.hsmetrics.txt 64569.379242 16489.157949 metrics/HsMetrics/PS/PB-P-HD181115-N-4minfrag-KH20201002-PS20201005.PS.hsmetrics.txt 67690.957504 17337.593473","title":"Command Line Usage in Real Life"},{"location":"lab_session/real_life_example_cli/#real-life-example-of-how-to-use-command-line-features","text":"This is an example of how to use various UNIX/bash/command line features for finding files, assigning variables, parsing and modifying text strings etc. The purpose in the example is to run programs that creates bam files from fastq files and then calculates some quality control metrics. This is done as a loop over multiple samples, where the same commands are applied to every sample. At the end, a table of the resulting metrics of interest is created. This is not to be run as part of the lab, just a demonstration of a real life example when multiple various command line features are utilized. cd /path/to/project_dir # move to project directory ref = /path/to/human_g1k_v37_decoy.fasta # assign path to file as variable bwaRef = /path/to/bwa/human_g1k_v37_decoy.fasta # assign path to file as variable tmpDir = /scratch/tmp/rebber # assign path to directory as variable inbox = /path/to/fastq_dir # assign path to directory as variable # find fastq files of interest, # extract the sample names from found paths with basename, # sort and assign results to array (one sample per array entry): samples =( $( find $inbox -maxdepth 1 -mindepth 1 -newermt 2021 -01-11 \\ ! -newermt 2021 -01-12 -name \"PB-P-HD*\" | xargs -I {} basename {} | sort ) ) echo ${ samples [@] } # display all entries of array echo ${# samples [@] } # display number of entries in array mkdir -p bams # create output directory # for loop over the samples: for samp in ${ samples [@] } ; do # get correct design files: # extract info from sample name (7th field, everything before \"2020\"): design = $( echo $samp | cut -f 7 -d \"-\" | sed 's/2020.*//g' ) # --> e.g. PB-P-HD181117-N-7minfrag-KH20201002-PS20201005 --> PS # assign variables depending on the value of the $design variable: if [[ $design == \"PB\" ]] ; then baits = /path/to/PB.baits.interval_list targets = /path/to/PB.targets.interval_list elif [[ $design == \"PS\" ]] ; then baits = /path/to/PS.baits.interval_list targets = /path/to/PS.targets.interval_list else echo \"Unknown design $design , skipping sample $samp \" # print message continue # skip the rest of the loop for this value of $samp fi # skewer (trimming of fastq files) threads = 6 # set variable to use for this tool dep_jids = \"\" # reset list of jobs to depend on (from previous iterations) mkdir -p skewer/ $samp #create output dir # find the \"first in pair\" fastq files for this sample: fq1s =( $( find $inbox / $samp -name \"*_1.fastq.gz\" | sort ) ) for fq1 in ${ fq1s [@] } ; do # loop over the found fastq files # get the basename of the current fastq file, # sed replaces the pattern (to nothing), # basename removes the path to the file: fqbase = $( echo $fq1 | sed 's/_1.fastq.gz//g' | xargs -I {} basename {} ) # find the corresponding second in pair fastq file, # for the current first in pair fastq file: fq2 = $( find $inbox / $samp -name ${ fqbase } _2.fastq.gz ) # assign output path and base name: outPrefix = skewer/ $samp /skewer. $fqbase # use sbatch for submitting a job to the slurm queue, set options for it, # then use \"here document\" to create script on the fly, # that is submitted to slurm sbatch -o $outPrefix .slurm.log -n $threads -J skewer_ $fqbase < ( cat << EOF #!/bin/bash # initiate bash script with shebang # activate a bash profile which activates access to e.g. a tool set: . /path/to/.bash_profile skewer -z -t $threads --quiet -o $outPrefix $fq1 $fq2 # run the desired program EOF ) # list jobs with squeue command, # get job ID of previously submitted job: jid = $( squeue -o \"%j %i\" | grep skewer_ $fqbase | cut -f 2 -d \" \" ) dep_jids = \" ${ dep_jids } : $jid \" # append to list of job IDs from this sample done # end of loop over fastq files # assign variable with job IDs that the next job is going to be dependent on: dep = \"--dependency=afterok $dep_jids \" # mapping threads = 12 fastqDir = skewer/ $samp outBam = bams/raw/ $samp .mapped.bam # submit to slurm queue, with dependency on the previous jobs, # now a pre-written script which takes positional arguments sbatch -o $outBam .slurm.log -n $threads -J ${ samp } _fastq2mappedBam $dep \\ /path/to/scripts/fastq2mappedBam.sh $fastqDir $outBam $ref $bwaRef \\ $threads $tmpDir # get jobid of submitted job jid = $( squeue -o \"%j %i\" | grep ${ samp } _fastq2mappedBam | cut -f 2 -d \" \" ) dep = \"--dependency=afterok: $jid \" # dependency # HsMetrics threads = 1 inbam = bams/raw/ ${ samp } .mapped.bam outdir1 = metrics/HsMetrics/ $design mkdir -p $outdir1 sbatch -o $outdir1 / $samp . $design .hsmetrics.slurm.log -J ${ samp } _HsMetrics \\ -n $threads $dep /path/to/scripts/runHsmetrics.sh $inbam $outdir1 \\ $targets $baits $design done # end of loop over samples squeue | less -SN # check the slurm queue ls -lhtr skewer/* # list content of skewer output dir # find the log files and view them with less: find skewer/ -newermt 2021 -01-11 -name \"*slurm.log\" | xargs less -SN # grep for any error message in the log files, # remove irrelevant hits, and view the remaining results: grep -iP \"(error|fail|exception|traceback|cancel|slurm|retry|No such file or directory)\" \\ $( find skewer/* -newermt 2021 -01-11 -name \"*.slurm.log\" | sort ) | \\ grep -v \"error ratio allowed\" | less -SN # more checks of output files can be done... # make a table of columns 34 (MEAN_TARGET_COVERAGE) and 13 (FOLD_ENRICHMENT) # for all the samples # the relevant data is on line 8, with the header on line 7 # use awk to extract the relevant lines and columns # NR: row number in total; FNR: row number in file # column -t aligns the columns nicely awk -F \"\\t\" -v OFS = \"\\t\" 'NR==7 || FNR==8 {print FILENAME, $34, $13}' \\ metrics/HsMetrics/*/*txt | column -t Resulting QC metrics table: metrics/HsMetrics/PB/PB-P-HD181110-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt MEAN_TARGET_COVERAGE FOLD_ENRICHMENT metrics/HsMetrics/PB/PB-P-HD181110-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt 36390.640776 6267.968261 metrics/HsMetrics/PB/PB-P-HD181115-CFDNA-benchmarking-KH20201002-PB20201005.PB.hsmetrics.txt 34498.125895 6251.155829 metrics/HsMetrics/PS/PB-P-HD181110-N-4minfrag-KH20201002-PS20201005.PS.hsmetrics.txt 64569.379242 16489.157949 metrics/HsMetrics/PS/PB-P-HD181115-N-4minfrag-KH20201002-PS20201005.PS.hsmetrics.txt 67690.957504 17337.593473","title":"Real life example of how to use command line features"},{"location":"lab_session/variant_calling/","text":"Calling germline- and somatic mutations \u00b6 Germline variants \u00b6 We will start by calling germline variants. For this purpose we will use the GATK HaplotypeCaller from the Broad Institute. We will use this tool on the germline DNA bam files that were processed yesterday. Active forum and tutorials for the GATK software suit is available here . The HaplotypeCaller can be run in single- or joint sample mode. The joint sample mode gives improved accuracy, especially in samples with low coverage. For simplicity, we will only use the single sample mode in this course. When haplotype caller detects variation it performes local realignment of the reads which improves variant accuracy, especially in e.g. repetitive regions. #Make sure that $GATK_REGIONS is set correctly echo $GATK_REGIONS #Create working dir for germline results if not already created mkdir -p ~/workspace/germline cd ~/workspace/germline #The argument --java-options '-Xmx12g' tells GATK to use 12GB of memory #To list the haplotype caller arguments run gatk --java-options '-Xmx12g' HaplotypeCaller --help #Call variants for exome data gatk --java-options '-Xmx12g' HaplotypeCaller \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -O ~/workspace/germline/Exome_Norm_HC_calls.vcf \\ --bam-output ~/workspace/germline/Exome_Norm_HC_out.bam $GATK_REGIONS Explore the ouput files \u00b6 Realigned bam file \u00b6 Navigate to the folder where you want to place the BAMs e.g. ~/course_bams #Copy the haplotype caller realigned bam-file to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_out.ba* . Launch IGV (remember hg38 ) - Open the bam files - Exome_Norm_sorted_mrkdup_bqsr.bam (intput file to haplotype caller). - Exome_Norm_HC_out.bam (realigned bam file by haplotype caller). - Navigate to chr17:76286974-76287203. - Color alignments by: no color. - Enable ruler in the menue bar. - In the first base of the indel: Sort aligments according to base. - Select squished view. - Zoom out. - Try to answer the following: - What characterises misaligned variant-supporting reads in non-realigned bam file? - Are there any false positive variants take in the non-realigned bam file? - OBS : Leave IGV open, we will use it in the below section. VCF (variant call format) file \u00b6 #Make sure you hare in the same directory as the output files cd ~/workspace/germline #List files ls -halt #Open the VCF file less -SN Exome_Norm_HC_calls.vcf The VCF file format is quite complex, the specification can be found here . Try to do the following: - Identify the VCF header section. - Identify the data column header. - Find the variant on chr17 pos 106742 in the VCF file. - Lead, type /106742 and press ENTER - What is the read depth in this position? - Is it a high quality variant? - Go go IGV in this position - Select expanded view. - Color alignments by read strand. - Sort alignments by base. - Does the visual impression in IGV support the quality of the variant? - Identify a way to quickly separate SNPs and INDELs in the VCF. - Find the variant on chr17 pos 1787008 in the VCF file - Is this a high quality variant? - Navgiate in IGV to the position. - Try to explain the information in the VCF to what is shown in IGV, is it correct? Filtering germline variants \u00b6 As noted above, the raw output of GATK HaplotypeCaller will include many variants with varying degrees of quality. For various reasons we might wish to further filter these to a higher confidence set of variants. The hard-filtering approached applied here is futher described in detail at the GATK webside . Separate SNPs from Indels in the variant call set \u00b6 First, we will separate out the SNPs and Indels from the VCF into new separate VCFs. Note that the variant type (SNP, INDEL, MIXED, etc) is not stored explicitly in the vcf but instead inferred from the genotypes. We will use a versatile GATK tool called SelectVariants . This command can be used for all kinds of simple filtering or subsetting purposes. We will run it twice to select by variant type, once for SNPs, and then again for Indels, to produce two new VCFs. cd ~/workspace/germline/ gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type SNP \\ -O ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type INDEL \\ -O ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf Apply filters to the SNP and Indel call sets \u00b6 Next, we will perform so-called hard-filtering by applying a number of cutoffs. Multiple filters can be combined arbitrarily. Each can be given its own name so that you can later determine which one or more filters a variant fails. Visit the GATK documentation on hard-filtering to learn more about the following hard filtering options. Notice that different filters and cutoffs are recommended for SNVs and Indels. This is why we first split them into separate files. cd ~/workspace/germline/ gatk --java-options '-Xmx12g' VariantFiltration \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD_lt_2\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS_gt_60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ_lt_40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRS_lt_n12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"RPRS_lt_n8\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR_gt_3\" \\ -O ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf gatk --java-options '-Xmx12g' VariantFiltration \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD_lt_2\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS_gt_200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"RPRS_lt_n20\" \\ --filter-expression \"SOR > 10.0\" --filter-name \"SOR_gt_10\" \\ -O ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf Notice that warnings are given with regards to MQRankSum and ReadPosRankSum, these are only calculated if the site is called as heterozygous. E.g. for this site (inspect in IGV and in the VCF file): - chr17:1067361 This was discussed at the GATK forum . The variants are now marked as PASS or or filter-name . Use grep -v and head to skip past all the VCF header lines and view the first few records. #inspect the 10 first variants that did not pass grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -vP \"\\tPASS\\t\" | head -10 #inspect the 10 first variants that did pass grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -P \"\\tPASS\\t\" | head -10 Try to do the following: - Count the number of variants that failed and passed filtering. Merge filtered SNP and INDEL vcfs back together \u00b6 gatk --java-options '-Xmx12g' MergeVcfs \\ -I ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf \\ -I ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf \\ -O ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf #Nbr of variants in the SNP file grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | wc -l #Nbr of variants in the INDEL file grep -v \"##\" Exome_Norm_HC_calls.indels.filtered.vcf | wc -l #Nbr of variants in the merged file grep -v \"##\" Exome_Norm_HC_calls.filtered.vcf | wc -l Extract PASS variants only \u00b6 It would also be convenient to have a vcf with only passing variants. For this, we can go back the GATK SelectVariants tool. This will be run much as above except with the --exlude-filtered option instead of -select-type . gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf \\ -O ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\ --exclude-filtered Perform annotation of filtered variants \u00b6 Now that we have high-confidence, filtered variants, we want to start understanding which of these variants might be clinically or biologically relevant. Ensembl's Variant Effect Predictor (VEP) annotation software is a powerful tool for annotating variants with a great deal of biological features. This includes such information as protein consequence (non-coding or coding), population frequencies, links to external databases, various scores designed to estimate the importance of individual variants on protein function, and much more. Note, the first time you run VEP it will create a fasta index for the reference genome in VEP cache. Therefore, it will take longer to run that first time but should speed up substantially for subsequent runs on files with similar numbers of variants. #VEP annotate hard-filtered exome results ####### Already done code below #cd ~/workspace/vep_cache/ #wget https://github.com/Ensembl/VEP_plugins/archive/refs/heads/release/104.zip #unzip 104.zip #ls -halt ~/workspace/vep_cache/ #Already done #mv VEP_plugins-release-104 Plugins #ls -halt ~/workspace/vep_cache/Plugins #get annotation fasta for homo_sapiens #cd ~/workspace/vep_cache/homo_sapiens #Already done #wget ftp://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.toplevel.fa.gz ####### Start here cd ~/workspace/germline #Remember, we are using the REF ~/workspace/inputs/references/genome/ref_genome.fa which only contains chr6 and chr17. nohup vep --cache --dir_cache ~/workspace/vep_cache \\ --dir_plugins ~/workspace/vep_cache/Plugins \\ --fasta ~/workspace/inputs/references/genome/ref_genome.fa \\ --fork 8 --assembly = GRCh38 --offline --vcf \\ -i ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\ -o ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf \\ --check_existing --total_length --allele_number --no_escape --everything \\ --use_given_ref --force_overwrite --coding_only & #Open the VEP vcf, row 39 and 40, contains information about the VEP annotation. less -SN Exome_Norm_HC_calls.filtered.PASS.vep.vcf #Or just grep the VEP info. grep \"##\" Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"VEP\" | less -SN #Copy the html VEP summary to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf_summary.html . Open the HTML file. Answer the following: - How many variants were processed by VEP? - What is the most common kind of consequence? - Why are only variants the coding regions detected? - Open the Exome_Norm_HC_out.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV - Find a missense variant and examine it in IGV, does the variant look real? - E.g. take chr17 744946 - Find the variant in the VEP VCF as well as IGV. - VEP assigns each variant a consequence types, as described here . - What is the difference between splice region variants and splice donor/acceptor variants? - The --pick option is used, as described here , do you think it makes sense to use? Somatic variants \u00b6 We will run multiple variant callers and merging the callset. Varscan \u00b6 First off is VARSCAN , that employs a robust heuristic/statistic approach to call variants that meet desired thresholds for read depth, base quality, variant allele frequency, and statistical significance. As seen below varscan uses the mpileup command from samtools. What the mpileup command does can be explored here mkdir -p ~/workspace/somatic/varscan cd ~/workspace/somatic/varscan #Have a look at the input data to the varscan caller samtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\ --no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\ ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam | less -SN # Run varscan java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar somatic \\ < ( samtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\ --no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\ ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam ) \\ ~/workspace/somatic/varscan/exome --min-var-freq 0 .05 --mpileup 1 --output-vcf ls -halt java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\ exome.snp.vcf exome.snp --min-tumor-freq 0 .05 --max-normal-freq 0 .01 java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\ exome.indel.vcf exome.indel --min-tumor-freq 0 .05 --max-normal-freq 0 .01 find ~/workspace/somatic/varscan -name '*.vcf' -exec bgzip -f {} \\; find ~/workspace/somatic/varscan -name '*.vcf.gz' -exec tabix -f {} \\; gatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V exome.snp.Somatic.vcf.gz --mask exome.snp.Somatic.hc.vcf.gz \\ --mask-name \"processSomatic\" --filter-not-in-mask -O exome.snp.Somatic.hc.filter.vcf.gz gatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V exome.indel.Somatic.vcf.gz --mask exome.indel.Somatic.hc.vcf.gz \\ --mask-name \"processSomatic\" --filter-not-in-mask -O exome.indel.Somatic.hc.filter.vcf.gz bcftools concat -a -o exome.vcf.gz -O z exome.snp.Somatic.hc.filter.vcf.gz exome.indel.Somatic.hc.filter.vcf.gz tabix -f ~/workspace/somatic/varscan/exome.vcf.gz Strelka \u00b6 Now we are going to run the second variant caller, STRELKA . Strelka calls germline and somatic small variants from mapped sequencing reads and is optimized for rapid clinical analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs. Both germline and somatic callers include a final empirical variant rescoring step using a random forest model to reflect numerous features indicative of call reliability which may not be represented in the core variant calling probability model. mkdir -p ~/workspace/somatic/strelka/exome cd ~ source activate strelka-env ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py \\ --normalBam = workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ --tumorBam = workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\ --referenceFasta = workspace/inputs/references/genome/ref_genome.fa \\ --exome --runDir = workspace/somatic/strelka/exome #Please specify according to the number of cpus available or how many you would like to allocate to this job. In this case, four were given. # Runtime: ~ 3min python2 ~/workspace/somatic/strelka/exome/runWorkflow.py -m local -j 8 conda deactivate cd ~/workspace/somatic/strelka/exome/results/variants zcat somatic.snvs.vcf.gz | \\ awk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\ > somatic.snvs.gt.vcf zcat somatic.indels.vcf.gz | \\ awk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\ > somatic.indels.gt.vcf find ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf\" -exec bgzip -f {} \\; find ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf.gz\" -exec tabix -f {} \\; bcftools concat -a -o exome.vcf.gz -O z somatic.snvs.gt.vcf.gz somatic.indels.gt.vcf.gz tabix exome.vcf.gz Mutect2 \u00b6 Last up is MuTect2 . MuTect2 is a somatic SNP and indel caller that combines the DREAM challenge-winning somatic genotyping engine of the original MuTect (Cibulskis et al., 2013) with the assembly-based machinery of HaplotypeCaller. Exome data commands \u00b6 #Obtaining germline resource from GATK cd ~/workspace/inputs/references #Already done #gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz . #gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz.tbi . mkdir -p ~/workspace/somatic/mutect cd ~/workspace/somatic/mutect #Creating a panel of normals # Runtime: ~ 17min gatk --java-options \"-Xmx12G\" Mutect2 \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -tumor-sample HCC1395BL_DNA -O Exome_Norm_PON.vcf.gz #Running Mutect2 Using latest version of GATK # Runtime: ~20m gatk --java-options \"-Xmx12G\" Mutect2 \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\ -tumor HCC1395_DNA -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -normal HCC1395BL_DNA --germline-resource ~/workspace/inputs/references/af-only-gnomad.hg38.vcf.gz \\ --af-of-alleles-not-in-resource 0 .00003125 --panel-of-normals \\ ~/workspace/somatic/mutect/Exome_Norm_PON.vcf.gz -O ~/workspace/somatic/mutect/exome.vcf.gz \\ -L chr6 -L chr17 # Filtering mutect variants gatk --java-options \"-Xmx12G\" FilterMutectCalls -V ~/workspace/somatic/mutect/exome.vcf.gz \\ -O ~/workspace/somatic/mutect/exome_filtered.vcf.gz \\ -R ~/workspace/inputs/references/genome/ref_genome.fa #Running mutect2 using gatk version 3.6 #java -Xmx12g -jar /usr/local/bin/GenomeAnalysisTK.jar -T MuTect2 --disable_auto_index_creation_and_locking_when_reading_rods -R ~/workspace/data/raw_data/references/ref_genome.fa -I:tumor ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Tumor_sorted_mrkdup_bqsr.bam -I:Normal ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Norm_sorted_mrkdup_bqsr.bam --dbsnp ~/workspace/data/raw_data/references/Homo_sapiens_assembly38.dbsnp138.vcf.gz --cosmic ~/workspace/data/raw_data/references/Cosmic_v79.dictsorted.vcf.gz -o ~/workspace/data/results/somatic/mutect/exome.vcf.gz -L ~/workspace/data/results/inputs/SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.interval_list echo ~/workspace/somatic/mutect/exome_filtered.vcf.gz > ~/workspace/somatic/mutect/exome_vcf.fof bcftools concat --allow-overlaps --remove-duplicates \\ --file-list ~/workspace/somatic/mutect/exome_vcf.fof --output-type z \\ --output ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz tabix ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz Now we are going to merge the variants detected from all three variant callers \u00b6 The reason for merging is that the variant callers are working differently internally to identify somatic alterations, with different strengths and weaknesses. With outputs from all three algorithms, we can now merge the variants to generate a comprehensive list of detected variants: # Unzip the vcf.gz files before combining Variants cd ~/workspace/somatic ls -halt ~/workspace/somatic/varscan/exome.vcf.gz ls -halt ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz ls -halt ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz gunzip ~/workspace/somatic/varscan/exome.vcf.gz gunzip ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz gunzip ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz #Need to change header sample names in vcf file produced by mutect2 in order to combine variants with those from other algorithms sed -i 's/HCC1395BL_DNA/NORMAL/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf sed -i 's/HCC1395_DNA/TUMOR/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf # (UNIQUIFY command) java -Xmx4g -jar /usr/local/bin/GenomeAnalysisTK.jar -T CombineVariants -R ~/workspace/data/raw_data/references/ref_genome.fa -genotypeMergeOptions UNIQUIFY --variant:varscan ~/workspace/data/results/somatic/varscan/exome.vcf --variant:strelka ~/workspace/data/results/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/data/results/somatic/mutect/new_gatk_files/exome.vcf -o ~/workspace/data/results/somatic/exome.unique.vcf.gz #java -Xmx24g -jar ~/workspace/bin/GenomeAnalysisTK.jar -T CombineVariants -R ~/workspace/inputs/references/genome/ref_genome.fa -genotypeMergeOptions PRIORITIZE --rod_priority_list mutect,varscan,strelka --variant:varscan ~/workspace/somatic/varscan/exome.vcf --variant:strelka ~/workspace/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/somatic/mutect/exome.vcf -o ~/workspace/somatic/exome.merged.vcf.gz java -Xmx12g -jar ~/workspace/bin/picard.jar MergeVcfs \\ -I ~/workspace/somatic/varscan/exome.vcf \\ -I ~/workspace/somatic/strelka/exome/results/variants/exome.vcf \\ -I ~/workspace/somatic/mutect/mutect_exome_filtered.vcf \\ -O ~/workspace/somatic/exome.merged.vcf bgzip -c ~/workspace/somatic/exome.merged.vcf > ~/workspace/somatic/exome.merged.vcf.gz tabix -p vcf ~/workspace/somatic/exome.merged.vcf.gz Left Align and Trim \u00b6 The reason for left align the variants and trim then is explained here . cd ~/workspace/somatic/ gatk --java-options \"-Xmx12G\" LeftAlignAndTrimVariants \\ -V ~/workspace/somatic/exome.merged.vcf.gz \\ -O exome.merged.leftalignandtrim.vcf \\ -R ~/workspace/inputs/references/genome/ref_genome.fa Note that when running on chromosome 6 and 17 merged variants file, this gave 0 variants aligned. We will split multi-allelic variants into multiple records \u00b6 cd ~/workspace/somatic/ vt decompose -s ~/workspace/somatic/exome.merged.leftalignandtrim.vcf \\ -o ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf Basic Filtering on Somatic Variants \u00b6 First, let's do a basic filtering for PASS only variants on our merged and normalized vcf file: cd ~/workspace/somatic gatk --java-options \"-Xmx12G\" SelectVariants -R ~/workspace/inputs/references/genome/ref_genome.fa \\ --exclude-filtered -V ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf \\ -O ~/workspace/somatic/exome.merged.norm.pass_only.vcf Annotation with VEP. \u00b6 Again, we will use VEP to annotate the somatic variants as we did for the germline variants. cd ~/workspace/somatic # Runtime: ~4min #ssh -o ServerAliveInterval=300 -i course_EC2_01.pem ubuntu@ec2-52-23-206-90.compute-1.amazonaws.com #source .bashrc #cd workspace/somatic/ nohup vep --cache --dir_cache ~/workspace/vep_cache \\ --dir_plugins ~/workspace/vep_cache/Plugins \\ --fasta ~/workspace/inputs/references/genome/ref_genome.fa --fork 8 \\ --assembly = GRCh38 --offline --vcf -i ~/workspace/somatic/exome.merged.norm.pass_only.vcf \\ -o ~/workspace/somatic/exome.merged.norm.annotated.vcf \\ --check_existing --total_length --allele_number --no_escape --everything \\ --use_given_ref --force_overwrite & #Copy the html VEP summary to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/somatic/exome.merged.norm.annotated.vcf_summary.html . Open the HTML file. Answer the following: - How many variants were processed by VEP? - Any diffrence in the distribution of variants vs. the germline variants? - The variant categories e.g. intron_variant and regulatory_region_variant should be approached carefully, why? Inspecting variants in IGV \u00b6 Download the bam files to your local machine scp -ri course_EC2_01.pem ubuntu@ec2-54-163-59-166.compute-1.amazonaws.com:~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.* . scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.* . Open the Exome_Tumor_sorted_mrkdup_bqsr.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV. Keep the VCF file open in a terminal window on AWS Try to find a relevant variant in TP53 in the VCF file on AWS #Can be done in many ways, this is just an example. cd ~/workspace/somatic grep \"TP53\" exome.merged.norm.annotated.vcf | less -SN #### Or less -SN exome.merged.norm.annotated.vcf #In the less window type /TP53 Why is the TP53 variant in three rows in the VCF file? lead: how many somatic variant callers were run? Go to the position (chr17 7675088) on IGV in your local machine Is it present in the normal DNA? Do you still think it is valid? How come the variant can be present in the germline DNA? Do the same thing for a stop_gained and a frameshift variant cd ~/workspace/somatic grep \"stop_gained\" exome.merged.norm.annotated.vcf | less -SN The top variant is seen only once, this is just identified correctly by one caller because it changes two bases in a row open chr6 1930220 in IGV let us convert the vcf-file to a MAF file format note all information is not kept but it simplifies looking at the variants cd ~/workspace/somatic perl ~/workspace/bin/mskcc-vcf2maf/vcf2maf.pl --input-vcf ~/workspace/somatic/exome.merged.norm.annotated.vcf --output-maf ~/workspace/somatic/exome.merged.norm.annotated.maf --tumor-id TUMOR --normal-id NORMAL --inhibit-vep --ref-fasta ~/workspace/inputs/references/genome/ref_genome.fa less -SN ~/workspace/somatic/exome.merged.norm.annotated.maf #Count the number of variant types cut -f9 ~/workspace/somatic/exome.merged.norm.annotated.maf | sort | uniq -c #As you see the variant nomenclature is not the same in VCF and MAF grep Nonsense_Mutation ~/workspace/somatic/exome.merged.norm.annotated.maf | less -SN Have a look at the nosense variant in CCDC40 in IGV chr17 80050160 What is the VAF? What is the VAF of the TP53 variant? Reflections? Lead: clonality Can you find a variant in BRCA1? What is the impact? Is it relevant? There was a BRCA 1 variant in the germline that we did not dicsuss #Let us grep \"BRCA1\" and send the output to another grep command and take the \"HIGH\" impact variants grep BRCA1 ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"HIGH\" | less -SN let us inspect this variant in IGV, why is one allel missing in the tumor? This is the variant location: chr17 43057078","title":"Variant Calling"},{"location":"lab_session/variant_calling/#calling-germline-and-somatic-mutations","text":"","title":"Calling germline- and somatic mutations"},{"location":"lab_session/variant_calling/#germline-variants","text":"We will start by calling germline variants. For this purpose we will use the GATK HaplotypeCaller from the Broad Institute. We will use this tool on the germline DNA bam files that were processed yesterday. Active forum and tutorials for the GATK software suit is available here . The HaplotypeCaller can be run in single- or joint sample mode. The joint sample mode gives improved accuracy, especially in samples with low coverage. For simplicity, we will only use the single sample mode in this course. When haplotype caller detects variation it performes local realignment of the reads which improves variant accuracy, especially in e.g. repetitive regions. #Make sure that $GATK_REGIONS is set correctly echo $GATK_REGIONS #Create working dir for germline results if not already created mkdir -p ~/workspace/germline cd ~/workspace/germline #The argument --java-options '-Xmx12g' tells GATK to use 12GB of memory #To list the haplotype caller arguments run gatk --java-options '-Xmx12g' HaplotypeCaller --help #Call variants for exome data gatk --java-options '-Xmx12g' HaplotypeCaller \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -O ~/workspace/germline/Exome_Norm_HC_calls.vcf \\ --bam-output ~/workspace/germline/Exome_Norm_HC_out.bam $GATK_REGIONS","title":"Germline variants"},{"location":"lab_session/variant_calling/#explore-the-ouput-files","text":"","title":"Explore the ouput files"},{"location":"lab_session/variant_calling/#realigned-bam-file","text":"Navigate to the folder where you want to place the BAMs e.g. ~/course_bams #Copy the haplotype caller realigned bam-file to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_out.ba* . Launch IGV (remember hg38 ) - Open the bam files - Exome_Norm_sorted_mrkdup_bqsr.bam (intput file to haplotype caller). - Exome_Norm_HC_out.bam (realigned bam file by haplotype caller). - Navigate to chr17:76286974-76287203. - Color alignments by: no color. - Enable ruler in the menue bar. - In the first base of the indel: Sort aligments according to base. - Select squished view. - Zoom out. - Try to answer the following: - What characterises misaligned variant-supporting reads in non-realigned bam file? - Are there any false positive variants take in the non-realigned bam file? - OBS : Leave IGV open, we will use it in the below section.","title":"Realigned bam file"},{"location":"lab_session/variant_calling/#vcf-variant-call-format-file","text":"#Make sure you hare in the same directory as the output files cd ~/workspace/germline #List files ls -halt #Open the VCF file less -SN Exome_Norm_HC_calls.vcf The VCF file format is quite complex, the specification can be found here . Try to do the following: - Identify the VCF header section. - Identify the data column header. - Find the variant on chr17 pos 106742 in the VCF file. - Lead, type /106742 and press ENTER - What is the read depth in this position? - Is it a high quality variant? - Go go IGV in this position - Select expanded view. - Color alignments by read strand. - Sort alignments by base. - Does the visual impression in IGV support the quality of the variant? - Identify a way to quickly separate SNPs and INDELs in the VCF. - Find the variant on chr17 pos 1787008 in the VCF file - Is this a high quality variant? - Navgiate in IGV to the position. - Try to explain the information in the VCF to what is shown in IGV, is it correct?","title":"VCF (variant call format) file"},{"location":"lab_session/variant_calling/#filtering-germline-variants","text":"As noted above, the raw output of GATK HaplotypeCaller will include many variants with varying degrees of quality. For various reasons we might wish to further filter these to a higher confidence set of variants. The hard-filtering approached applied here is futher described in detail at the GATK webside .","title":"Filtering germline variants"},{"location":"lab_session/variant_calling/#separate-snps-from-indels-in-the-variant-call-set","text":"First, we will separate out the SNPs and Indels from the VCF into new separate VCFs. Note that the variant type (SNP, INDEL, MIXED, etc) is not stored explicitly in the vcf but instead inferred from the genotypes. We will use a versatile GATK tool called SelectVariants . This command can be used for all kinds of simple filtering or subsetting purposes. We will run it twice to select by variant type, once for SNPs, and then again for Indels, to produce two new VCFs. cd ~/workspace/germline/ gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type SNP \\ -O ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.vcf -select-type INDEL \\ -O ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf","title":"Separate SNPs from Indels in the variant call set"},{"location":"lab_session/variant_calling/#apply-filters-to-the-snp-and-indel-call-sets","text":"Next, we will perform so-called hard-filtering by applying a number of cutoffs. Multiple filters can be combined arbitrarily. Each can be given its own name so that you can later determine which one or more filters a variant fails. Visit the GATK documentation on hard-filtering to learn more about the following hard filtering options. Notice that different filters and cutoffs are recommended for SNVs and Indels. This is why we first split them into separate files. cd ~/workspace/germline/ gatk --java-options '-Xmx12g' VariantFiltration \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.snps.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD_lt_2\" \\ --filter-expression \"FS > 60.0\" --filter-name \"FS_gt_60\" \\ --filter-expression \"MQ < 40.0\" --filter-name \"MQ_lt_40\" \\ --filter-expression \"MQRankSum < -12.5\" --filter-name \"MQRS_lt_n12.5\" \\ --filter-expression \"ReadPosRankSum < -8.0\" --filter-name \"RPRS_lt_n8\" \\ --filter-expression \"SOR > 3.0\" --filter-name \"SOR_gt_3\" \\ -O ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf gatk --java-options '-Xmx12g' VariantFiltration \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.indels.vcf \\ --filter-expression \"QD < 2.0\" --filter-name \"QD_lt_2\" \\ --filter-expression \"FS > 200.0\" --filter-name \"FS_gt_200\" \\ --filter-expression \"ReadPosRankSum < -20.0\" --filter-name \"RPRS_lt_n20\" \\ --filter-expression \"SOR > 10.0\" --filter-name \"SOR_gt_10\" \\ -O ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf Notice that warnings are given with regards to MQRankSum and ReadPosRankSum, these are only calculated if the site is called as heterozygous. E.g. for this site (inspect in IGV and in the VCF file): - chr17:1067361 This was discussed at the GATK forum . The variants are now marked as PASS or or filter-name . Use grep -v and head to skip past all the VCF header lines and view the first few records. #inspect the 10 first variants that did not pass grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -vP \"\\tPASS\\t\" | head -10 #inspect the 10 first variants that did pass grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | grep -P \"\\tPASS\\t\" | head -10 Try to do the following: - Count the number of variants that failed and passed filtering.","title":"Apply filters to the SNP and Indel call sets"},{"location":"lab_session/variant_calling/#merge-filtered-snp-and-indel-vcfs-back-together","text":"gatk --java-options '-Xmx12g' MergeVcfs \\ -I ~/workspace/germline/Exome_Norm_HC_calls.snps.filtered.vcf \\ -I ~/workspace/germline/Exome_Norm_HC_calls.indels.filtered.vcf \\ -O ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf #Nbr of variants in the SNP file grep -v \"##\" Exome_Norm_HC_calls.snps.filtered.vcf | wc -l #Nbr of variants in the INDEL file grep -v \"##\" Exome_Norm_HC_calls.indels.filtered.vcf | wc -l #Nbr of variants in the merged file grep -v \"##\" Exome_Norm_HC_calls.filtered.vcf | wc -l","title":"Merge filtered SNP and INDEL vcfs back together"},{"location":"lab_session/variant_calling/#extract-pass-variants-only","text":"It would also be convenient to have a vcf with only passing variants. For this, we can go back the GATK SelectVariants tool. This will be run much as above except with the --exlude-filtered option instead of -select-type . gatk --java-options '-Xmx12g' SelectVariants \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V ~/workspace/germline/Exome_Norm_HC_calls.filtered.vcf \\ -O ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\ --exclude-filtered","title":"Extract PASS variants only"},{"location":"lab_session/variant_calling/#perform-annotation-of-filtered-variants","text":"Now that we have high-confidence, filtered variants, we want to start understanding which of these variants might be clinically or biologically relevant. Ensembl's Variant Effect Predictor (VEP) annotation software is a powerful tool for annotating variants with a great deal of biological features. This includes such information as protein consequence (non-coding or coding), population frequencies, links to external databases, various scores designed to estimate the importance of individual variants on protein function, and much more. Note, the first time you run VEP it will create a fasta index for the reference genome in VEP cache. Therefore, it will take longer to run that first time but should speed up substantially for subsequent runs on files with similar numbers of variants. #VEP annotate hard-filtered exome results ####### Already done code below #cd ~/workspace/vep_cache/ #wget https://github.com/Ensembl/VEP_plugins/archive/refs/heads/release/104.zip #unzip 104.zip #ls -halt ~/workspace/vep_cache/ #Already done #mv VEP_plugins-release-104 Plugins #ls -halt ~/workspace/vep_cache/Plugins #get annotation fasta for homo_sapiens #cd ~/workspace/vep_cache/homo_sapiens #Already done #wget ftp://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.toplevel.fa.gz ####### Start here cd ~/workspace/germline #Remember, we are using the REF ~/workspace/inputs/references/genome/ref_genome.fa which only contains chr6 and chr17. nohup vep --cache --dir_cache ~/workspace/vep_cache \\ --dir_plugins ~/workspace/vep_cache/Plugins \\ --fasta ~/workspace/inputs/references/genome/ref_genome.fa \\ --fork 8 --assembly = GRCh38 --offline --vcf \\ -i ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vcf \\ -o ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf \\ --check_existing --total_length --allele_number --no_escape --everything \\ --use_given_ref --force_overwrite --coding_only & #Open the VEP vcf, row 39 and 40, contains information about the VEP annotation. less -SN Exome_Norm_HC_calls.filtered.PASS.vep.vcf #Or just grep the VEP info. grep \"##\" Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"VEP\" | less -SN #Copy the html VEP summary to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf_summary.html . Open the HTML file. Answer the following: - How many variants were processed by VEP? - What is the most common kind of consequence? - Why are only variants the coding regions detected? - Open the Exome_Norm_HC_out.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV - Find a missense variant and examine it in IGV, does the variant look real? - E.g. take chr17 744946 - Find the variant in the VEP VCF as well as IGV. - VEP assigns each variant a consequence types, as described here . - What is the difference between splice region variants and splice donor/acceptor variants? - The --pick option is used, as described here , do you think it makes sense to use?","title":"Perform annotation of filtered variants"},{"location":"lab_session/variant_calling/#somatic-variants","text":"We will run multiple variant callers and merging the callset.","title":"Somatic variants"},{"location":"lab_session/variant_calling/#varscan","text":"First off is VARSCAN , that employs a robust heuristic/statistic approach to call variants that meet desired thresholds for read depth, base quality, variant allele frequency, and statistical significance. As seen below varscan uses the mpileup command from samtools. What the mpileup command does can be explored here mkdir -p ~/workspace/somatic/varscan cd ~/workspace/somatic/varscan #Have a look at the input data to the varscan caller samtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\ --no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\ ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam | less -SN # Run varscan java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar somatic \\ < ( samtools mpileup -l ~/workspace/inputs/references/exome/exome_regions.bed \\ --no-BAQ -f ~/workspace/inputs/references/genome/ref_genome.fa \\ ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam ) \\ ~/workspace/somatic/varscan/exome --min-var-freq 0 .05 --mpileup 1 --output-vcf ls -halt java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\ exome.snp.vcf exome.snp --min-tumor-freq 0 .05 --max-normal-freq 0 .01 java -Xmx12g -jar ~/workspace/bin/VarScan.v2.4.2.jar processSomatic \\ exome.indel.vcf exome.indel --min-tumor-freq 0 .05 --max-normal-freq 0 .01 find ~/workspace/somatic/varscan -name '*.vcf' -exec bgzip -f {} \\; find ~/workspace/somatic/varscan -name '*.vcf.gz' -exec tabix -f {} \\; gatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V exome.snp.Somatic.vcf.gz --mask exome.snp.Somatic.hc.vcf.gz \\ --mask-name \"processSomatic\" --filter-not-in-mask -O exome.snp.Somatic.hc.filter.vcf.gz gatk VariantFiltration -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -V exome.indel.Somatic.vcf.gz --mask exome.indel.Somatic.hc.vcf.gz \\ --mask-name \"processSomatic\" --filter-not-in-mask -O exome.indel.Somatic.hc.filter.vcf.gz bcftools concat -a -o exome.vcf.gz -O z exome.snp.Somatic.hc.filter.vcf.gz exome.indel.Somatic.hc.filter.vcf.gz tabix -f ~/workspace/somatic/varscan/exome.vcf.gz","title":"Varscan"},{"location":"lab_session/variant_calling/#strelka","text":"Now we are going to run the second variant caller, STRELKA . Strelka calls germline and somatic small variants from mapped sequencing reads and is optimized for rapid clinical analysis of germline variation in small cohorts and somatic variation in tumor/normal sample pairs. Both germline and somatic callers include a final empirical variant rescoring step using a random forest model to reflect numerous features indicative of call reliability which may not be represented in the core variant calling probability model. mkdir -p ~/workspace/somatic/strelka/exome cd ~ source activate strelka-env ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py \\ --normalBam = workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ --tumorBam = workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\ --referenceFasta = workspace/inputs/references/genome/ref_genome.fa \\ --exome --runDir = workspace/somatic/strelka/exome #Please specify according to the number of cpus available or how many you would like to allocate to this job. In this case, four were given. # Runtime: ~ 3min python2 ~/workspace/somatic/strelka/exome/runWorkflow.py -m local -j 8 conda deactivate cd ~/workspace/somatic/strelka/exome/results/variants zcat somatic.snvs.vcf.gz | \\ awk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\ > somatic.snvs.gt.vcf zcat somatic.indels.vcf.gz | \\ awk '{if(/^##/) print; else if(/^#/) print \"##FORMAT=<ID=GT,Number=1,Type=String,Description=\\\"Genotype\\\">\\n\"$0; else print $1\"\\t\"$2\"\\t\"$3\"\\t\"$4\"\\t\"$5\"\\t\"$6\"\\t\"$7\"\\t\"$8\"\\tGT:\"$9\"\\t./.:\"$10\"\\t./.:\"$11;}' - \\ > somatic.indels.gt.vcf find ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf\" -exec bgzip -f {} \\; find ~/workspace/somatic/strelka/exome/results/variants/ -name \"*.vcf.gz\" -exec tabix -f {} \\; bcftools concat -a -o exome.vcf.gz -O z somatic.snvs.gt.vcf.gz somatic.indels.gt.vcf.gz tabix exome.vcf.gz","title":"Strelka"},{"location":"lab_session/variant_calling/#mutect2","text":"Last up is MuTect2 . MuTect2 is a somatic SNP and indel caller that combines the DREAM challenge-winning somatic genotyping engine of the original MuTect (Cibulskis et al., 2013) with the assembly-based machinery of HaplotypeCaller.","title":"Mutect2"},{"location":"lab_session/variant_calling/#exome-data-commands","text":"#Obtaining germline resource from GATK cd ~/workspace/inputs/references #Already done #gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz . #gsutil cp gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz.tbi . mkdir -p ~/workspace/somatic/mutect cd ~/workspace/somatic/mutect #Creating a panel of normals # Runtime: ~ 17min gatk --java-options \"-Xmx12G\" Mutect2 \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -tumor-sample HCC1395BL_DNA -O Exome_Norm_PON.vcf.gz #Running Mutect2 Using latest version of GATK # Runtime: ~20m gatk --java-options \"-Xmx12G\" Mutect2 \\ -R ~/workspace/inputs/references/genome/ref_genome.fa \\ -I ~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.bam \\ -tumor HCC1395_DNA -I ~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.bam \\ -normal HCC1395BL_DNA --germline-resource ~/workspace/inputs/references/af-only-gnomad.hg38.vcf.gz \\ --af-of-alleles-not-in-resource 0 .00003125 --panel-of-normals \\ ~/workspace/somatic/mutect/Exome_Norm_PON.vcf.gz -O ~/workspace/somatic/mutect/exome.vcf.gz \\ -L chr6 -L chr17 # Filtering mutect variants gatk --java-options \"-Xmx12G\" FilterMutectCalls -V ~/workspace/somatic/mutect/exome.vcf.gz \\ -O ~/workspace/somatic/mutect/exome_filtered.vcf.gz \\ -R ~/workspace/inputs/references/genome/ref_genome.fa #Running mutect2 using gatk version 3.6 #java -Xmx12g -jar /usr/local/bin/GenomeAnalysisTK.jar -T MuTect2 --disable_auto_index_creation_and_locking_when_reading_rods -R ~/workspace/data/raw_data/references/ref_genome.fa -I:tumor ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Tumor_sorted_mrkdup_bqsr.bam -I:Normal ~/workspace/data/DNA_alignments/chr6+chr17/final/Exome_Norm_sorted_mrkdup_bqsr.bam --dbsnp ~/workspace/data/raw_data/references/Homo_sapiens_assembly38.dbsnp138.vcf.gz --cosmic ~/workspace/data/raw_data/references/Cosmic_v79.dictsorted.vcf.gz -o ~/workspace/data/results/somatic/mutect/exome.vcf.gz -L ~/workspace/data/results/inputs/SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.interval_list echo ~/workspace/somatic/mutect/exome_filtered.vcf.gz > ~/workspace/somatic/mutect/exome_vcf.fof bcftools concat --allow-overlaps --remove-duplicates \\ --file-list ~/workspace/somatic/mutect/exome_vcf.fof --output-type z \\ --output ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz tabix ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz","title":"Exome data commands"},{"location":"lab_session/variant_calling/#now-we-are-going-to-merge-the-variants-detected-from-all-three-variant-callers","text":"The reason for merging is that the variant callers are working differently internally to identify somatic alterations, with different strengths and weaknesses. With outputs from all three algorithms, we can now merge the variants to generate a comprehensive list of detected variants: # Unzip the vcf.gz files before combining Variants cd ~/workspace/somatic ls -halt ~/workspace/somatic/varscan/exome.vcf.gz ls -halt ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz ls -halt ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz gunzip ~/workspace/somatic/varscan/exome.vcf.gz gunzip ~/workspace/somatic/strelka/exome/results/variants/exome.vcf.gz gunzip ~/workspace/somatic/mutect/mutect_exome_filtered.vcf.gz #Need to change header sample names in vcf file produced by mutect2 in order to combine variants with those from other algorithms sed -i 's/HCC1395BL_DNA/NORMAL/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf sed -i 's/HCC1395_DNA/TUMOR/' ~/workspace/somatic/mutect/mutect_exome_filtered.vcf # (UNIQUIFY command) java -Xmx4g -jar /usr/local/bin/GenomeAnalysisTK.jar -T CombineVariants -R ~/workspace/data/raw_data/references/ref_genome.fa -genotypeMergeOptions UNIQUIFY --variant:varscan ~/workspace/data/results/somatic/varscan/exome.vcf --variant:strelka ~/workspace/data/results/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/data/results/somatic/mutect/new_gatk_files/exome.vcf -o ~/workspace/data/results/somatic/exome.unique.vcf.gz #java -Xmx24g -jar ~/workspace/bin/GenomeAnalysisTK.jar -T CombineVariants -R ~/workspace/inputs/references/genome/ref_genome.fa -genotypeMergeOptions PRIORITIZE --rod_priority_list mutect,varscan,strelka --variant:varscan ~/workspace/somatic/varscan/exome.vcf --variant:strelka ~/workspace/somatic/strelka/exome/results/variants/exome.vcf --variant:mutect ~/workspace/somatic/mutect/exome.vcf -o ~/workspace/somatic/exome.merged.vcf.gz java -Xmx12g -jar ~/workspace/bin/picard.jar MergeVcfs \\ -I ~/workspace/somatic/varscan/exome.vcf \\ -I ~/workspace/somatic/strelka/exome/results/variants/exome.vcf \\ -I ~/workspace/somatic/mutect/mutect_exome_filtered.vcf \\ -O ~/workspace/somatic/exome.merged.vcf bgzip -c ~/workspace/somatic/exome.merged.vcf > ~/workspace/somatic/exome.merged.vcf.gz tabix -p vcf ~/workspace/somatic/exome.merged.vcf.gz","title":"Now we are going to merge the variants detected from all three variant callers"},{"location":"lab_session/variant_calling/#left-align-and-trim","text":"The reason for left align the variants and trim then is explained here . cd ~/workspace/somatic/ gatk --java-options \"-Xmx12G\" LeftAlignAndTrimVariants \\ -V ~/workspace/somatic/exome.merged.vcf.gz \\ -O exome.merged.leftalignandtrim.vcf \\ -R ~/workspace/inputs/references/genome/ref_genome.fa Note that when running on chromosome 6 and 17 merged variants file, this gave 0 variants aligned.","title":"Left Align and Trim"},{"location":"lab_session/variant_calling/#we-will-split-multi-allelic-variants-into-multiple-records","text":"cd ~/workspace/somatic/ vt decompose -s ~/workspace/somatic/exome.merged.leftalignandtrim.vcf \\ -o ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf","title":"We will split multi-allelic variants into multiple records"},{"location":"lab_session/variant_calling/#basic-filtering-on-somatic-variants","text":"First, let's do a basic filtering for PASS only variants on our merged and normalized vcf file: cd ~/workspace/somatic gatk --java-options \"-Xmx12G\" SelectVariants -R ~/workspace/inputs/references/genome/ref_genome.fa \\ --exclude-filtered -V ~/workspace/somatic/exome.merged.leftalignandtrim.decomposed.vcf \\ -O ~/workspace/somatic/exome.merged.norm.pass_only.vcf","title":"Basic Filtering on Somatic Variants"},{"location":"lab_session/variant_calling/#annotation-with-vep","text":"Again, we will use VEP to annotate the somatic variants as we did for the germline variants. cd ~/workspace/somatic # Runtime: ~4min #ssh -o ServerAliveInterval=300 -i course_EC2_01.pem ubuntu@ec2-52-23-206-90.compute-1.amazonaws.com #source .bashrc #cd workspace/somatic/ nohup vep --cache --dir_cache ~/workspace/vep_cache \\ --dir_plugins ~/workspace/vep_cache/Plugins \\ --fasta ~/workspace/inputs/references/genome/ref_genome.fa --fork 8 \\ --assembly = GRCh38 --offline --vcf -i ~/workspace/somatic/exome.merged.norm.pass_only.vcf \\ -o ~/workspace/somatic/exome.merged.norm.annotated.vcf \\ --check_existing --total_length --allele_number --no_escape --everything \\ --use_given_ref --force_overwrite & #Copy the html VEP summary to your local computer scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/somatic/exome.merged.norm.annotated.vcf_summary.html . Open the HTML file. Answer the following: - How many variants were processed by VEP? - Any diffrence in the distribution of variants vs. the germline variants? - The variant categories e.g. intron_variant and regulatory_region_variant should be approached carefully, why?","title":"Annotation with VEP."},{"location":"lab_session/variant_calling/#inspecting-variants-in-igv","text":"Download the bam files to your local machine scp -ri course_EC2_01.pem ubuntu@ec2-54-163-59-166.compute-1.amazonaws.com:~/workspace/align/Exome_Tumor_sorted_mrkdup_bqsr.* . scp -ri ~/PEM_KEY_ID.pem ubuntu@AWS_ADDRESS_HERE:~/workspace/align/Exome_Norm_sorted_mrkdup_bqsr.* . Open the Exome_Tumor_sorted_mrkdup_bqsr.bam and Exome_Norm_sorted_mrkdup_bqsr.bam in IGV. Keep the VCF file open in a terminal window on AWS Try to find a relevant variant in TP53 in the VCF file on AWS #Can be done in many ways, this is just an example. cd ~/workspace/somatic grep \"TP53\" exome.merged.norm.annotated.vcf | less -SN #### Or less -SN exome.merged.norm.annotated.vcf #In the less window type /TP53 Why is the TP53 variant in three rows in the VCF file? lead: how many somatic variant callers were run? Go to the position (chr17 7675088) on IGV in your local machine Is it present in the normal DNA? Do you still think it is valid? How come the variant can be present in the germline DNA? Do the same thing for a stop_gained and a frameshift variant cd ~/workspace/somatic grep \"stop_gained\" exome.merged.norm.annotated.vcf | less -SN The top variant is seen only once, this is just identified correctly by one caller because it changes two bases in a row open chr6 1930220 in IGV let us convert the vcf-file to a MAF file format note all information is not kept but it simplifies looking at the variants cd ~/workspace/somatic perl ~/workspace/bin/mskcc-vcf2maf/vcf2maf.pl --input-vcf ~/workspace/somatic/exome.merged.norm.annotated.vcf --output-maf ~/workspace/somatic/exome.merged.norm.annotated.maf --tumor-id TUMOR --normal-id NORMAL --inhibit-vep --ref-fasta ~/workspace/inputs/references/genome/ref_genome.fa less -SN ~/workspace/somatic/exome.merged.norm.annotated.maf #Count the number of variant types cut -f9 ~/workspace/somatic/exome.merged.norm.annotated.maf | sort | uniq -c #As you see the variant nomenclature is not the same in VCF and MAF grep Nonsense_Mutation ~/workspace/somatic/exome.merged.norm.annotated.maf | less -SN Have a look at the nosense variant in CCDC40 in IGV chr17 80050160 What is the VAF? What is the VAF of the TP53 variant? Reflections? Lead: clonality Can you find a variant in BRCA1? What is the impact? Is it relevant? There was a BRCA 1 variant in the germline that we did not dicsuss #Let us grep \"BRCA1\" and send the output to another grep command and take the \"HIGH\" impact variants grep BRCA1 ~/workspace/germline/Exome_Norm_HC_calls.filtered.PASS.vep.vcf | grep \"HIGH\" | less -SN let us inspect this variant in IGV, why is one allel missing in the tumor? This is the variant location: chr17 43057078","title":"Inspecting variants in IGV"},{"location":"lectures/lectures/","text":"Day 1 : Introduction to cancer genome and mutational processes Course overview PDF An introduction to the cancer genome and mutational processes in cancer. PDF Practical considerations for performing cancer genomics. PDF How to connect to AWS server. PDF Day 2 : Liquid Biopsies Liquid biopsies PDF The clinical impact of analysing the cancer genome. PDF Lab Introduction. PDF Day 3 : Bioinformatics pipelines & SNV calling Bioinformatics pipelines & HTC computing environments. PDF Processing and QC of DNA- and RNA sequencing data. PDF Somatic and germline variant callers & Lab Introduction. Day 4 : GSR calling & RNA-Seq Calling somatic- and germline copy-number alterations. Calling structural variation. \"RNA sequencing etc.Lab intro.\" Day 5 : Manual Curation - Clinical Trials Clinical trials. How to curate somatic- and germline variation for clinical use. Annotating, interpreting and reporting somatic- and germline variation for clinical use.","title":"Lectures"}]}