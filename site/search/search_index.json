{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Clinical Cancer Genomics \u00b6 This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The exercises will include processing and analysis of DNA- and RNA-sequencing data covering file formats, quality control aspects, identification of somatic variation, curation of identified somatic- and germline variants for clinical use, clonality estimation and annotation of variants. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory. Learning Outcomes \u00b6 At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation. Contents of the course \u00b6 An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use. Literature and other teaching material \u00b6 Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam. Additional Information \u00b6 2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Course Details"},{"location":"#welcome-to-clinical-cancer-genomics","text":"This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The exercises will include processing and analysis of DNA- and RNA-sequencing data covering file formats, quality control aspects, identification of somatic variation, curation of identified somatic- and germline variants for clinical use, clonality estimation and annotation of variants. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory.","title":"Welcome to Clinical Cancer Genomics"},{"location":"#learning-outcomes","text":"At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation.","title":"Learning Outcomes"},{"location":"#contents-of-the-course","text":"An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use.","title":"Contents of the course"},{"location":"#literature-and-other-teaching-material","text":"Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam.","title":"Literature and other teaching material"},{"location":"#additional-information","text":"2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Additional Information"},{"location":"annotations/","text":"HUMAN GENOME ANNOTATION FILES \u00b6 Files for annotation of variation in the human genome \u00b6 cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh Index the variation files \u00b6 cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz Interval files and coordinates for the exome sequencing assay \u00b6 # change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed Calculate the size of the SeqCap v3 exome \u00b6 #This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed Create an inverval list for the exome bed files \u00b6 # first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Annotation"},{"location":"annotations/#human-genome-annotation-files","text":"","title":"HUMAN GENOME ANNOTATION FILES"},{"location":"annotations/#files-for-annotation-of-variation-in-the-human-genome","text":"cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh","title":"Files for annotation of variation in the human genome"},{"location":"annotations/#index-the-variation-files","text":"cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz","title":"Index the variation files"},{"location":"annotations/#interval-files-and-coordinates-for-the-exome-sequencing-assay","text":"# change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed","title":"Interval files and coordinates for the exome sequencing assay"},{"location":"annotations/#calculate-the-size-of-the-seqcap-v3-exome","text":"#This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed","title":"Calculate the size of the SeqCap v3 exome"},{"location":"annotations/#create-an-inverval-list-for-the-exome-bed-files","text":"# first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Create an inverval list for the exome bed files"},{"location":"installation/","text":"INSTALLATION NOTES \u00b6 This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies. Prepare for installation \u00b6 For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin Install Samtools \u00b6 Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help Install PICARD \u00b6 PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h Install BWA \u00b6 BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa Install GATK 4 \u00b6 GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate Install VEP 93.4 \u00b6 VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help Install Varscan \u00b6 Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar Install BCFtools \u00b6 BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h Install Strelka \u00b6 Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h Install Sambamba \u00b6 Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba Install HISAT2 \u00b6 HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help Install StringTie \u00b6 StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h Install Gffcompare \u00b6 Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare Install R \u00b6 R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")' Install copyCat \u00b6 copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")' Install CNVnator \u00b6 CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator Install CNVkit \u00b6 CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate Install Kallisto \u00b6 Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto Install Pizzly \u00b6 Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help Manta \u00b6 Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate mosdepth \u00b6 mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth bam-readcount \u00b6 bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount vt \u00b6 vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt vcf-annotation-tools \u00b6 VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h Install seqtk \u00b6 Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk","title":"Installation"},{"location":"installation/#installation-notes","text":"This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies.","title":"INSTALLATION NOTES"},{"location":"installation/#prepare-for-installation","text":"For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin","title":"Prepare for installation"},{"location":"installation/#install-samtools","text":"Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help","title":"Install Samtools"},{"location":"installation/#install-picard","text":"PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h","title":"Install PICARD"},{"location":"installation/#install-bwa","text":"BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa","title":"Install BWA"},{"location":"installation/#install-gatk-4","text":"GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate","title":"Install GATK 4"},{"location":"installation/#install-vep-934","text":"VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help","title":"Install VEP 93.4"},{"location":"installation/#install-varscan","text":"Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar","title":"Install Varscan"},{"location":"installation/#install-bcftools","text":"BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h","title":"Install BCFtools"},{"location":"installation/#install-strelka","text":"Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h","title":"Install Strelka"},{"location":"installation/#install-sambamba","text":"Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba","title":"Install Sambamba"},{"location":"installation/#install-hisat2","text":"HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help","title":"Install HISAT2"},{"location":"installation/#install-stringtie","text":"StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h","title":"Install StringTie"},{"location":"installation/#install-gffcompare","text":"Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare","title":"Install Gffcompare"},{"location":"installation/#install-r","text":"R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")'","title":"Install R"},{"location":"installation/#install-copycat","text":"copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")'","title":"Install copyCat"},{"location":"installation/#install-cnvnator","text":"CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator","title":"Install CNVnator"},{"location":"installation/#install-cnvkit","text":"CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate","title":"Install CNVkit"},{"location":"installation/#install-kallisto","text":"Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto","title":"Install Kallisto"},{"location":"installation/#install-pizzly","text":"Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help","title":"Install Pizzly"},{"location":"installation/#manta","text":"Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate","title":"Manta"},{"location":"installation/#mosdepth","text":"mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth","title":"mosdepth"},{"location":"installation/#bam-readcount","text":"bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount","title":"bam-readcount"},{"location":"installation/#vt","text":"vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt","title":"vt"},{"location":"installation/#vcf-annotation-tools","text":"VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h","title":"vcf-annotation-tools"},{"location":"installation/#install-seqtk","text":"Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk","title":"Install seqtk"},{"location":"lab-session/","text":"","title":"Lab Session"},{"location":"lectures/","text":"Lecture 1 : Sequencing evolution Introduction Exome sequencing vs WGS sequencing vs targeted sequencing PDF RNA sequencing vs targeted RNA sequening vs single cell sequencing PDF Lecture 2 : Cancer genome and mutational processes Mutational signatures Concept of cancer drivers Lecture 3 : Liquid biopsies A targeted sequencing assay Screening of localised disease Lecture 4 : Bioinformatics pipelines Snakemake Lecture 5 : HTC computing environments Slurm Lecture 6 : Calling somatic- and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Lecture 7 : Variation for clinical use. How to curate somatic- and germline","title":"Lectures"},{"location":"references/","text":"HUMAN GENOME REFERENCE FILES \u00b6 Download a refernce file for human genome \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\" Split the long fasta by chromosome \u00b6 # Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/ Explore the contents of the reference genome file \u00b6 # View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"? Index the fasta files \u00b6 # first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa Transcriptome reference files \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf Explore the contents of the transcriptome reference files \u00b6 #How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome \u00b6 cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome Create a reference transcriptome index for use with Kallisto \u00b6 cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Reference"},{"location":"references/#human-genome-reference-files","text":"","title":"HUMAN GENOME REFERENCE FILES"},{"location":"references/#download-a-refernce-file-for-human-genome","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\"","title":"Download a refernce file for human genome"},{"location":"references/#split-the-long-fasta-by-chromosome","text":"# Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/","title":"Split the long fasta by chromosome"},{"location":"references/#explore-the-contents-of-the-reference-genome-file","text":"# View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"?","title":"Explore the contents of the reference genome file"},{"location":"references/#index-the-fasta-files","text":"# first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa","title":"Index the fasta files"},{"location":"references/#transcriptome-reference-files","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf","title":"Transcriptome reference files"},{"location":"references/#explore-the-contents-of-the-transcriptome-reference-files","text":"#How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c","title":"Explore the contents of the transcriptome reference files"},{"location":"references/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","text":"cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome"},{"location":"references/#create-a-reference-transcriptome-index-for-use-with-kallisto","text":"cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Create a reference transcriptome index for use with Kallisto"},{"location":"schedule/","text":"Day 1 : 2021-01-24 - Monday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Sequencing Evolution Cancer genome and mutational processes in cancer I Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II Johan Wargentin 10:45-11:00 Break 11:00-11:45 The clinical impact of analysing the cancer genome Felix + Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Github organisation/using github Use the command line and running bioinformatic tools How to order targted sequencing assay Visualise data in R Johan & Rebecka & Sarath Wargentin Day 2 : 2021-01-25 - Tuesday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Cancer genomics - Tissues and liquid biopsies I A targeted sequencing assay Screening of localised disease Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II. Johan Wargentin 10:45-11:00 Break 11:00-11:45 Lab Introduction basic pipeline basic tools IGV Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Illumina sequencing file formats. ( fastq, bam etc ) Refence genome Tools to manipulate the sequencing formats (Samtools, bedtools etc.) Visualise in IGV Johan & Rebecka & Sarath Wargentin Day 3 : 2021-01-26 - Wednesday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Bioinformatics pipelines HTC computing environments Sarath Wargentin 9:45-10:00 Break 10:00-10:45 Processing of DNA and RNA sequencing data QC of both DNA and RNA sequencing data Rebecka Wargentin 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers Point mutations and indels ICGC/TCGA - Identification of drivers Landmark paper from TCGA/ICGC on significantly mutated genes Lab Introduction To Be Continued... Rebecka Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Run the commands in a bash script (awk etc.) Targeted sequencing QC Add variant callers to the bioinformatic pipeline Visualise in IGV To be Continued... Johan & Rebecka & Sarath Wargentin Day 4 : 2021-01-27 - Thursday \u00b6 Time Topic Responsible Location 9:00-9:45 Calling somatic and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Markus Wargentin 9:45-10:00 Break 10:00-10:45 Calling somatic and germline variation To be Continued... Structural variation Landmark GSR papers from ICGC/TCGA Johan Wargentin 10:45-11:00 Break 11:00-11:45 Calling RNA fusions and getting expression values Lab Introduction To Be Continued... Sarath Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Use a skeleton of a bioinformatics pipeline and add some extra steps Run the bioinformatic pipeline Plot the output in ggplot Inspecting variants Johan & Rebecka & Markus & Sarath Wargentin Day 5 : 2021-01-28 - Friday \u00b6 Time Topic Responsible Location 9:00-9:45 How to curate somatic and germline variation for clinical use Johan Wargentin 9:45-10:00 Break 10:00-10:45 Annotating somatic and germline variation David Tamborero Wargentin 10:45-11:00 Break 11:00-11:45 Annotating somatic and germline variation To Be Continued... David Tamborero Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Annotating variants using VEP and setting the arguments correctly Curating variants manually using Curator Phenotypes David Tamborero & Johan & Rebecka & Sarath Wargentin","title":"Schedule"},{"location":"schedule/#day-1-2021-01-24-monday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Sequencing Evolution Cancer genome and mutational processes in cancer I Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II Johan Wargentin 10:45-11:00 Break 11:00-11:45 The clinical impact of analysing the cancer genome Felix + Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Github organisation/using github Use the command line and running bioinformatic tools How to order targted sequencing assay Visualise data in R Johan & Rebecka & Sarath Wargentin","title":"Day 1 : 2021-01-24 - Monday"},{"location":"schedule/#day-2-2021-01-25-tuesday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Cancer genomics - Tissues and liquid biopsies I A targeted sequencing assay Screening of localised disease Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II. Johan Wargentin 10:45-11:00 Break 11:00-11:45 Lab Introduction basic pipeline basic tools IGV Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Illumina sequencing file formats. ( fastq, bam etc ) Refence genome Tools to manipulate the sequencing formats (Samtools, bedtools etc.) Visualise in IGV Johan & Rebecka & Sarath Wargentin","title":"Day 2 : 2021-01-25 - Tuesday"},{"location":"schedule/#day-3-2021-01-26-wednesday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Bioinformatics pipelines HTC computing environments Sarath Wargentin 9:45-10:00 Break 10:00-10:45 Processing of DNA and RNA sequencing data QC of both DNA and RNA sequencing data Rebecka Wargentin 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers Point mutations and indels ICGC/TCGA - Identification of drivers Landmark paper from TCGA/ICGC on significantly mutated genes Lab Introduction To Be Continued... Rebecka Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Run the commands in a bash script (awk etc.) Targeted sequencing QC Add variant callers to the bioinformatic pipeline Visualise in IGV To be Continued... Johan & Rebecka & Sarath Wargentin","title":"Day 3 : 2021-01-26 - Wednesday"},{"location":"schedule/#day-4-2021-01-27-thursday","text":"Time Topic Responsible Location 9:00-9:45 Calling somatic and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Markus Wargentin 9:45-10:00 Break 10:00-10:45 Calling somatic and germline variation To be Continued... Structural variation Landmark GSR papers from ICGC/TCGA Johan Wargentin 10:45-11:00 Break 11:00-11:45 Calling RNA fusions and getting expression values Lab Introduction To Be Continued... Sarath Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Use a skeleton of a bioinformatics pipeline and add some extra steps Run the bioinformatic pipeline Plot the output in ggplot Inspecting variants Johan & Rebecka & Markus & Sarath Wargentin","title":"Day 4 : 2021-01-27 - Thursday"},{"location":"schedule/#day-5-2021-01-28-friday","text":"Time Topic Responsible Location 9:00-9:45 How to curate somatic and germline variation for clinical use Johan Wargentin 9:45-10:00 Break 10:00-10:45 Annotating somatic and germline variation David Tamborero Wargentin 10:45-11:00 Break 11:00-11:45 Annotating somatic and germline variation To Be Continued... David Tamborero Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Annotating variants using VEP and setting the arguments correctly Curating variants manually using Curator Phenotypes David Tamborero & Johan & Rebecka & Sarath Wargentin","title":"Day 5 : 2021-01-28 - Friday"}]}