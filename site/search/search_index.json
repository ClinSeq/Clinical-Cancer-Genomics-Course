{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Clinical Cancer Genomics \u00b6 This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The exercises will include processing and analysis of DNA- and RNA-sequencing data covering file formats, quality control aspects, identification of somatic variation, curation of identified somatic- and germline variants for clinical use, clonality estimation and annotation of variants. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory. Learning Outcomes \u00b6 At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation. Contents of the course \u00b6 An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use. Literature and other teaching material \u00b6 Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam. Additional Information \u00b6 2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Course Details"},{"location":"#welcome-to-clinical-cancer-genomics","text":"This course aims to provide an introduction to cancer genomics and to support to obtain practical knowledge regarding how to apply state of the art methodology to interrogate the cancer genome in a routine clinical setting or a clinical trial setting. The course will include lectures covering the technology advancements that have enabled high-throughput analysis of cancer genomes and the knowledge that can be obtained by applying these technologies. This encompasses both laboratory sample processing and downstream bioinformatics. Lectures will be held in the mornings with computer-based exercises in the afternoon. The exercises will include processing and analysis of DNA- and RNA-sequencing data covering file formats, quality control aspects, identification of somatic variation, curation of identified somatic- and germline variants for clinical use, clonality estimation and annotation of variants. The main objective of the course is to facilitate that students get an understanding of basic theory and obtain practical knowledge that will enable course participants to apply the covered methodologies in their own research- or clinical laboratory.","title":"Welcome to Clinical Cancer Genomics"},{"location":"#learning-outcomes","text":"At the end of this course the student will be able to: Show a basic insight into the cancer genome. Understand how the cancer genome can be interrogated through tissues and liquid biopsies. Understand how to apply technology to obtain relevant information from the cancer genome. Understand the file formats used in high throughput sequencing. use the command line and running bioinformatic tools. Understand the constituents of a bioinformatics pipeline for processing Illumina sequencing data and to run such a pipeline. Perform quality control on DNA- and RNA sequencing data for cancer sequencing purposes. Call somatic and germline variation. Curate somatic and germline variation for a clinical setting. Annotate somatic and germline variation. Visualise data in R. Use online resources such as genome browsers and portals for variant annotation.","title":"Learning Outcomes"},{"location":"#contents-of-the-course","text":"An introduction to the cancer genome and mutational processes in cancer. Overview of disease heterogeneity \u2013 the concept of cancer subtypes. The clinical impact of analysing the cancer genome. The concept of personalized therapy by tumour profiling. Intra-patient tumour heterogeneity. How to enable cancer genomics through tissues and liquid biopsies How to apply to high-throughput methodology to interrogate the cancer genome. Illumina sequencing file formats. Bioinformatics pipelines. Processing of DNA and RNA sequencing data. QC of both DNA and RNA sequencing data Calling somatic and germline variation: Point mutations and indels. Copy-number alterations. Structural variation. File formats for variant calling. Annotating somatic and germline variation. How to curate somatic- and germline variation for clinical use.","title":"Contents of the course"},{"location":"#literature-and-other-teaching-material","text":"Recommended reading before the course: Clinical cancer genomic profiling Standard operating procedure for somatic variant refinement of sequencing data with paired tumor and normal samples Note Lectures in the morning with computer exercises in the afternoon. 100% attendance is recommended, due to that each session is exclusive and cannot be compensated for later on. The student will be asked to review the issue presented in case of absence in a session. Each computer exercise addresses one or multiple learning outcomes. Each student will hand in a written report form each computer exercise. All intended learning outcomes need to be achieved in order to pass the exam.","title":"Literature and other teaching material"},{"location":"#additional-information","text":"2022-01-24 to 2022-01-28 Monday - Friday ( 09:00 - 17:00 ) johan.lindberg@ki.se","title":"Additional Information"},{"location":"annotations/","text":"HUMAN GENOME ANNOTATION FILES \u00b6 Files for annotation of variation in the human genome \u00b6 cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh Index the variation files \u00b6 cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz Interval files and coordinates for the exome sequencing assay \u00b6 # change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed Calculate the size of the SeqCap v3 exome \u00b6 #This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed Create an inverval list for the exome bed files \u00b6 # first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Annotation"},{"location":"annotations/#human-genome-annotation-files","text":"","title":"HUMAN GENOME ANNOTATION FILES"},{"location":"annotations/#files-for-annotation-of-variation-in-the-human-genome","text":"cd ~/workspace/inputs/references/ mkdir -p gatk cd gatk #In case gsutil needs to be installed conda install gsutil # SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: < 2min gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf . # Runtime: ~ 2min bgzip --threads 8 Homo_sapiens_assembly38.dbsnp138.vcf gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/hapmap_3.3.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_omni2.5.hg38.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/1000G_phase1.snps.high_confidence.hg38.vcf.gz . # Indel calibration call sets - dbsnp, Mills gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.known_indels.vcf.gz . gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz . # Interval lists that can be used to parallelize certain GATK tasks gsutil cp gs://genomics-public-data/resources/broad/hg38/v0/wgs_calling_regions.hg38.interval_list . gsutil cp -r gs://genomics-public-data/resources/broad/hg38/v0/scattered_calling_intervals/ . # list the files we just downloaded ls -lh","title":"Files for annotation of variation in the human genome"},{"location":"annotations/#index-the-variation-files","text":"cd ~/workspace/inputs/references/gatk/ #SNP calibration call sets - dbsnp, hapmap, omni, and 1000G # Runtime: ~ 4min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.dbsnp138.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/hapmap_3.3.hg38.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_omni2.5.hg38.vcf.gz # Runtime: ~ 3min gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/1000G_phase1.snps.high_confidence.hg38.vcf.gz #Indel calibration call sets - dbsnp, Mills gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Homo_sapiens_assembly38.known_indels.vcf.gz gatk --java-options '-Xmx12g' IndexFeatureFile -I ~/workspace/inputs/references/gatk/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz","title":"Index the variation files"},{"location":"annotations/#interval-files-and-coordinates-for-the-exome-sequencing-assay","text":"# change directories mkdir -p ~/workspace/inputs/references/exome cd ~/workspace/inputs/references/exome # download the files wget -c https://sequencing.roche.com/content/dam/rochesequence/worldwide/shared-designs/SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip unzip SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # remove the zip rm -f SeqCapEZ_Exome_v3.0_Design_Annotation_files.zip # Lift-over of the Roche coordinates from hg19 to the hg38 assembly. # download the software cd ~/workspace/bin wget http://hgdownload.soe.ucsc.edu/admin/exe/linux.x86_64/liftOver chmod +x liftOver # change to the appropriate directory cd ~/workspace/inputs/references/exome # download the chain file wget -c http://hgdownload.cse.ucsc.edu/goldenPath/hg19/liftOver/hg19ToHg38.over.chain.gz # run liftover liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_primary_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_primary_targets.bed unMapped.bed liftOver SeqCapEZ_Exome_v3.0_Design_Annotation_files/SeqCap_EZ_Exome_v3_hg19_capture_targets.bed hg19ToHg38.over.chain.gz SeqCap_EZ_Exome_v3_hg38_capture_targets.bed unMapped1.bed # create a version in standard bed format (chr, start, stop) cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_primary_targets.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed cut -f 1 -3 SeqCap_EZ_Exome_v3_hg38_capture_targets.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed # take a quick look at the format of these files head SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed head SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed","title":"Interval files and coordinates for the exome sequencing assay"},{"location":"annotations/#calculate-the-size-of-the-seqcap-v3-exome","text":"#This can be done in many ways - give it a try yourself before trying the code below and compare results # first sort the bed files and store the sorted versions bedtools sort -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed bedtools sort -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed # now merge the bed files to collapse any overlapping regions so they are not double counted. bedtools merge -i SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed bedtools merge -i SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.bed > SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed # finally use a Perl one liner to determine the size of the files in Mb FILES =( SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed ) echo ${ FILES [0] } for FILE in ${ FILES [@] } do echo \"--------------------------------------------------------\" echo $FILE #With merge cat $FILE | sortBed -i stdin | mergeBed -i stdin | perl -e '$counter=0; while(<>){chomp; @array = split \"\\t\", $_; $counter = $counter + ($array[2] - $array[1]);}; print $counter/1000000, \"\\n\";' done # note that the size of the space targeted by the exome reagent is ~63 Mbp. Does that sound reasonable? # now create a subset of these bed files grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed > exome_regions.bed #When creating files, make a habit to investigate the output to avoid downstream confusion head -n 10 exome_regions.bed grep -w -P \"^chr6|^chr17\" SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed > probe_regions.bed head -n 10 probe_regions.bed # clean up intermediate files #rm -fr SeqCapEZ_Exome_v3.0_Design_Annotation_files/ SeqCap_EZ_Exome_v3_hg38_primary_targets.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.bed SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.bed unMapped.bed","title":"Calculate the size of the SeqCap v3 exome"},{"location":"annotations/#create-an-inverval-list-for-the-exome-bed-files","text":"# first for the complete exome and probe bed file cd ~/workspace/inputs/references/ mkdir temp cd temp wget http://genomedata.org/pmbio-workshop/references/genome/all/ref_genome.dict cd ~/workspace/inputs/references/exome java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_primary_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict java -jar $PICARD BedToIntervalList I = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.bed O = SeqCap_EZ_Exome_v3_hg38_capture_targets.v2.sort.merge.interval_list SD = ~/workspace/inputs/references/temp/ref_genome.dict rm -fr ~/workspace/inputs/references/temp/ # next for our subset exome and probe regions file cd ~/workspace/inputs/references/exome java -jar /usr/local/bin/picard.jar BedToIntervalList I = exome_regions.bed O = exome_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict java -jar /usr/local/bin/picard.jar BedToIntervalList I = probe_regions.bed O = probe_regions.bed.interval_list SD = ~/workspace/inputs/references/genome/ref_genome.dict","title":"Create an inverval list for the exome bed files"},{"location":"installation/","text":"INSTALLATION NOTES \u00b6 This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies. Prepare for installation \u00b6 For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin Install Samtools \u00b6 Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help Install PICARD \u00b6 PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h Install BWA \u00b6 BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa Install GATK 4 \u00b6 GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate Install VEP 93.4 \u00b6 VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help Install Varscan \u00b6 Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar Install BCFtools \u00b6 BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h Install Strelka \u00b6 Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h Install Sambamba \u00b6 Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba Install HISAT2 \u00b6 HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help Install StringTie \u00b6 StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h Install Gffcompare \u00b6 Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare Install R \u00b6 R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")' Install copyCat \u00b6 copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")' Install CNVnator \u00b6 CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator Install CNVkit \u00b6 CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate Install Kallisto \u00b6 Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto Install Pizzly \u00b6 Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help Manta \u00b6 Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate mosdepth \u00b6 mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth bam-readcount \u00b6 bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount vt \u00b6 vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt vcf-annotation-tools \u00b6 VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h Install seqtk \u00b6 Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk","title":"Installation"},{"location":"installation/#installation-notes","text":"This workshop requires a large number of different bioinformatics tools. The instructions for installing these tools exist here. Note that depending on the operating system and environment, some additional dependencies would likely be needed. If you are using the AWS instance built for this course these dependencies have already been installed. The remainder of this section will assume that you are on the AWS instance, however these instructions should work on any ubuntu distribution with the required dependencies.","title":"INSTALLATION NOTES"},{"location":"installation/#prepare-for-installation","text":"For this workshop we will be using the workspace folder to store results, executables, and input files. To start we must choose a single directory for installing tools, typically in linux, user compiled tools are installed in /usr/local/bin however backups of the tools we will be using have already been installed there. In this tutorial we will install tools in ~/workspace/bin. Lets go ahead and make a bin directory in ~/workspace to get started. # make a bin directory mkdir -p ~/workspace/bin","title":"Prepare for installation"},{"location":"installation/#install-samtools","text":"Samtools is a software package based in C which provies utilities for manipulating alignment files (SAM/BAM/CRAM). It is open source, available on github, and is under an MIT license. Let\u2019s go ahead and download the source code from github to our bin directory and extract it with tar. Next we need to cd into our extracted samtools source code and configure the software. Running ./configure will make sure all dependencies are available and will also let the software know where it should install to. After that we will need to run make to actually build the software. Finally we can run make install which will copy the built software and the underlying libraries, documentation, etc. to their final locations. We can check the installation and print out the help message by providing the full path to the executable. # change to bin directory cd ~/workspace/bin # download and extract the source code wget https://github.com/samtools/samtools/releases/download/1.14/samtools-1.14.tar.bz2 tar --bzip2 -xvf samtools-1.14.tar.bz2 # configure and compile cd samtools-1.14/ ./configure --prefix = /home/ubuntu/workspace/bin/samtools-1.14/ make make install ln -s /home/ubuntu/workspace/bin/samtools-1.14/bin/samtools /home/ubuntu/workspace/bin/samtools # check instalation ~/workspace/bin/samtools --help","title":"Install Samtools"},{"location":"installation/#install-picard","text":"PICARD is a set of java based tools developed by the Broad institute. It is very useful for manipulating next generation sequencing data and is available under an open source MIT license. The version of Picard we will be using requires java 8 which has already been installed. All we need to do is download the jar file which is a package file used to distribute java code. We can do this with wget. To run the software, we simply need to call java with the -jar option and provide the jar file. # change to the bin directory and download the jar file cd ~/workspace/bin wget https://github.com/broadinstitute/picard/releases/download/2.26.6/picard.jar # check the installation java -jar ~/workspace/bin/picard.jar -h","title":"Install PICARD"},{"location":"installation/#install-bwa","text":"BWA is a popular DNA alignment tool used for mapping sequences to a reference genome. It is available under an open source GPLv3 license. To install BWA, we first need to download and extract the source code. Unlike with samtools theres no ./configure file so we can just run make to build the software. We can then make a symlink with ln -s which is just a reference to another file. In this case we will make a symlink so the executable in ~/workspace/bin/bwa-0.7.17/bwa and be found in ~/workspace/bin/bwa. # change to the bin folder, download, and extract the source code cd ~/workspace/bin wget https://sourceforge.net/projects/bio-bwa/files/bwa-0.7.17.tar.bz2 tar --bzip2 -xvf bwa-0.7.17.tar.bz2 # build the software cd bwa-0.7.17 make # make symlink ln -s ~/workspace/bin/bwa-0.7.17/bwa ~/workspace/bin/bwa # check the installation ~/workspace/bin/bwa","title":"Install BWA"},{"location":"installation/#install-gatk-4","text":"GATK is a toolkit developed by the broad institute focused primarily on variant discovery and genotyping. It is open source, hosted on github, and available under a BSD 3-clause license. First let\u2019s download and unzip GATK from github. The creators of GATK recommend running GATK through conda which is a package, environment, and dependency management software, in essence conda basically creates a virtual environment from which to run software. The next step then is to tell conda to create a virtual environment for GATK by using the yaml file included within GATK as the instructions for creating the virtual environment. We do this with the command conda env create, we also use the -p option to specify where this environment should be stored. We will also make a symlink so the executable downloaded is available directly from our bin folder. To run GATK we must first start up the virtual environment with the command source activate, we can then run the program by providing the path to the executable. To exit the virtual environment run the command source deactivate. # download and unzip cd ~/workspace/bin wget https://github.com/broadinstitute/gatk/releases/download/4.2.3.0/gatk-4.2.3.0.zip unzip gatk-4.2.3.0.zip # make sure ubuntu user can create their own conda environments sudo chown -R ubuntu:ubuntu /home/ubuntu/.conda # create conda environment for gatk cd gatk-4.2.3.0/ conda env create -f gatkcondaenv.yml -p ~/workspace/bin/conda/gatk # make symlink ln -s ~/workspace/bin/gatk-4.2.3.0/gatk ~/workspace/bin/gatk # test installation conda activate ~/workspace/bin/conda/gatk ~/workspace/bin/gatk # to exit the virtual environment conda deactivate","title":"Install GATK 4"},{"location":"installation/#install-vep-934","text":"VEP is a variant annotation tool developed by ensembl and written in perl. By default VEP will perform annotations by making web-based API queries however it is much faster to have a local copy of cache and fasta files. The AWS AMI image we\u2019re using already has these files for hg38 in the directory ~/workspace/vep_cache as they can take a bit of time to download. To get an idea of what it\u2019s like to install these we will install a vep_cache for petromyzon_marinus, a much smaller genome. To start we need to download vep from github using wget and unzip VEP. From there we can use the INSTALL.pl script vep provides to install the software which will ask a series of questions listed below. We also make a symlink when the installer completes. Note that the following assumes the existence of a particular version of Perl. We had to install Perl 5.22.0 since this is the last version supported by VEP and the version that comes with Ubuntu 18.04 is newer than this. When prompted by the install step below use these answers: Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] Do you want to continue installing the API (y/n)? y [ENTER] Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] Do you want to install any plugins (y/n)? n [ENTER] # download and unzip vep cd ~/workspace/bin wget https://github.com/Ensembl/ensembl-vep/archive/refs/tags/release/104.3.zip unzip 104 .3.zip # run the INSTALL.pl script provided by VEP cd ensembl-vep-release-104.3/ /usr/local/bin/perl-5.22.0/perl -MCPAN -e 'install DBI' /usr/local/bin/perl-5.22.0/perl INSTALL.pl --CACHEDIR ~/workspace/vep_cache #1. Do you wish to exit so you can get updates (y) or continue (n): n [ENTER] #2. Do you want to continue installing the API (y/n)? y [ENTER] (if asked) #3. Do you want to install any cache files (y/n)? y [ENTER] 147 [ENTER] #4. Do you want to install any FASTA files (y/n)? y [ENTER] 42 [ENTER] #5. Do you want to install any plugins (y/n)? n [ENTER] # make a symlink ln -s ~/workspace/bin/ensembl-vep-release-104.3/vep ~/workspace/bin/vep # test the Installation ~/workspace/bin/vep --help","title":"Install VEP 93.4"},{"location":"installation/#install-varscan","text":"Varscan is a java program designed to call variants in sequencing data. It was developed at the Genome Institute at Washington University and is hosted on github. To use Varscan we simply need to download the distributed jar file into our~/workspace/bin. As with the other java programs which have already been installed in this section we can invoke Varscan via java -jar # Install Varscan cd ~/workspace/bin curl -L -k -o VarScan.v2.4.2.jar https://github.com/dkoboldt/varscan/releases/download/2.4.2/VarScan.v2.4.2.jar java -jar ~/workspace/bin/VarScan.v2.4.2.jar","title":"Install Varscan"},{"location":"installation/#install-bcftools","text":"BCFtools is an open source program for variant calling and manipulating files in Variant Call Format (VCF) or Binary Variant Call Format (BCF). To install we first need to download and extract the source code with curl and tar respectively. We can then call make to build the program and make install to copy the program to the desired directory. cd ~/workspace/bin curl -L -k -o bcftools-1.14.tar.bz2 https://github.com/samtools/bcftools/releases/download/1.14/bcftools-1.14.tar.bz2 tar --bzip2 -xvf bcftools-1.14.tar.bz2 #install the software cd bcftools-1.14 make -j make prefix = ~/workspace/bin/bcftools-1.14 install ln -s ~/workspace/bin/bcftools-1.14/bin/bcftools ~/workspace/bin/bcftools # test installation ~/workspace/bin/bcftools -h","title":"Install BCFtools"},{"location":"installation/#install-strelka","text":"Strekla is a germline and somatic variant caller developed by illumina and available under an open source GPLv3 license. The binary distribution for strelka is already built and hosted on github so to install all we have to do is download and extract the software. It is important to note that strelka is built on python 2 and won\u2019t work for python 3. The AMI we\u2019re using contains both python versions so we just have to make sure we invoke strelka with python2, you can view the python versions on the AMI with python2 --version and python3 --version. # download and extract cd ~/workspace/bin conda create --name strelka-env python = 2 .7 curl -L -k -o strelka-2.9.10.centos6_x86_64.tar.bz2 https://github.com/Illumina/strelka/releases/download/v2.9.10/strelka-2.9.10.centos6_x86_64.tar.bz2 tar --bz2 -xvf strelka-2.9.10.centos6_x86_64.tar.bz2 # test installation python2 ~/workspace/bin/strelka-2.9.10.centos6_x86_64/bin/configureStrelkaSomaticWorkflow.py -h","title":"Install Strelka"},{"location":"installation/#install-sambamba","text":"Sambamba is a high performance alternative to samtools and provides a subset of samtools functionality. It is up to 6x faster for duplicate read marking and 4x faster for viewing alignment files. To install sambamba we can just download the binary distribution and extract it. From there we just make a symlink to make using it a bit more intuitive. # download and extract cd ~/workspace/bin curl -L -k -o sambamba_v0.6.4_linux.tar.bz2 https://github.com/lomereiter/sambamba/releases/download/v0.6.4/sambamba_v0.6.4_linux.tar.bz2 tar --bzip2 -xvf sambamba_v0.6.4_linux.tar.bz2 # create symlink ln -s ~/workspace/bin/sambamba_v0.6.4 ~/workspace/bin/sambamba # test installation ~/workspace/bin/sambamba","title":"Install Sambamba"},{"location":"installation/#install-hisat2","text":"HISAT2 is a graph based alignment algorithm devoloped at Johns Hopkins University. It is heavily used in the bioinformatics community for RNAseq based alignments. To Install we will need to download and extract the binary executable. We then make a symlink to put it with the other executables we\u2019ve installed. # download and extract cd ~/workspace/bin wget ftp://ftp.ccb.jhu.edu/pub/infphilo/hisat2/downloads/hisat2-2.1.0-Linux_x86_64.zip unzip hisat2-2.1.0-Linux_x86_64.zip # create symlink ln -s ~/workspace/bin/hisat2-2.1.0/hisat2 ~/workspace/bin/hisat2 # test installation ~/workspace/bin/hisat2 --help","title":"Install HISAT2"},{"location":"installation/#install-stringtie","text":"StringTie is a software program to perform transcript assembly and quantification of RNAseq data. The binary distributions are available so to install we can just download this distribution and extract it. Like with our other programs we also make a symlink to make it easier to find. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/stringtie-2.2.0.Linux_x86_64.tar.gz tar -xzvf stringtie-2.2.0.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/stringtie-2.2.0.Linux_x86_64/stringtie ~/workspace/bin/stringtie # test installation ~/workspace/bin/stringtie -h","title":"Install StringTie"},{"location":"installation/#install-gffcompare","text":"Gffcompare is a program that is used to perform operations on general feature format (GFF) and general transfer format (GTF) files. It has a binary distribution compatible with the linux we\u2019re using so we will just download, extract, and make a symlink. # download and extract cd ~/workspace/bin wget http://ccb.jhu.edu/software/stringtie/dl/gffcompare-0.9.8.Linux_x86_64.tar.gz tar -xzvf gffcompare-0.9.8.Linux_x86_64.tar.gz # make symlink ln -s ~/workspace/bin/gffcompare-0.9.8.Linux_x86_64/gffcompare ~/workspace/bin/gffcompare # check Installation ~/workspace/bin/gffcompare","title":"Install Gffcompare"},{"location":"installation/#install-r","text":"R is a feature rich interpretive programming language originally released in 1995. It is heavily used in the bioinformatics community largely due to numerous R libraries available on bioconductor. It takes a several minutes to compile so we\u2019ll use one which has already been setup. If we were to install R, we first would need to download and extract the source code. Next we\u2019d configure the installation with --with-x=no which tells R to install without X11, a windowing system for displays. We\u2019d also specify --prefix which is where the R framework will go, this includes the additional R libraries we\u2019ll download later. From there we\u2019d do make and make install to build the software and copy the files to their proper location and create symlinks for the executables. Finally we\u2019d install the devtools and Biocmanager packages from the command line to make installing additional packages easier. We\u2019ve commented out the code below, however it is exactly what was run to set up the R we will be using, except the installation location. ## download and extract cd ~/workspace/bin wget https://cran.r-project.org/src/base/R-3/R-3.5.1.tar.gz tar -zxvf R-3.5.1.tar.gz ## configure the installation, build the code cd R-3.5.1 ./configure --prefix = /home/ubuntu/workspace/bin --with-x = no make make install ## make symlinks ln -s ~/workspace/bin/R-3.5.1/bin/Rscript ~/workspace/bin/Rscript ln -s ~/workspace/bin/lib64/R/bin/R ~/workspace/bin/R ## test installation cd ~/workspace/bin ~/workspace/bin/Rscript --version ## install additional packages ~/workspace/bin/R --vanilla -e 'install.packages(c(\"devtools\", \"BiocManager\", \"dplyr\", \"tidyr\", \"ggplot2\"), repos=\"http://cran.us.r-project.org\")'","title":"Install R"},{"location":"installation/#install-copycat","text":"copyCat is an R library for detecting copy number aberrations in sequencing data. The library is only available on github so we will have to use the BiocManager library to install a few of the underlying package dependencies. If installing a package from cran or bioconductor these dependencies would be automatically installed. After these dependencies are installed we can use the devtools package to install copycat directory from its github repository. # Install R Library dependencies ~/workspace/bin/R --vanilla -e 'BiocManager::install(c(\"IRanges\", \"DNAcopy\"))' # install copyCat ~/workspace/bin/R --vanilla -e 'devtools::install_github(\"chrisamiller/copycat\")'","title":"Install copyCat"},{"location":"installation/#install-cnvnator","text":"CNVnator is a depth based copy number caller. It is open source and available on github under a creative common public license (CCPL). To install we first download and extract the source code. CNVnator relies on a specific version of samtools which is distributed with CNVnator, so our first step is to run make on that samtools. To finish the installation process we can then run make in CNVnator\u2019s main source directory. # download and decompress cd ~/workspace/bin #download and install dependency package \"root\" from Cern (https://root.cern/install/). curl -OL https://root.cern/download/root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz tar -xvzf root_v6.20.08.Linux-ubuntu20-x86_64-gcc9.3.tar.gz source root/bin/thisroot.sh wget https://github.com/abyzovlab/CNVnator/releases/download/v0.3.3/CNVnator_v0.3.3.zip unzip CNVnator_v0.3.3.zip # make the samtools dependency distributed with CNVnator cd CNVnator_v0.3.3/src/samtools make # make CNVnator cd ../ make # make a symlink ln -s ~/workspace/bin/CNVnator_v0.3.3/src/cnvnator ~/workspace/bin/cnvnator # test installation ~/workspace/bin/cnvnator","title":"Install CNVnator"},{"location":"installation/#install-cnvkit","text":"CNVkit is a python based copy number caller designed for use with hybrid capture. To install we can download and extract the package. We then must use conda to set up the environment to run cnvkit. This process, while straight forward, takes some time so we\u2019ve commented out the installation instructions for this tool and will use the conda environment that has already been set up. ## download and unzip cd ~/workspace/bin wget https://github.com/etal/cnvkit/archive/refs/tags/v0.9.9.zip unzip v0.9.9.zip ## add conda channels conda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda ## create conda environment conda create -n cnvkit python = 3 ln -s ~/workspace/bin/cnvkit-0.9.9/cnvkit.py ~/workspace/bin/cnvkit.py # test installation source activate cnvkit #install all dependencies ~/workspace/bin/cnvkit.py --help # to exit the virtual environment source deactivate","title":"Install CNVkit"},{"location":"installation/#install-kallisto","text":"Kallisto is a kmer-based alignment algorithm used for quantifying transcripts in RNAseq data. Kallisto has a binary distribution available so to use the program we only have to download and extract the software from github. # download and extract cd ~/workspace/bin wget https://github.com/pachterlab/kallisto/releases/download/v0.46.2/kallisto_linux-v0.46.2.tar.gz tar -zxvf kallisto_linux-v0.46.2.tar.gz mv kallisto kallisto_linux-v0.46.2 # make symlink ln -s ~/workspace/bin/kallisto_linux-v0.46.2/kallisto ~/workspace/bin/kallisto # test installation ~/workspace/bin/kallisto","title":"Install Kallisto"},{"location":"installation/#install-pizzly","text":"Pizzly is a fusion detection algorithm which uses output from Kallisto. Pizzly has a binary distribution so we can download and extract that from github to get started. # download and extract cd ~/workspace/bin mkdir pizzly-v0.37.3 cd pizzly-v0.37.3 wget https://github.com/pmelsted/pizzly/releases/download/v0.37.3/pizzly_linux.tar.gz tar -zxvf pizzly_linux.tar.gz # make symlink ln -s ~/workspace/bin/pizzly-v0.37.3/pizzly ~/workspace/bin/pizzly # test executable ~/workspace/bin/pizzly --help","title":"Install Pizzly"},{"location":"installation/#manta","text":"Manta is a structural variant caller developed by Illumina and available on gitub under the GPL_v3 license. It uses paired-end sequencing reads to build a breakend association graph to identify structural varaints. # download and extract cd ~/workspace/bin wget https://github.com/Illumina/manta/releases/download/v1.6.0/manta-1.6.0.centos6_x86_64.tar.bz2 tar --bzip2 -xvf manta-1.6.0.centos6_x86_64.tar.bz2 #we can use strelka-env for this also conda activate strelka-env # test installation python2 ~/workspace/bin/manta-1.6.0.centos6_x86_64/bin/configManta.py --help conda deactivate","title":"Manta"},{"location":"installation/#mosdepth","text":"mosdepth is a program for determining depth in sequencing data. The easiest way to install mosdepth is through bioconda a channel for the conda package manager. The AMI already has conda setup to install to /usr/local/bin/miniconda and so we\u2019ve already installed mosdepth for you. However below are the commands used during the installation. # add the bioconda channel conda config --add channels bioconda # install mosdepth with the conda package manager conda install mosdepth","title":"mosdepth"},{"location":"installation/#bam-readcount","text":"bam-readcount is a program for determing read support for individual variants (SNVs and Indels only). We are going to point this local install of bam-readcount to use the samtools installation we completed above. Samtools is a dependency of bam-readcount. This tool uses Cmake to create its makefile, so compiling from source has an extra step here. Instead of using an official release from github we are cloning the latest code from the master branch. In general this practice should be avoided and you should use an official release instead. # install bam-readcount cd ~/workspace/bin git clone https://github.com/genome/bam-readcount.git mv bam-readcount bam-readcount-latest cd bam-readcount-latest export SAMTOOLS_ROOT = /home/ubuntu//workspace/bin/samtools-1.14 cmake -Wno-dev /home/ubuntu/workspace/bin/bam-readcount-latest make # create symlink ln -s ~/workspace/bin/bam-readcount-latest/bin/bam-readcount ~/workspace/bin/bam-readcount # test installation ~/workspace/bin/bam-readcount","title":"bam-readcount"},{"location":"installation/#vt","text":"vt is a variant tool set that discovers short variants from Next Generation Sequencing data. We will use this for the purpose of splitting multi-allelic variants. #install vt cd ~/workspace/bin conda install -c bioconda vt # create symlink ln -s /home/ubuntu/miniconda3/bin/vt ~/workspace/bin/vt # test installation ~/workspace/bin/vt","title":"vt"},{"location":"installation/#vcf-annotation-tools","text":"VCF Annotation Tools is a python package that includes several tools to annotate VCF files with data from other tools. We will be using this for the purpose of adding bam readcounts to the vcf files. #install vcf-annotation-tools pip install vcf-annotation-tools #testing Installation vcf-readcount-annotator -h","title":"vcf-annotation-tools"},{"location":"installation/#install-seqtk","text":"Seqtk is a lighweight tool for processing FASTQ and FASTA files. We will use seqtk to subset RNA-seq fastq files to more quickly run the fusion alignment module. # Download cd ~/workspace/bin git clone https://github.com/lh3/seqtk.git seqtk.v1 # Install cd seqtk.v1 make # (Ignore warning message) make install # Check install ln -s /usr/local/bin/seqtk ~/workspace/bin/seqtk ~/workspace/bin/seqtk","title":"Install seqtk"},{"location":"references/","text":"HUMAN GENOME REFERENCE FILES \u00b6 Download a refernce file for human genome \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\" Split the long fasta by chromosome \u00b6 # Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/ Explore the contents of the reference genome file \u00b6 # View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"? Index the fasta files \u00b6 # first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa Transcriptome reference files \u00b6 # Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf Explore the contents of the transcriptome reference files \u00b6 #How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome \u00b6 cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome Create a reference transcriptome index for use with Kallisto \u00b6 cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Reference"},{"location":"references/#human-genome-reference-files","text":"","title":"HUMAN GENOME REFERENCE FILES"},{"location":"references/#download-a-refernce-file-for-human-genome","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for reference genome files and enter this dir. mkdir -p ~/workspace/inputs/references/genome cd ~/workspace/inputs/references/genome # Dowload human reference genome files from the course data server. wget http://genomedata.org/pmbio-workshop/references/genome/ $CHRS /ref_genome.tar # Unpack the archive using `tar -xvf` (`x` for extract, `v` for verbose, # `f` for file). tar -xvf ref_genome.tar # View contents. tree # Remove the archive. rm -f ref_genome.tar # Uncompress the reference genome FASTA file. gunzip ref_genome.fa.gz # View contents. tree # Check the chromosome headers in the fasta file. cat ref_genome.fa | grep -P \"^>\"","title":"Download a refernce file for human genome"},{"location":"references/#split-the-long-fasta-by-chromosome","text":"# Make new directory and change directories. mkdir -p ~/workspace/inputs/references/genome/ref_genome_split/ cd ~/workspace/inputs/references/genome # Split. faSplit byname ref_genome.fa ./ref_genome_split/","title":"Split the long fasta by chromosome"},{"location":"references/#explore-the-contents-of-the-reference-genome-file","text":"# View the first 10 lines of this file. Note the header line starting with `>`. # Why does the sequence look like this? cd ~/workspace/inputs/references/genome head -n 10 ref_genome.fa # Pull out only the header lines. grep \">\" ref_genome.fa # How many lines and characters are in this file? wc ref_genome.fa # How long are to two chromosomes combined (in bases and Mbp)? Use grep to skip the header lines for each chromosome. grep -v \">\" ref_genome.fa | wc # How long does that command take to run? time grep -v \">\" ref_genome.fa | wc # View 10 lines from approximately the middle of this file head -n 2500000 ref_genome.fa | tail # What is the count of each base in the entire reference genome file (skipping the header lines for each sequence)? # Runtime: ~30s cat ref_genome.fa | grep -v \">\" | perl -ne 'chomp $_; $bases{$_}++ for split //; if (eof){print \"$_ $bases{$_}\\n\" for sort keys %bases}' # What does each of these bases refer to? What are the \"unexpected bases\"?","title":"Explore the contents of the reference genome file"},{"location":"references/#index-the-fasta-files","text":"# first remove the .fai and .dict files that were downloaded. Do not remove the .fa file though! cd ~/workspace/inputs/references/genome rm -f ref_genome.fa.fai ref_genome.dict # Use samtools to create a fasta index file. samtools faidx ref_genome.fa # View the contents of the index file. head ref_genome.fa.fai # Use picard to create a dictionary file. java -jar $PICARD CreateSequenceDictionary -R ref_genome.fa -O ref_genome.dict # View the content of the dictionary file. cat ref_genome.dict #Also index the split chromosomes. samtools faidx ./ref_genome_split/chr6.fa samtools faidx ./ref_genome_split/chr17.fa # Create reference index for the genome to use BWA bwa index ref_genome.fa","title":"Index the fasta files"},{"location":"references/#transcriptome-reference-files","text":"# Make sure CHRS environment variable is set. echo $CHRS # Create a directory for transcriptome input files. mkdir -p ~/workspace/inputs/references/transcriptome cd ~/workspace/inputs/references/transcriptome # Download the files. wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.gtf wget http://genomedata.org/pmbio-workshop/references/transcriptome/ $CHRS /ref_transcriptome.fa # Take a look at the contents of the gtf file. Press 'q' to exit the 'less' display. less -p start_codon -S ref_transcriptome.gtf","title":"Transcriptome reference files"},{"location":"references/#explore-the-contents-of-the-transcriptome-reference-files","text":"#How many chromsomes are represented? cut -f1 ref_transcriptome.gtf | sort | uniq -c # How many unique gene IDs are in the .gtf file? # We can use a perl command-line command to find out: perl -ne 'if ($_ =~ /(gene_id\\s\\\"ENSG\\w+\\\")/){print \"$1\\n\"}' ref_transcriptome.gtf | sort | uniq | wc -l # what are all the feature types listed in the third column of the GTF? # how does the following command (3 commands piped together) answer that question? cut -f 3 ref_transcriptome.gtf | sort | uniq -c","title":"Explore the contents of the transcriptome reference files"},{"location":"references/#create-a-reference-index-for-transcriptome-with-hisat-for-splice-rna-alignments-to-the-genome","text":"cd ~/workspace/inputs/references/transcriptome # Create a database of observed splice sites represented in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_splice_sites.py ref_transcriptome.gtf > splicesites.tsv head splicesites.tsv # Create a database of exon regions in our reference transcriptome GTF ~/workspace/bin/hisat2-2.1.0/hisat2_extract_exons.py ref_transcriptome.gtf > exons.tsv head exons.tsv # build the reference genome index for HISAT and supply the exon and splice site information extracted in the previous steps # specify to use 8 threads with the `-p 8` option # run time for this index is ~5 minutes ~/workspace/bin/hisat2-2.1.0/hisat2-build -p 8 --ss splicesites.tsv --exon exons.tsv ~/workspace/inputs/references/genome/ref_genome.fa ref_genome","title":"Create a reference index for transcriptome with HISAT for splice RNA alignments to the genome"},{"location":"references/#create-a-reference-transcriptome-index-for-use-with-kallisto","text":"cd ~/workspace/inputs/references/transcriptome mkdir kallisto cd kallisto # tidy up the headers to just include the ensembl transcript ids cat ../ref_transcriptome.fa | perl -ne 'if ($_ =~ /\\d+\\s+(ENST\\d+)/){print \">$1\\n\"}else{print $_}' > ref_transcriptome_clean.fa # run time for this index is ~30 seconds kallisto index --index = ref_transcriptome_kallisto_index ref_transcriptome_clean.fa","title":"Create a reference transcriptome index for use with Kallisto"},{"location":"schedule/","text":"Day 1 : 2021-01-24 - Monday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Sequencing Evolution Cancer genome and mutational processes in cancer I Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II Johan Wargentin 10:45-11:00 Break 11:00-11:45 The clinical impact of analysing the cancer genome Felix + Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Github organisation/using github Use the command line and running bioinformatic tools How to order targted sequencing assay Visualise data in R Johan & Rebecka & Sarath Wargentin Day 2 : 2021-01-25 - Tuesday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Cancer genomics - Tissues and liquid biopsies I A targeted sequencing assay Screening of localised disease Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II. Johan Wargentin 10:45-11:00 Break 11:00-11:45 Lab Introduction basic pipeline basic tools IGV Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Illumina sequencing file formats. ( fastq, bam etc ) Refence genome Tools to manipulate the sequencing formats (Samtools, bedtools etc.) Visualise in IGV Johan & Rebecka & Sarath Wargentin Day 3 : 2021-01-26 - Wednesday \u00b6 Time Topic Responsible Location 9:00-9:45 Introduction Bioinformatics pipelines HTC computing environments Sarath Wargentin 9:45-10:00 Break 10:00-10:45 Processing of DNA and RNA sequencing data QC of both DNA and RNA sequencing data Rebecka Wargentin 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers Point mutations and indels ICGC/TCGA - Identification of drivers Landmark paper from TCGA/ICGC on significantly mutated genes Lab Introduction To Be Continued... Rebecka Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Run the commands in a bash script (awk etc.) Targeted sequencing QC Add variant callers to the bioinformatic pipeline Visualise in IGV To be Continued... Johan & Rebecka & Sarath Wargentin Day 4 : 2021-01-27 - Thursday \u00b6 Time Topic Responsible Location 9:00-9:45 Calling somatic and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Markus Wargentin 9:45-10:00 Break 10:00-10:45 Calling somatic and germline variation To be Continued... Structural variation Landmark GSR papers from ICGC/TCGA Johan Wargentin 10:45-11:00 Break 11:00-11:45 Calling RNA fusions and getting expression values Lab Introduction To Be Continued... Sarath Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Use a skeleton of a bioinformatics pipeline and add some extra steps Run the bioinformatic pipeline Plot the output in ggplot Inspecting variants Johan & Rebecka & Markus & Sarath Wargentin Day 5 : 2021-01-28 - Friday \u00b6 Time Topic Responsible Location 9:00-9:45 How to curate somatic and germline variation for clinical use Johan Wargentin 9:45-10:00 Break 10:00-10:45 Annotating somatic and germline variation David Tamborero Wargentin 10:45-11:00 Break 11:00-11:45 Annotating somatic and germline variation To Be Continued... David Tamborero Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Annotating variants using VEP and setting the arguments correctly Curating variants manually using Curator Phenotypes David Tamborero & Johan & Rebecka & Sarath Wargentin","title":"Schedule"},{"location":"schedule/#day-1-2021-01-24-monday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Sequencing Evolution Cancer genome and mutational processes in cancer I Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II Johan Wargentin 10:45-11:00 Break 11:00-11:45 The clinical impact of analysing the cancer genome Felix + Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Github organisation/using github Use the command line and running bioinformatic tools How to order targted sequencing assay Visualise data in R Johan & Rebecka & Sarath Wargentin","title":"Day 1 : 2021-01-24 - Monday"},{"location":"schedule/#day-2-2021-01-25-tuesday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Cancer genomics - Tissues and liquid biopsies I A targeted sequencing assay Screening of localised disease Johan Wargentin 9:45-10:00 Break 10:00-10:45 An introduction to the cancer genome and mutational processes in cancer II. Johan Wargentin 10:45-11:00 Break 11:00-11:45 Lab Introduction basic pipeline basic tools IGV Johan Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Illumina sequencing file formats. ( fastq, bam etc ) Refence genome Tools to manipulate the sequencing formats (Samtools, bedtools etc.) Visualise in IGV Johan & Rebecka & Sarath Wargentin","title":"Day 2 : 2021-01-25 - Tuesday"},{"location":"schedule/#day-3-2021-01-26-wednesday","text":"Time Topic Responsible Location 9:00-9:45 Introduction Bioinformatics pipelines HTC computing environments Sarath Wargentin 9:45-10:00 Break 10:00-10:45 Processing of DNA and RNA sequencing data QC of both DNA and RNA sequencing data Rebecka Wargentin 10:45-11:00 Break 11:00-11:45 Somatic and germline variant callers Point mutations and indels ICGC/TCGA - Identification of drivers Landmark paper from TCGA/ICGC on significantly mutated genes Lab Introduction To Be Continued... Rebecka Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Run the commands in a bash script (awk etc.) Targeted sequencing QC Add variant callers to the bioinformatic pipeline Visualise in IGV To be Continued... Johan & Rebecka & Sarath Wargentin","title":"Day 3 : 2021-01-26 - Wednesday"},{"location":"schedule/#day-4-2021-01-27-thursday","text":"Time Topic Responsible Location 9:00-9:45 Calling somatic and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Markus Wargentin 9:45-10:00 Break 10:00-10:45 Calling somatic and germline variation To be Continued... Structural variation Landmark GSR papers from ICGC/TCGA Johan Wargentin 10:45-11:00 Break 11:00-11:45 Calling RNA fusions and getting expression values Lab Introduction To Be Continued... Sarath Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Use a skeleton of a bioinformatics pipeline and add some extra steps Run the bioinformatic pipeline Plot the output in ggplot Inspecting variants Johan & Rebecka & Markus & Sarath Wargentin","title":"Day 4 : 2021-01-27 - Thursday"},{"location":"schedule/#day-5-2021-01-28-friday","text":"Time Topic Responsible Location 9:00-9:45 How to curate somatic and germline variation for clinical use Johan Wargentin 9:45-10:00 Break 10:00-10:45 Annotating somatic and germline variation David Tamborero Wargentin 10:45-11:00 Break 11:00-11:45 Annotating somatic and germline variation To Be Continued... David Tamborero Wargentin 12:00-13:00 Lunch 13:00-17:00 Excerises Annotating variants using VEP and setting the arguments correctly Curating variants manually using Curator Phenotypes David Tamborero & Johan & Rebecka & Sarath Wargentin","title":"Day 5 : 2021-01-28 - Friday"},{"location":"lab_session/basic_unix/","text":"Unix for Bioinformatics \u00b6 Outline: \u00b6 What is the command line? Directory Structure Syntax of a Command Options of a Command Command Line Basics (ls, pwd, Ctrl-C, man, alias, ls -lthra) Getting Around (cd) Absolute and Relative Paths Tab Completion History Repeats Itself (history, head, tail, ) Editing Yourself (Ctrl-A, Ctrl-E, Ctrl-K, Ctrl-W) Create and Destroy (echo, cat, rm, rmdir) Transferring Files (scp) Piping and Redirection (|, >, \u00bb, cut, sort, grep) Compressions and Archives (tar, gzip, gunzip) Forced Removal (rm -r) BASH Wildcard Characters (?, *, find, environment variables($), quotes/ticks) Manipulation of a FASTA file (cp, mv, wc -l/-c) Symbolic Links (ln -s) STDOUT and STDERR (>1, >2) Paste Command (paste, for loops) Shell Scripts and File Permissions (chmod, nano, ./) A Simple Unix cheat sheet \u00b6 Download Unix Cheat Sheet What is UNIX? \u00b6 UNIX is an operating system which was first developed in the 1960s, and has been under constant development ever since. By operating system, we mean the suite of programs which make the computer work. It is a stable, multi-user, multi-tasking system for servers, desktops and laptops. UNIX systems also have a graphical user interface (GUI) similar to Microsoft Windows which provides an easy to use environment. However, knowledge of UNIX is required for operations which aren't covered by a graphical program, or for when there is no windows interface available, for example, in a telnet session. Types of UNIX \u00b6 There are many different versions of UNIX, although they share common similarities. The most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X. Here in the School, we use Solaris on our servers and workstations, and Fedora Linux on the servers and desktop PCs. The UNIX operating system \u00b6 The UNIX operating system is made up of three parts; the kernel, the shell and the programs. The kernel \u00b6 The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore and communications in response to system calls. As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile (which has the effect of removing the file myfile). The shell searches the filestore for the file containing the program rm, and then requests the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished running, the shell then returns the UNIX prompt % to the user, indicating that it is waiting for further commands. The shell \u00b6 The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt (% on our systems). The adept user can customise his/her own shell, and users can use different shells on the same machine. This shell is an all-text display (most of the time your mouse doesn\u2019t work) and is accessed using an application called the \"terminal\" which usually looks like a black window with white letters or a white window with black letters by default. The tcsh shell has certain features to help the user inputting commands. Filename Completion - By typing part of the name of a command, filename or directory and pressing the [Tab] key, the tcsh shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will beep, prompting you to type a few more letters before pressing the tab key again. History - The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or type history for a list of previous commands. Files and processes \u00b6 Everything in UNIX is either a file or a process. A process is an executing program identified by a unique PID (process identifier). A file is a collection of data. They are created by users using text editors, running compilers etc. Examples of files: a document (report, essay etc.) the text of a program written in some high-level programming language instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file); a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files. The Directory Structure \u00b6 All the files are grouped together in the directory structure. The file-system is arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root (written as a slash / ) Absolute path: always starts with \u201d/\u201d - the root folder /home/ubuntu/workspace the folder (or file) \u201cworkspace\u201d in the folder \u201cubuntu\u201d or \" student#1 \" in the folder \u201chome\u201d in the folder from the root. Relative path: always relative to our current location. a single dot (.) refers to the current directory two dots (..) refers to the directory one level up \u00b6 Usually, /home is where the user accounts reside, ie. users\u2019 \u2018home\u2019 directories. For example, for a user that has a username of \u201c student#1 \u201d: their home directory is /home/student1. It is the directory that a user starts in after starting a new shell or logging into a remote server. The tilde (~) is a short form of a user\u2019s home directory. Starting an UNIX terminal \u00b6 \u00b6 After opening or logging into a terminal, system messages are often displayed, followed by the \u201cprompt\u201d. A prompt is a short text message at the start of the command line and ends with a $ in bash shell, commands are typed after the prompt. The prompt typically follows the form username@server:current_directory $ . If your screen looks like the one below, i.e. your see your a bunch of messages and then your ubuntu student instance number followed by \u201c @ec2-3-88-248-40 .compute-1.amazonaws.com:~$\u201d at the beginning of the line, then you are successfully logged in. INSERT UBUNTU TERMINAL SCREENSHOT AFTER LOGGING IN and highlight the username \"ubuntu\" in the login message and also annotate the image to show different components of the prompt text \u00b6 Unix Basics \u00b6 First some basics - how to look at your surroundings present working directory \u2026 where am I? \u00b6 pwd see the words separated by /. this is called the path. In unix, the location of each folder or file is shown like this.. this is more like the address or the way to find a folder or file. for example, my Desktop, Documents and Downloads folder usually are located in my home directory so in that case, it path to the folders will be /home/ubuntu/Desktop /home/ubuntu/Documents /home/ubuntu/Downloads Syntax of a command \u00b6 A command plus the required parameters/arguments The separator used in issuing a command is space, number of spaces does not matter. Now let's try some basic and simple commands \u00b6 list files here\u2026 you should see just the folders that we have created for you here and nothing else. ls list files somewhere else, like /tmp/ ls /tmp TIP! \u00b6 In unix one of the first things that\u2019s good to know is how to escape once you\u2019ve started something you don\u2019t want. Use Ctrl-c (shows as \u2018^C\u2019 in the terminal) to exit (kill) a command. In some cases, a different key sequence is required (Ctrl-d). Note that anything including and after a \u201c#\u201d symbol is ignored, i.e. a comment. So in all the commands below, you do not have to type anything including and past a \u201c#\u201d. Options \u00b6 Each command can act as a basic tool, or you can add \u2018options\u2019 or \u2018flags\u2019 that modify the default behavior of the tool. These flags come in the form of \u2018-v\u2019 \u2026 or, when it\u2019s a more descriptive word, two dashes: \u2018--verbose\u2019 \u2026 that\u2019s a common (but not universal) one that tells a tool that you want it to give you output with more detail. Sometimes, options require specifying amounts or strings, like \u2018-o results.txt\u2019 or \u2018--output results.txt\u2019 \u2026 or \u2018-n 4\u2019 or \u2018--numCPUs 4\u2019. Let\u2019s try some, and see what the man page for the \u2018list files\u2019 command \u2018ls\u2019 is like. ls -R Lists directories and files recursively. This will be a very long output, so use Ctrl-C to break out of it. Sometimes you have to press Ctrl-C many times to get the terminal to recognize it. In order to know which options do what, you can use the manual pages. To look up a command in the manual pages type \u201cman\u201d and then the command name. So to look up the options for \u201cls\u201d, type: man ls Navigate this page using the up and down arrow keys, PageUp and PageDown, and then use q to quit out of the manual. In this manual page, find the following options, quit the page, and then try those commands. You could even open another terminal, log in again, and run manual commands in that terminal. ls -l /usr/bin/ #long format, gives permission values, owner, group, size, modification time, and name Exercise: \u00b6 Feel free to see manual pages for these basic commands Commands and their Meanings ls ls -a mkdir Now see the difference between these three commands cd cd ~ cd .. change to parent directory Also these commands ls -l ls -a ls -l -a ls -la ls -ltrha ls -ltrha --color Quick aside: what if I want to use same options repeatedly? and be lazy? You can create a shortcut to another command using \u2018alias\u2019. alias ll = 'ls -lah' ll Getting Around \u00b6 The filesystem you\u2019re working on is like the branching root system of a tree. The top level, right at the root of the tree, is called the \u2018root\u2019 directory, specified by \u2018/\u2019 \u2026 which is the divider for directory addresses, or \u2018paths\u2019. Now let's see a little about the commands you checked in the exercise above. We move around using the \u2018change directory\u2019 command, \u2018cd\u2019. The command pwd return the present working directory. cd # no effect? that's because by itself it sends you home (to ~) cd / # go to root of tree's root system cd home # go to where everyone's homes are pwd cd username # use your actual home, not \"username\" pwd cd / pwd cd ~ # a shortcut to home, from anywhere pwd cd . # '.' always means *this* directory pwd cd .. # '..' always means *one directory up* pwd Absolute and Relative Paths \u00b6 You can think of paths like addresses. You can tell your friend how to go to a particular store from where they are currently (a \u2018relative\u2019 path), or from the main Interstate Highway that everyone uses (in this case, the root of the filesystem, \u2018/\u2019 \u2026 this is an \u2018absolute\u2019 path). Both are valid. But absolute paths can\u2019t be confused, because they always start off from the same place, and are unique. Relative paths, on the other hand, could be totally wrong for your friend if you assume they\u2019re somewhere they\u2019re not. With this in mind, let\u2019s try a few more: cd /usr/bin # let's start in /usr/bin #relative (start here, take one step up, then down through lib and gcc) cd ../lib/init/ pwd #absolute (start at root, take steps) cd /usr/lib/init/ pwd Now, because it can be a real pain to type out, or remember these long paths, we need to discuss \u2026 Tab Completion \u00b6 Using tab-completion is a must on the command line. A single <tab> auto-completes file or directory names when there\u2019s only one name that could be completed correctly. If multiple files could satisfy the tab-completion, then nothing will happen after the first <tab> . In this case, press <tab> a second time to list all the possible completing names. Note that if you\u2019ve already made a mistake that means that no files will ever be completed correctly from its current state, then <tab> \u2019s will do nothing. touch updates the timestamp on a file, here we use it to create three empty files. cd # go to your home directory mkdir ~/tmp cd ~/tmp touch one seven september ls o tab with no enter should complete to \u2018one\u2019, then enter ls s tab with no enter completes up to \u2018se\u2019 since that\u2019s in common between seven and september. tab again and no enter, this second tab should cause listing of seven and september. type \u2018v\u2019 then tab and no enter now it\u2019s unique to seven, and should complete to seven. enter runs \u2018ls seven\u2019 command. It cannot be overstated how useful tab completion is. You should get used to using it constantly. Watch experienced users type and they maniacally hit tab once or twice in between almost every character. You don\u2019t have to go that far, of course, but get used to constantly getting feedback from hitting tab and you will save yourself a huge amount of typing and trying to remember weird directory and filenames. TIME TO SHIFT GEARS AND PICK UP SOME SPEED NOW! \u00b6 History Repeats Itself \u00b6 Linux remembers everything you\u2019ve done (at least in the current shell session), which allows you to pull steps from your history, potentially modify them, and redo them. This can obviously save a lot of time and typing. The \u2018head\u2019 command views the first 10 (by default) lines of a file. The \u2018tail\u2019 commands views the last 10 (by default) lines of a file. Type \u2018man head\u2019 or \u2018man tail\u2019 to consult their manuals. <up arrow> # last command <up> # next-to-last command <down> # last command, again <down> # current command, empty or otherwise history # usually too much for one screen, so ... history | head # we discuss pipes (the vertical bar) below history | tail history | less # use 'q' to exit less ls -l pwd history | tail !560 # re-executes 560th command (yours will have different numbers; choose the one that recreates your really important result!) Editing Yourself \u00b6 Here are some more ways to make editing previous commands, or novel commands that you\u2019re building up, easier: <up><up> # go to some previous command, just to have something to work on <ctrl-a> # go to the beginning of the line <ctrl-e> # go to the end of the line #now use left and right to move to a single word (surrounded by whitespace: spaces or tabs) <ctrl-k> # delete from here to end of line <ctrl-w> # delete from here to beginning of preceeding word blah blah blah<ctrl-w><ctrl-w> # leaves you with only one 'blah' You can also search your history from the command line: <ctrl-r>fir # should find most recent command containing 'fir' string: echo 'first' > test.txt <enter> # to run command <ctrl-c> # get out of recursive search <ctr-r> # repeat <ctrl-r> to find successively older string matches Create and Destroy \u00b6 We already learned one command that will create a file, touch. Now let\u2019s look at create and removing files and directories. cd # home again mkdir ~/tmp2 cd ~/tmp2 echo 'Hello, world!' > first.txt echo text then redirect ( \u2018>\u2019 ) to a file. cat first.txt # 'cat' means 'concatenate', or just spit the contents of the file to the screen why \u2018concatenate\u2019? try this: \u00b6 cat first.txt first.txt first.txt > second.txt cat second.txt OK, let\u2019s destroy what we just created: cd ../ rmdir tmp2 # 'rmdir' meands 'remove directory', but this shouldn't work! rm tmp2/first.txt rm tmp2/second.txt # clear directory first rmdir tmp2 # should succeed now So, \u2018mkdir\u2019 and \u2018rmdir\u2019 are used to create and destroy (empty) directories. \u2018rm\u2019 to remove files. To create a file can be as simple as using \u2018echo\u2019 and the \u2018>\u2019 (redirection) character to put text into a file. Even simpler is the \u2018touch\u2019 command. mkdir ~/cli cd ~/cli touch newFile ls -ltra # look at the time listed for the file you just created cat newFile # it's empty! sleep 60 # go grab some coffee touch newFile ls -ltra # same time? So \u2018touch\u2019 creates empty files, or updates the \u2018last modified\u2019 time. Note that the options on the \u2018ls\u2019 command you used here give you a Long listing, of All files, in Reverse Time order (l, a, r, t). Forced Removal (CAUTION!!!) \u00b6 When you\u2019re on the command line, there\u2019s no \u2018Recycle Bin\u2019. Since we\u2019ve expanded a whole directory tree, we need to be able to quickly remove a directory without clearing each subdirectory and using \u2018rmdir\u2019. cd mkdir -p rmtest/dir1/dir2 # the -p option creates all the directories at once rmdir rmtest # gives an error since rmdir can only remove directories that are empty rm -rf rmtest # will remove the directory and EVERYTHING in it Here -r = recursively remove sub-directories, -f means force. Obviously, be careful with \u2018rm -rf\u2019 , there is no going back , if you delete something with rm, rmdir its gone! There is no Recycle Bin on the Command-Line! Piping and Redirection \u00b6 Pipes (\u2018|\u2019) allow commands to hand output to other commands, and redirection characters (\u2018>\u2019 and \u2018\u00bb\u2019) allow you to put output into files. echo 'first' > test.txt cat test.txt # outputs the contents of the file to the terminal echo 'second' > test.txt cat test.txt echo 'third' >> test.txt cat test.txt The \u2018>\u2019 character redirects output of a command that would normally go to the screen instead into a specified file. \u2018>\u2019 overwrites the file, \u2018\u00bb\u2019 appends to the file. The \u2018cut\u2019 command pieces of lines from a file line by line. This command cuts characters 1 to 3, from every line, from file \u2018test.txt\u2019 cut -c 1 -3 test.txt same thing, piping output of one command into input of another cat test.txt | cut -c 1 -3 This pipes (i.e., sends the output of) cat to cut to sort (-r means reverse order sort), and then grep searches for pattern (\u2018s\u2019) matches (i.e. for any line where an \u2018s\u2019 appears anywhere on the line.) cat test.txt | cut -c 1 -3 | sort -r cat test.txt | cut -c 1 -3 | sort -r | grep s This is a great way to build up a set of operations while inspecting the output of each step in turn. We\u2019ll do more of this in a bit. Compression and Archives \u00b6 As file sizes get large, you\u2019ll often see compressed files, or whole compressed folders. Note that any good bioinformatics software should be able to work with compressed file formats. gzip test.txt cat test.txt.gz To uncompress a file gunzip -c test.txt.gz The \u2018-c\u2019 leaves the original file alone, but dumps expanded output to screen gunzip test.txt.gz # now the file should change back to uncompressed test.txt Tape archives, or .tar files, are one way to compress entire folders and all contained folders into one file. When they\u2019re further compressed they\u2019re called \u2018tarballs\u2019. We can use wget (web get). wget -L -O PhiX_Illumina_RTA.tar.gz http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/PhiX/Illumina/RTA/PhiX_Illumina_RTA.tar.gz The .tar.gz and .tgz are commonly used extensions for compressed tar files, when gzip compression is used. The application tar is used to uncompress .tar files tar -xzvf PhiX_Illumina_RTA.tar.gz Here -x = extract, -z = use gzip/gunzip, -v = verbose (show each file in archive), -f filename Note that, unlike Windows, linux does not depend on file extensions to determine file behavior. So you could name a tarball \u2018fish.puppy\u2019 and the extract command above should work just fine. The only thing that should be different is that tab-completion doesn\u2019t work within the \u2018tar\u2019 command if it doesn\u2019t see the \u2018correct\u2019 file extension. BASH Wildcard Characters \u00b6 We can use \u2018wildcard characters\u2019 when we want to specify or operate on sets of files all at once. ls ?hiX/Illumina list files in Illumina sub-directory of any directory ending in \u2018hiX\u2019 ls PhiX/Illumina/RTA/Sequence/*/*.fa list all files ending in \u2018.fa\u2019 a few directories down. So, \u2018?\u2019 fills in for zero or one character, \u2018*\u2019 fills in for zero or more characters. The \u2018find\u2019 command can be used to locate files using a similar form. find . -name \"*.f*\" find . -name \"*.f?\" how is this different from the previous ls commands? Quick Note About the Quote(s) \u00b6 The quote characters \u201c and \u2018 are different. In general, single quotes preserve the literal meaning of all characters between them. On the other hand, double quotes allow the shell to see what\u2019s between them and make substitutions when appropriate. For example: VRBL = someText echo '$VRBL' echo \" $VRBL \" However, some commands try to be \u2018smarter\u2019 about this behavior, so it\u2019s a little hard to predict what will happen in all cases. It\u2019s safest to experiment first when planning a command that depends on quoting \u2026 list filenames first, instead of changing them, etc. Finally, the \u2018backtick\u2019 characters ` (same key - unSHIFTED - as the tilde ~) causes the shell to interpret what\u2019s between them as a command, and return the result. counts the number of lines in file and stores result in the LINES variable \u00b6 LINES = ` cat PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome.1.bt2 | wc -l ` echo $LINES Symbolic Links \u00b6 Since copying or even moving large files (like sequence data) around your filesystem may be impractical, we can use links to reference \u2018distant\u2019 files without duplicating the data in the files. Symbolic links are disposable pointers that refer to other files, but behave like the referenced files in commands. I.e., they are essentially \u2018Shortcuts\u2019 (to use a Windows term) to a file or directory. The \u2018ln\u2019 command creates a link. You should, by default, always create a symbolic link using the -s option. ln -s PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa . ls -ltrhaF # notice the symbolic link pointing at its target grep -c \">\" genome.fa STDOUT & STDERR \u00b6 Programs can write to two separate output streams, \u2018standard out\u2019 (STDOUT), and \u2018standard error\u2019 (STDERR). The former is generally for direct output of a program, while the latter is supposed to be used for reporting problems. I\u2019ve seen some bioinformatics tools use STDERR to report summary statistics about the output, but this is probably bad practice. Default behavior in a lot of cases is to dump both STDOUT and STDERR to the screen, unless you specify otherwise. In order to nail down what goes where, and record it for posterity: wc -c genome.fa 1 > chars.txt 2 > any.err the 1 st output, STDOUT, goes to \u2018chars.txt\u2019 the 2 nd output, STDERR, goes to \u2018any.err\u2019 cat chars.txt Contains the character count of the file genome.fa cat any.err Empty since no errors occured. Saving STDOUT is pretty routine (you want your results, yes?), but remember that explicitly saving STDERR is important on a remote server, since you may not directly see the \u2018screen\u2019 when you\u2019re running jobs. The sed command \u00b6 Let\u2019s take a look at the \u2018sed\u2019 command. NOTE: On Macs use \u2018gsed\u2019. sed (short for stream editor) is a command that allows you to manipulate character data in various ways. One useful thing it can do is substitution. Let\u2019s download a simple file to work on: wget region.bed -O region.bed Take a look at the file: cat region.bed Now, let\u2019s make all the uppercase \u201cCHR\u201ds into lowercase: cat region.bed | sed 's/CHR/chr/' What happened? Only the first CHR changed. That is because we need to add the \u201cg\u201d option: cat region.bed | sed 's/CHR/chr/g' We can also do the the substitution without regards to case: cat region.bed | sed 's/chr/chr/gi' Let\u2019s break down the argument to sed (within the single quotes)\u2026 The \u201cs\u201d means \u201csubstitute\u201d, the word between the 1 st and 2 nd forward slashes (i.e. /) is the word the substitute for, the word between the 2 nd and 3 rd slashes is the word to substitute with, and finally the \u201cgi\u201d at the end are flags for global substitution (i.e. substituting along an entire line instead of just the first occurence on a line), and for case insenstivity (i.e. it will ignore the case of the letters when doing the substitution). Note that this doesn\u2019t change the file itself, it is simply piping the output of the cat command to sed and outputting to the screen. If you wanted to change the file itself, you could use the \u201c-i\u201d option to sed: cat region.bed sed -i 's/chr/chr/gi' region.bed Now if you look at the file, the lines have changed. cat region.bed Another useful use of sed is for capturing certain lines from a file. You can select certain lines from a file: sed '4q;d' region.bed This will just select the 4 th line from the file. You can also extract a range of lines from a file: sed -n '10,20p' region.bed This gets the 10 th through 20 th lines from the file. CHALLENGE: See if you can find a way to use sed to remove all the \u201cCHR\u201ds from the file. More pipes \u00b6 Now, let\u2019s delve into pipes a little more. Pipes are a very powerful way to look at and manipulate complex data using a series of simple programs. Let\u2019s look at some fastq files. Get a few small fastq files: wget C61.subset.fq.gz -O C61.subset.fq.gz wget I561.subset.fq.gz -O I561.subset.fq.gz wget I894.subset.fq.gz -O I894.subset.fq.gz Since the files are gzipped files we need to use \u201czcat\u201d to look at them. zcat is just like cat except for gzipped files: zcat C61.subset.fq.gz | head Fastq records are 4 lines per sequence, a header line, the sequence, a plus sign (which is historical), and then the quality encoding for the sequence. Notice that each header line has the barcode for that read at the end of the line. Let\u2019s count the number of each barcode. In order to do that we need to just capture the header lines from this file. We can use \u201csed\u201d to do that: zcat C61.subset.fq.gz | sed -n '1~4p' | head By default sed prints every line. In this case we are giving the \u201c-n\u201d option to sed which will not print every line. Instead, we are giving it the argument \u201c1~4p\u201d, which means to print the first line, then skip 4 lines and print again, and then continue to do that. Now that we have a way to get just the headers, we need to isolate the part of the header that is the barcode. There are multiple ways to do this\u2026 we will use the cut command: zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | head So we are using the \u201c-d\u201d option to cut with \u201c:\u201d as the argument to that option, meaning that we will be using the delimiter \u201c:\u201d to split the input. Then we use the \u201c-f\u201d option with argument \u201c10\u201d, meaning that we want the 10 th field after the split. In this case, that is the barcode. Finally, as before, we need to sort the data and then use \u201cuniq -c\u201d to count. Then put it all together and run it on the entire dataset (This will take about a minute to run): zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | sort | uniq -c Now you have a list of how many reads were categorized into each barcode. Here is a sed tutorial for more exercises. CHALLENGE: Find the distribution of the first 5 bases of all the reads in C61_S67_L006_R1_001.fq.gz. I.e., count the number of times the first 5 bases of every read occurs across all reads. Loops \u00b6 Loops are useful for quickly telling the shell to perform one operation after another, in series. For example: for i in { 1 ..21 } ; do echo $i >> a ; done # put multiple lines of code on one line, each line terminated by ';' cat a <1 through 21 on separate lines> \u00b6 The general form is: for name in { list } ; do commands done The list can be a sequence of numbers or letters, or a group of files specified with wildcard characters: for i in { 3 ,2,1,liftoff } ; do echo $i ; done # needs more excitement! for i in { 3 ,2,1, \"liftoff!\" } ; do echo $i ; done # exclamation point will confuse the shell unless quoted A \u201cwhile\u201d loop is more convenient than a \u201cfor\u201d loop \u2026 if you don\u2019t readily know how many iterations of the loop you want: while { condition } ; do commands done Now, let\u2019s do some bioinformatics-y things with loops and pipes. First, let\u2019s write a command to get the nucleotide count of the first 10,000 reads in a file. Use zcat and sed to get only the read lines of a file, and then only take the first 10,000: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | less Use grep\u2019s \u201c-o\u201d option to get each nucleotide on a separate line (take a look at the man page for grep to understand how this works): zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | less Finally, use sort and uniq to get the counts: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c 264012 A 243434 C 215045 G 278 N 277231 T And, voila, we have the per nucleotide count for these reads! We just did this for one file, but what if we wanted to do it for all of our files? We certainly don\u2019t want to type the command by hand dozens of times. So we\u2019ll use a while loop. You can pipe a command into a while loop and it will iterate through each line of the input. First, get a listing of all your files: ls -1 *.fq.gz Pipe that into a while loop and read in the lines into a variable called \u201cx\u201d. We use \u201c$x\u201d to get the value of the variable in that iteration of the loop: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; done Add the command we created above into the loop, placing $x where the filename would be and semi-colons inbetween commands: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c ; done When this runs it will print the name of every single file being processed and the nucleotide count for the reads from those files. Now, let\u2019s say you wanted to write the output of each command to a separate file. We would redirect the output to a filename, but we need to create a different file name for each command and we want the file name to reflect its contents, i.e. the output file name should be based on the input file name. So we use \u201cparameter expansion\u201d, which is fancy way of saying substitution: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; done This will put the output of the counting command into a file whose name is the prefix of the input file plus \u201c.nucl_count.txt\u201d. It will do this for every input file. Manipulation of a FASTA File \u00b6 Let\u2019s copy the phiX-174 genome (using the \u2018cp\u2019 command) to our current directory so we can play with it: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa phix.fa Similarly we can also use the move command here, but then ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa will no longer be there: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ mv ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa phix.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ This functionality of mv is why it is used to rename files. Note how we copied the \u2018genome.fa\u2019 file to a different name: \u2018phix.fa\u2019 wc -l phix.fa count the number of lines in the file using \u2018wc\u2019 (word count) and parameter \u2018-l\u2019 (lines). We can use the \u2018grep\u2019 command to search for matches to patterns. \u2018grep\u2019 comes from \u2018globally search for a regular expression and print\u2019. grep -c '>' phix.fa #Only one FASTA sequence entry, since only one header line (\u2018>gi|somethingsomething\u2026\u2019) cat phix.fa This may not be useful for anything larger than a virus! Let\u2019s look at the start codon and the two following codons: grep --color \"ATG......\" phix.fa #\u2019.\u2019 characters are the single-character wildcards for grep. So \u201cATG\u2026\u2026\u201d matches any set of 9 characters that starts with ATG. #Use the \u2013color \u2018-o\u2019 option to only print the pattern matches, one per line grep -o \"ATG......\" phix.fa #Use the \u2018cut\u2019 command with \u2018-c\u2019 to select characters 4-6, the second codon grep --color -o \"ATG......\" phix.fa | cut -c4-6 \u2018sort\u2019 the second codon sequences ( default order is same as ASCII table ; see \u2018man ascii\u2019 ) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort #Combine successive identical sequences, but count them using the \u2018uniq\u2019 command with the \u2018-c\u2019 option grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c #Finally sort using reverse numeric order (\u2018-rn\u2019) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c | sort -rn \u2026 which gives us the most common codons first This may not be a particularly useful thing to do with a genomic FASTA file, but it illustrates the process by which one can build up a string of operations, using pipes, in order to ask quantitative questions about sequence content. More generally than that, this process allows one to ask questions about files and file contents and the operating system, and verify at each step that the process so far is working as expected. The command line is, in this sense, really a modular workflow management system. Shell Scripts, File Permissions \u00b6 Often it\u2019s useful to define a whole string of commands to run on some input, so that (1) you can be sure you\u2019re running the same commands on all data, and (2) so you don\u2019t have to type the same commands in over and over! Let\u2019s use the \u2018nano\u2019 text editor program that\u2019s pretty reliably installed on most linux systems. nano test.sh insert cli_figure7 \u00b6 nano now occupies the whole screen; see commands at the bottom. Let\u2019s type in a few commands. First we need to put the following line at the top of the file: #!/bin/bash The \u201c#!\u201d at the beginning of a script tells the shell what language to use to interpret the rest of the script. In our case, we will be writing \u201cbash\u201d commands, so we specify the full path of the bash executable after the \u201c#!\u201d. Then, add some commands: #!/bin/bash echo \"Start script...\" pwd ls -l sleep 10 echo \"End script.\" Hit Cntl-O and then enter to save the file, and then Cntl-X to exit nano. Though there are ways to run the commands in test.sh right now, it\u2019s generally useful to give yourself (and others) \u2018execute\u2019 permissions for test.sh, really making it a shell script. Note the characters in the first (left-most) field of the file listing: ls -lh test.sh -rw-rw-r-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first \u2018-\u2018 becomes a \u2018d\u2019 if the \u2018file\u2019 is actually a directory. The next three characters represent read, write, and execute permissions for the file owner (you), followed by three characters for users in the owner\u2019s group, followed by three characters for all other users. Run the \u2018chmod\u2019 command to change permissions for the \u2018test.sh\u2019 file, adding execute permissions (\u2018+x\u2019) for the user (you) and your group (\u2018ug\u2019): chmod ug+x test.sh ls -lh test.sh -rwxr-xr-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first 10 characters of the output represent the file and permissions. The first character is the file type, the next three sets of three represent the file permissions for the user, group, and everyone respectively. r = read w = write x = execute So let\u2019s run this script. We have to provide a relative reference to the script \u2018./\u2019 because its not our our \u201cPATH\u201d.: #you can do either ./test.sh #or you can run it like this bash test.sh And you should see all the commands in the file run in sequential order in the terminal. Command Line Arguments for Shell Scripts \u00b6 Now let\u2019s modify our script to use command line arguments, which are arguments that can come after the script name (when executing) to be part of the input inside the script. This allows us to use the same script with different inputs. In order to do so, we add variables $1, $2, $3, etc\u2026. in the script where we want our input to be. So, for example, use nano to modify your test.sh script to look like this: #!/bin/bash echo \"Start script...\" PWD = ` pwd ` echo \"The present working directory is $PWD \" ls -l $1 sleep $2 wc -l $3 echo \"End script.\" Now, rerun the script using command line arguments like this: ./test.sh genome.fa 15 PhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt Note that each argument is separated by a space, so $1 becomes \u201cgenome.fa\u201d, $2 becomes \u201c15\u201d, and $3 becomes \u201cPhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\u201d. Then the commands are run using those values. Now rerun the script with some other values: ./test.sh .. 5 genome.fa Now, $1 becomes \u201c..\u201d, $2 is \u201c5\u201d, and $3 is \u201cgenome.fa\u201d. Pipes and Loops inside scripts \u00b6 Open a new text file using the text editor \u201cnano\u201d: nano get_nucl_counts.sh Copy and Paste the following into the file: #!/bin/bash zcat $1 | sed -n '2~4p' | head - $2 | grep -o . | sort | uniq -c Save the file and exit. Change the permissions on the file to make it executable: chmod a+x get_nucl_counts.sh Now, we can run this script giving it different arguments every time. The first argument (i.e. the first text after the script name when it is run) will get put into the variable \u201c \\(1\u201d. The second argument (delimited by spaces) will get put into \u201c\\) 2\u201d. In this case, \u201c \\(1\u201d is the file name, and \u201c\\) 2\u201d is the number of reads we want to count. So, then we can run the script over and over again using different values and the command will run based on those values: ./get_nucl_counts.sh I561.subset.fq.gz 1000 ./get_nucl_counts.sh I561.subset.fq.gz 10000 ./get_nucl_counts.sh C61.subset.fq.gz 555 We can also put loops into a script. We\u2019ll take the loop we created earlier and put it into a file, breaking it up for readability and using backslashes for line continuation: nano get_nucl_counts_loop.sh Put this in the file and save it: #!/bin/bash ls -1 *.fq.gz | \\ while read x ; do \\ echo $x is being processed... ; \\ zcat $x | sed -n '2~4p' | head - $1 | \\ grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; \\ done Make it executable: chmod a+x get_nucl_counts_loop.sh And now we can execute the entire loop using the script. Note that there is only one argument now, the number of reads to use: ./get_nucl_counts_loop.sh 100 FINALLY - a good summary for rounding up the session \u00b6 TEN SIMPLE RULES FOR GETTING STARTED WITH COMMAND-LINE BIOINFORMATICS","title":"Basic Unix"},{"location":"lab_session/basic_unix/#unix-for-bioinformatics","text":"","title":"Unix for Bioinformatics"},{"location":"lab_session/basic_unix/#outline","text":"What is the command line? Directory Structure Syntax of a Command Options of a Command Command Line Basics (ls, pwd, Ctrl-C, man, alias, ls -lthra) Getting Around (cd) Absolute and Relative Paths Tab Completion History Repeats Itself (history, head, tail, ) Editing Yourself (Ctrl-A, Ctrl-E, Ctrl-K, Ctrl-W) Create and Destroy (echo, cat, rm, rmdir) Transferring Files (scp) Piping and Redirection (|, >, \u00bb, cut, sort, grep) Compressions and Archives (tar, gzip, gunzip) Forced Removal (rm -r) BASH Wildcard Characters (?, *, find, environment variables($), quotes/ticks) Manipulation of a FASTA file (cp, mv, wc -l/-c) Symbolic Links (ln -s) STDOUT and STDERR (>1, >2) Paste Command (paste, for loops) Shell Scripts and File Permissions (chmod, nano, ./)","title":"Outline:"},{"location":"lab_session/basic_unix/#a-simple-unix-cheat-sheet","text":"Download Unix Cheat Sheet","title":"A Simple Unix cheat sheet"},{"location":"lab_session/basic_unix/#what-is-unix","text":"UNIX is an operating system which was first developed in the 1960s, and has been under constant development ever since. By operating system, we mean the suite of programs which make the computer work. It is a stable, multi-user, multi-tasking system for servers, desktops and laptops. UNIX systems also have a graphical user interface (GUI) similar to Microsoft Windows which provides an easy to use environment. However, knowledge of UNIX is required for operations which aren't covered by a graphical program, or for when there is no windows interface available, for example, in a telnet session.","title":"What is UNIX?"},{"location":"lab_session/basic_unix/#types-of-unix","text":"There are many different versions of UNIX, although they share common similarities. The most popular varieties of UNIX are Sun Solaris, GNU/Linux, and MacOS X. Here in the School, we use Solaris on our servers and workstations, and Fedora Linux on the servers and desktop PCs.","title":"Types of UNIX"},{"location":"lab_session/basic_unix/#the-unix-operating-system","text":"The UNIX operating system is made up of three parts; the kernel, the shell and the programs.","title":"The UNIX operating system"},{"location":"lab_session/basic_unix/#the-kernel","text":"The kernel of UNIX is the hub of the operating system: it allocates time and memory to programs and handles the filestore and communications in response to system calls. As an illustration of the way that the shell and the kernel work together, suppose a user types rm myfile (which has the effect of removing the file myfile). The shell searches the filestore for the file containing the program rm, and then requests the kernel, through system calls, to execute the program rm on myfile. When the process rm myfile has finished running, the shell then returns the UNIX prompt % to the user, indicating that it is waiting for further commands.","title":"The kernel"},{"location":"lab_session/basic_unix/#the-shell","text":"The shell acts as an interface between the user and the kernel. When a user logs in, the login program checks the username and password, and then starts another program called the shell. The shell is a command line interpreter (CLI). It interprets the commands the user types in and arranges for them to be carried out. The commands are themselves programs: when they terminate, the shell gives the user another prompt (% on our systems). The adept user can customise his/her own shell, and users can use different shells on the same machine. This shell is an all-text display (most of the time your mouse doesn\u2019t work) and is accessed using an application called the \"terminal\" which usually looks like a black window with white letters or a white window with black letters by default. The tcsh shell has certain features to help the user inputting commands. Filename Completion - By typing part of the name of a command, filename or directory and pressing the [Tab] key, the tcsh shell will complete the rest of the name automatically. If the shell finds more than one name beginning with those letters you have typed, it will beep, prompting you to type a few more letters before pressing the tab key again. History - The shell keeps a list of the commands you have typed in. If you need to repeat a command, use the cursor keys to scroll up and down the list or type history for a list of previous commands.","title":"The shell"},{"location":"lab_session/basic_unix/#files-and-processes","text":"Everything in UNIX is either a file or a process. A process is an executing program identified by a unique PID (process identifier). A file is a collection of data. They are created by users using text editors, running compilers etc. Examples of files: a document (report, essay etc.) the text of a program written in some high-level programming language instructions comprehensible directly to the machine and incomprehensible to a casual user, for example, a collection of binary digits (an executable or binary file); a directory, containing information about its contents, which may be a mixture of other directories (subdirectories) and ordinary files.","title":"Files and processes"},{"location":"lab_session/basic_unix/#the-directory-structure","text":"All the files are grouped together in the directory structure. The file-system is arranged in a hierarchical structure, like an inverted tree. The top of the hierarchy is traditionally called root (written as a slash / ) Absolute path: always starts with \u201d/\u201d - the root folder /home/ubuntu/workspace the folder (or file) \u201cworkspace\u201d in the folder \u201cubuntu\u201d or \" student#1 \" in the folder \u201chome\u201d in the folder from the root. Relative path: always relative to our current location. a single dot (.) refers to the current directory two dots (..) refers to the directory one level up","title":"The Directory Structure"},{"location":"lab_session/basic_unix/#_1","text":"Usually, /home is where the user accounts reside, ie. users\u2019 \u2018home\u2019 directories. For example, for a user that has a username of \u201c student#1 \u201d: their home directory is /home/student1. It is the directory that a user starts in after starting a new shell or logging into a remote server. The tilde (~) is a short form of a user\u2019s home directory.","title":""},{"location":"lab_session/basic_unix/#starting-an-unix-terminal","text":"","title":"Starting an UNIX terminal"},{"location":"lab_session/basic_unix/#_2","text":"After opening or logging into a terminal, system messages are often displayed, followed by the \u201cprompt\u201d. A prompt is a short text message at the start of the command line and ends with a $ in bash shell, commands are typed after the prompt. The prompt typically follows the form username@server:current_directory $ . If your screen looks like the one below, i.e. your see your a bunch of messages and then your ubuntu student instance number followed by \u201c @ec2-3-88-248-40 .compute-1.amazonaws.com:~$\u201d at the beginning of the line, then you are successfully logged in.","title":""},{"location":"lab_session/basic_unix/#insert-ubuntu-terminal-screenshot-after-logging-in-and-highlight-the-username-ubuntu-in-the-login-message-and-also-annotate-the-image-to-show-different-components-of-the-prompt-text","text":"","title":"INSERT  UBUNTU TERMINAL SCREENSHOT AFTER LOGGING IN and highlight the username \"ubuntu\" in the login message and also annotate the image to show different components of the prompt text"},{"location":"lab_session/basic_unix/#unix-basics","text":"First some basics - how to look at your surroundings","title":"Unix Basics"},{"location":"lab_session/basic_unix/#present-working-directory-where-am-i","text":"pwd see the words separated by /. this is called the path. In unix, the location of each folder or file is shown like this.. this is more like the address or the way to find a folder or file. for example, my Desktop, Documents and Downloads folder usually are located in my home directory so in that case, it path to the folders will be /home/ubuntu/Desktop /home/ubuntu/Documents /home/ubuntu/Downloads","title":"present working directory \u2026 where am I?"},{"location":"lab_session/basic_unix/#syntax-of-a-command","text":"A command plus the required parameters/arguments The separator used in issuing a command is space, number of spaces does not matter.","title":"Syntax of a command"},{"location":"lab_session/basic_unix/#now-lets-try-some-basic-and-simple-commands","text":"list files here\u2026 you should see just the folders that we have created for you here and nothing else. ls list files somewhere else, like /tmp/ ls /tmp","title":"Now let's try some basic and simple commands"},{"location":"lab_session/basic_unix/#tip","text":"In unix one of the first things that\u2019s good to know is how to escape once you\u2019ve started something you don\u2019t want. Use Ctrl-c (shows as \u2018^C\u2019 in the terminal) to exit (kill) a command. In some cases, a different key sequence is required (Ctrl-d). Note that anything including and after a \u201c#\u201d symbol is ignored, i.e. a comment. So in all the commands below, you do not have to type anything including and past a \u201c#\u201d.","title":"TIP!"},{"location":"lab_session/basic_unix/#options","text":"Each command can act as a basic tool, or you can add \u2018options\u2019 or \u2018flags\u2019 that modify the default behavior of the tool. These flags come in the form of \u2018-v\u2019 \u2026 or, when it\u2019s a more descriptive word, two dashes: \u2018--verbose\u2019 \u2026 that\u2019s a common (but not universal) one that tells a tool that you want it to give you output with more detail. Sometimes, options require specifying amounts or strings, like \u2018-o results.txt\u2019 or \u2018--output results.txt\u2019 \u2026 or \u2018-n 4\u2019 or \u2018--numCPUs 4\u2019. Let\u2019s try some, and see what the man page for the \u2018list files\u2019 command \u2018ls\u2019 is like. ls -R Lists directories and files recursively. This will be a very long output, so use Ctrl-C to break out of it. Sometimes you have to press Ctrl-C many times to get the terminal to recognize it. In order to know which options do what, you can use the manual pages. To look up a command in the manual pages type \u201cman\u201d and then the command name. So to look up the options for \u201cls\u201d, type: man ls Navigate this page using the up and down arrow keys, PageUp and PageDown, and then use q to quit out of the manual. In this manual page, find the following options, quit the page, and then try those commands. You could even open another terminal, log in again, and run manual commands in that terminal. ls -l /usr/bin/ #long format, gives permission values, owner, group, size, modification time, and name","title":"Options"},{"location":"lab_session/basic_unix/#exercise","text":"Feel free to see manual pages for these basic commands Commands and their Meanings ls ls -a mkdir Now see the difference between these three commands cd cd ~ cd .. change to parent directory Also these commands ls -l ls -a ls -l -a ls -la ls -ltrha ls -ltrha --color Quick aside: what if I want to use same options repeatedly? and be lazy? You can create a shortcut to another command using \u2018alias\u2019. alias ll = 'ls -lah' ll","title":"Exercise:"},{"location":"lab_session/basic_unix/#getting-around","text":"The filesystem you\u2019re working on is like the branching root system of a tree. The top level, right at the root of the tree, is called the \u2018root\u2019 directory, specified by \u2018/\u2019 \u2026 which is the divider for directory addresses, or \u2018paths\u2019. Now let's see a little about the commands you checked in the exercise above. We move around using the \u2018change directory\u2019 command, \u2018cd\u2019. The command pwd return the present working directory. cd # no effect? that's because by itself it sends you home (to ~) cd / # go to root of tree's root system cd home # go to where everyone's homes are pwd cd username # use your actual home, not \"username\" pwd cd / pwd cd ~ # a shortcut to home, from anywhere pwd cd . # '.' always means *this* directory pwd cd .. # '..' always means *one directory up* pwd","title":"Getting Around"},{"location":"lab_session/basic_unix/#absolute-and-relative-paths","text":"You can think of paths like addresses. You can tell your friend how to go to a particular store from where they are currently (a \u2018relative\u2019 path), or from the main Interstate Highway that everyone uses (in this case, the root of the filesystem, \u2018/\u2019 \u2026 this is an \u2018absolute\u2019 path). Both are valid. But absolute paths can\u2019t be confused, because they always start off from the same place, and are unique. Relative paths, on the other hand, could be totally wrong for your friend if you assume they\u2019re somewhere they\u2019re not. With this in mind, let\u2019s try a few more: cd /usr/bin # let's start in /usr/bin #relative (start here, take one step up, then down through lib and gcc) cd ../lib/init/ pwd #absolute (start at root, take steps) cd /usr/lib/init/ pwd Now, because it can be a real pain to type out, or remember these long paths, we need to discuss \u2026","title":"Absolute and Relative Paths"},{"location":"lab_session/basic_unix/#tab-completion","text":"Using tab-completion is a must on the command line. A single <tab> auto-completes file or directory names when there\u2019s only one name that could be completed correctly. If multiple files could satisfy the tab-completion, then nothing will happen after the first <tab> . In this case, press <tab> a second time to list all the possible completing names. Note that if you\u2019ve already made a mistake that means that no files will ever be completed correctly from its current state, then <tab> \u2019s will do nothing. touch updates the timestamp on a file, here we use it to create three empty files. cd # go to your home directory mkdir ~/tmp cd ~/tmp touch one seven september ls o tab with no enter should complete to \u2018one\u2019, then enter ls s tab with no enter completes up to \u2018se\u2019 since that\u2019s in common between seven and september. tab again and no enter, this second tab should cause listing of seven and september. type \u2018v\u2019 then tab and no enter now it\u2019s unique to seven, and should complete to seven. enter runs \u2018ls seven\u2019 command. It cannot be overstated how useful tab completion is. You should get used to using it constantly. Watch experienced users type and they maniacally hit tab once or twice in between almost every character. You don\u2019t have to go that far, of course, but get used to constantly getting feedback from hitting tab and you will save yourself a huge amount of typing and trying to remember weird directory and filenames.","title":"Tab Completion"},{"location":"lab_session/basic_unix/#time-to-shift-gears-and-pick-up-some-speed-now","text":"","title":"TIME TO SHIFT GEARS AND PICK UP SOME SPEED NOW!"},{"location":"lab_session/basic_unix/#history-repeats-itself","text":"Linux remembers everything you\u2019ve done (at least in the current shell session), which allows you to pull steps from your history, potentially modify them, and redo them. This can obviously save a lot of time and typing. The \u2018head\u2019 command views the first 10 (by default) lines of a file. The \u2018tail\u2019 commands views the last 10 (by default) lines of a file. Type \u2018man head\u2019 or \u2018man tail\u2019 to consult their manuals. <up arrow> # last command <up> # next-to-last command <down> # last command, again <down> # current command, empty or otherwise history # usually too much for one screen, so ... history | head # we discuss pipes (the vertical bar) below history | tail history | less # use 'q' to exit less ls -l pwd history | tail !560 # re-executes 560th command (yours will have different numbers; choose the one that recreates your really important result!)","title":"History Repeats Itself"},{"location":"lab_session/basic_unix/#editing-yourself","text":"Here are some more ways to make editing previous commands, or novel commands that you\u2019re building up, easier: <up><up> # go to some previous command, just to have something to work on <ctrl-a> # go to the beginning of the line <ctrl-e> # go to the end of the line #now use left and right to move to a single word (surrounded by whitespace: spaces or tabs) <ctrl-k> # delete from here to end of line <ctrl-w> # delete from here to beginning of preceeding word blah blah blah<ctrl-w><ctrl-w> # leaves you with only one 'blah' You can also search your history from the command line: <ctrl-r>fir # should find most recent command containing 'fir' string: echo 'first' > test.txt <enter> # to run command <ctrl-c> # get out of recursive search <ctr-r> # repeat <ctrl-r> to find successively older string matches","title":"Editing Yourself"},{"location":"lab_session/basic_unix/#create-and-destroy","text":"We already learned one command that will create a file, touch. Now let\u2019s look at create and removing files and directories. cd # home again mkdir ~/tmp2 cd ~/tmp2 echo 'Hello, world!' > first.txt echo text then redirect ( \u2018>\u2019 ) to a file. cat first.txt # 'cat' means 'concatenate', or just spit the contents of the file to the screen","title":"Create and Destroy"},{"location":"lab_session/basic_unix/#why-concatenate-try-this","text":"cat first.txt first.txt first.txt > second.txt cat second.txt OK, let\u2019s destroy what we just created: cd ../ rmdir tmp2 # 'rmdir' meands 'remove directory', but this shouldn't work! rm tmp2/first.txt rm tmp2/second.txt # clear directory first rmdir tmp2 # should succeed now So, \u2018mkdir\u2019 and \u2018rmdir\u2019 are used to create and destroy (empty) directories. \u2018rm\u2019 to remove files. To create a file can be as simple as using \u2018echo\u2019 and the \u2018>\u2019 (redirection) character to put text into a file. Even simpler is the \u2018touch\u2019 command. mkdir ~/cli cd ~/cli touch newFile ls -ltra # look at the time listed for the file you just created cat newFile # it's empty! sleep 60 # go grab some coffee touch newFile ls -ltra # same time? So \u2018touch\u2019 creates empty files, or updates the \u2018last modified\u2019 time. Note that the options on the \u2018ls\u2019 command you used here give you a Long listing, of All files, in Reverse Time order (l, a, r, t).","title":"why \u2018concatenate\u2019? try this:"},{"location":"lab_session/basic_unix/#forced-removal-caution","text":"When you\u2019re on the command line, there\u2019s no \u2018Recycle Bin\u2019. Since we\u2019ve expanded a whole directory tree, we need to be able to quickly remove a directory without clearing each subdirectory and using \u2018rmdir\u2019. cd mkdir -p rmtest/dir1/dir2 # the -p option creates all the directories at once rmdir rmtest # gives an error since rmdir can only remove directories that are empty rm -rf rmtest # will remove the directory and EVERYTHING in it Here -r = recursively remove sub-directories, -f means force. Obviously, be careful with \u2018rm -rf\u2019 , there is no going back , if you delete something with rm, rmdir its gone! There is no Recycle Bin on the Command-Line!","title":"Forced Removal (CAUTION!!!)"},{"location":"lab_session/basic_unix/#piping-and-redirection","text":"Pipes (\u2018|\u2019) allow commands to hand output to other commands, and redirection characters (\u2018>\u2019 and \u2018\u00bb\u2019) allow you to put output into files. echo 'first' > test.txt cat test.txt # outputs the contents of the file to the terminal echo 'second' > test.txt cat test.txt echo 'third' >> test.txt cat test.txt The \u2018>\u2019 character redirects output of a command that would normally go to the screen instead into a specified file. \u2018>\u2019 overwrites the file, \u2018\u00bb\u2019 appends to the file. The \u2018cut\u2019 command pieces of lines from a file line by line. This command cuts characters 1 to 3, from every line, from file \u2018test.txt\u2019 cut -c 1 -3 test.txt same thing, piping output of one command into input of another cat test.txt | cut -c 1 -3 This pipes (i.e., sends the output of) cat to cut to sort (-r means reverse order sort), and then grep searches for pattern (\u2018s\u2019) matches (i.e. for any line where an \u2018s\u2019 appears anywhere on the line.) cat test.txt | cut -c 1 -3 | sort -r cat test.txt | cut -c 1 -3 | sort -r | grep s This is a great way to build up a set of operations while inspecting the output of each step in turn. We\u2019ll do more of this in a bit.","title":"Piping and Redirection"},{"location":"lab_session/basic_unix/#compression-and-archives","text":"As file sizes get large, you\u2019ll often see compressed files, or whole compressed folders. Note that any good bioinformatics software should be able to work with compressed file formats. gzip test.txt cat test.txt.gz To uncompress a file gunzip -c test.txt.gz The \u2018-c\u2019 leaves the original file alone, but dumps expanded output to screen gunzip test.txt.gz # now the file should change back to uncompressed test.txt Tape archives, or .tar files, are one way to compress entire folders and all contained folders into one file. When they\u2019re further compressed they\u2019re called \u2018tarballs\u2019. We can use wget (web get). wget -L -O PhiX_Illumina_RTA.tar.gz http://igenomes.illumina.com.s3-website-us-east-1.amazonaws.com/PhiX/Illumina/RTA/PhiX_Illumina_RTA.tar.gz The .tar.gz and .tgz are commonly used extensions for compressed tar files, when gzip compression is used. The application tar is used to uncompress .tar files tar -xzvf PhiX_Illumina_RTA.tar.gz Here -x = extract, -z = use gzip/gunzip, -v = verbose (show each file in archive), -f filename Note that, unlike Windows, linux does not depend on file extensions to determine file behavior. So you could name a tarball \u2018fish.puppy\u2019 and the extract command above should work just fine. The only thing that should be different is that tab-completion doesn\u2019t work within the \u2018tar\u2019 command if it doesn\u2019t see the \u2018correct\u2019 file extension.","title":"Compression and Archives"},{"location":"lab_session/basic_unix/#bash-wildcard-characters","text":"We can use \u2018wildcard characters\u2019 when we want to specify or operate on sets of files all at once. ls ?hiX/Illumina list files in Illumina sub-directory of any directory ending in \u2018hiX\u2019 ls PhiX/Illumina/RTA/Sequence/*/*.fa list all files ending in \u2018.fa\u2019 a few directories down. So, \u2018?\u2019 fills in for zero or one character, \u2018*\u2019 fills in for zero or more characters. The \u2018find\u2019 command can be used to locate files using a similar form. find . -name \"*.f*\" find . -name \"*.f?\" how is this different from the previous ls commands?","title":"BASH Wildcard Characters"},{"location":"lab_session/basic_unix/#quick-note-about-the-quotes","text":"The quote characters \u201c and \u2018 are different. In general, single quotes preserve the literal meaning of all characters between them. On the other hand, double quotes allow the shell to see what\u2019s between them and make substitutions when appropriate. For example: VRBL = someText echo '$VRBL' echo \" $VRBL \" However, some commands try to be \u2018smarter\u2019 about this behavior, so it\u2019s a little hard to predict what will happen in all cases. It\u2019s safest to experiment first when planning a command that depends on quoting \u2026 list filenames first, instead of changing them, etc. Finally, the \u2018backtick\u2019 characters ` (same key - unSHIFTED - as the tilde ~) causes the shell to interpret what\u2019s between them as a command, and return the result.","title":"Quick Note About the Quote(s)"},{"location":"lab_session/basic_unix/#counts-the-number-of-lines-in-file-and-stores-result-in-the-lines-variable","text":"LINES = ` cat PhiX/Illumina/RTA/Sequence/Bowtie2Index/genome.1.bt2 | wc -l ` echo $LINES","title":"counts the number of lines in file and stores result in the LINES variable"},{"location":"lab_session/basic_unix/#symbolic-links","text":"Since copying or even moving large files (like sequence data) around your filesystem may be impractical, we can use links to reference \u2018distant\u2019 files without duplicating the data in the files. Symbolic links are disposable pointers that refer to other files, but behave like the referenced files in commands. I.e., they are essentially \u2018Shortcuts\u2019 (to use a Windows term) to a file or directory. The \u2018ln\u2019 command creates a link. You should, by default, always create a symbolic link using the -s option. ln -s PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa . ls -ltrhaF # notice the symbolic link pointing at its target grep -c \">\" genome.fa","title":"Symbolic Links"},{"location":"lab_session/basic_unix/#stdout-stderr","text":"Programs can write to two separate output streams, \u2018standard out\u2019 (STDOUT), and \u2018standard error\u2019 (STDERR). The former is generally for direct output of a program, while the latter is supposed to be used for reporting problems. I\u2019ve seen some bioinformatics tools use STDERR to report summary statistics about the output, but this is probably bad practice. Default behavior in a lot of cases is to dump both STDOUT and STDERR to the screen, unless you specify otherwise. In order to nail down what goes where, and record it for posterity: wc -c genome.fa 1 > chars.txt 2 > any.err the 1 st output, STDOUT, goes to \u2018chars.txt\u2019 the 2 nd output, STDERR, goes to \u2018any.err\u2019 cat chars.txt Contains the character count of the file genome.fa cat any.err Empty since no errors occured. Saving STDOUT is pretty routine (you want your results, yes?), but remember that explicitly saving STDERR is important on a remote server, since you may not directly see the \u2018screen\u2019 when you\u2019re running jobs.","title":"STDOUT &amp; STDERR"},{"location":"lab_session/basic_unix/#the-sed-command","text":"Let\u2019s take a look at the \u2018sed\u2019 command. NOTE: On Macs use \u2018gsed\u2019. sed (short for stream editor) is a command that allows you to manipulate character data in various ways. One useful thing it can do is substitution. Let\u2019s download a simple file to work on: wget region.bed -O region.bed Take a look at the file: cat region.bed Now, let\u2019s make all the uppercase \u201cCHR\u201ds into lowercase: cat region.bed | sed 's/CHR/chr/' What happened? Only the first CHR changed. That is because we need to add the \u201cg\u201d option: cat region.bed | sed 's/CHR/chr/g' We can also do the the substitution without regards to case: cat region.bed | sed 's/chr/chr/gi' Let\u2019s break down the argument to sed (within the single quotes)\u2026 The \u201cs\u201d means \u201csubstitute\u201d, the word between the 1 st and 2 nd forward slashes (i.e. /) is the word the substitute for, the word between the 2 nd and 3 rd slashes is the word to substitute with, and finally the \u201cgi\u201d at the end are flags for global substitution (i.e. substituting along an entire line instead of just the first occurence on a line), and for case insenstivity (i.e. it will ignore the case of the letters when doing the substitution). Note that this doesn\u2019t change the file itself, it is simply piping the output of the cat command to sed and outputting to the screen. If you wanted to change the file itself, you could use the \u201c-i\u201d option to sed: cat region.bed sed -i 's/chr/chr/gi' region.bed Now if you look at the file, the lines have changed. cat region.bed Another useful use of sed is for capturing certain lines from a file. You can select certain lines from a file: sed '4q;d' region.bed This will just select the 4 th line from the file. You can also extract a range of lines from a file: sed -n '10,20p' region.bed This gets the 10 th through 20 th lines from the file. CHALLENGE: See if you can find a way to use sed to remove all the \u201cCHR\u201ds from the file.","title":"The sed command"},{"location":"lab_session/basic_unix/#more-pipes","text":"Now, let\u2019s delve into pipes a little more. Pipes are a very powerful way to look at and manipulate complex data using a series of simple programs. Let\u2019s look at some fastq files. Get a few small fastq files: wget C61.subset.fq.gz -O C61.subset.fq.gz wget I561.subset.fq.gz -O I561.subset.fq.gz wget I894.subset.fq.gz -O I894.subset.fq.gz Since the files are gzipped files we need to use \u201czcat\u201d to look at them. zcat is just like cat except for gzipped files: zcat C61.subset.fq.gz | head Fastq records are 4 lines per sequence, a header line, the sequence, a plus sign (which is historical), and then the quality encoding for the sequence. Notice that each header line has the barcode for that read at the end of the line. Let\u2019s count the number of each barcode. In order to do that we need to just capture the header lines from this file. We can use \u201csed\u201d to do that: zcat C61.subset.fq.gz | sed -n '1~4p' | head By default sed prints every line. In this case we are giving the \u201c-n\u201d option to sed which will not print every line. Instead, we are giving it the argument \u201c1~4p\u201d, which means to print the first line, then skip 4 lines and print again, and then continue to do that. Now that we have a way to get just the headers, we need to isolate the part of the header that is the barcode. There are multiple ways to do this\u2026 we will use the cut command: zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | head So we are using the \u201c-d\u201d option to cut with \u201c:\u201d as the argument to that option, meaning that we will be using the delimiter \u201c:\u201d to split the input. Then we use the \u201c-f\u201d option with argument \u201c10\u201d, meaning that we want the 10 th field after the split. In this case, that is the barcode. Finally, as before, we need to sort the data and then use \u201cuniq -c\u201d to count. Then put it all together and run it on the entire dataset (This will take about a minute to run): zcat C61.subset.fq.gz | sed -n '1~4p' | cut -d: -f10 | sort | uniq -c Now you have a list of how many reads were categorized into each barcode. Here is a sed tutorial for more exercises. CHALLENGE: Find the distribution of the first 5 bases of all the reads in C61_S67_L006_R1_001.fq.gz. I.e., count the number of times the first 5 bases of every read occurs across all reads.","title":"More pipes"},{"location":"lab_session/basic_unix/#loops","text":"Loops are useful for quickly telling the shell to perform one operation after another, in series. For example: for i in { 1 ..21 } ; do echo $i >> a ; done # put multiple lines of code on one line, each line terminated by ';' cat a","title":"Loops"},{"location":"lab_session/basic_unix/#1-through-21-on-separate-lines","text":"The general form is: for name in { list } ; do commands done The list can be a sequence of numbers or letters, or a group of files specified with wildcard characters: for i in { 3 ,2,1,liftoff } ; do echo $i ; done # needs more excitement! for i in { 3 ,2,1, \"liftoff!\" } ; do echo $i ; done # exclamation point will confuse the shell unless quoted A \u201cwhile\u201d loop is more convenient than a \u201cfor\u201d loop \u2026 if you don\u2019t readily know how many iterations of the loop you want: while { condition } ; do commands done Now, let\u2019s do some bioinformatics-y things with loops and pipes. First, let\u2019s write a command to get the nucleotide count of the first 10,000 reads in a file. Use zcat and sed to get only the read lines of a file, and then only take the first 10,000: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | less Use grep\u2019s \u201c-o\u201d option to get each nucleotide on a separate line (take a look at the man page for grep to understand how this works): zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | less Finally, use sort and uniq to get the counts: zcat C61.subset.fq.gz | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c 264012 A 243434 C 215045 G 278 N 277231 T And, voila, we have the per nucleotide count for these reads! We just did this for one file, but what if we wanted to do it for all of our files? We certainly don\u2019t want to type the command by hand dozens of times. So we\u2019ll use a while loop. You can pipe a command into a while loop and it will iterate through each line of the input. First, get a listing of all your files: ls -1 *.fq.gz Pipe that into a while loop and read in the lines into a variable called \u201cx\u201d. We use \u201c$x\u201d to get the value of the variable in that iteration of the loop: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; done Add the command we created above into the loop, placing $x where the filename would be and semi-colons inbetween commands: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c ; done When this runs it will print the name of every single file being processed and the nucleotide count for the reads from those files. Now, let\u2019s say you wanted to write the output of each command to a separate file. We would redirect the output to a filename, but we need to create a different file name for each command and we want the file name to reflect its contents, i.e. the output file name should be based on the input file name. So we use \u201cparameter expansion\u201d, which is fancy way of saying substitution: ls -1 *.fq.gz | while read x ; do echo $x is being processed... ; zcat $x | sed -n '2~4p' | head -10000 | grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; done This will put the output of the counting command into a file whose name is the prefix of the input file plus \u201c.nucl_count.txt\u201d. It will do this for every input file.","title":"&lt;1 through 21 on separate lines&gt;"},{"location":"lab_session/basic_unix/#manipulation-of-a-fasta-file","text":"Let\u2019s copy the phiX-174 genome (using the \u2018cp\u2019 command) to our current directory so we can play with it: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa phix.fa Similarly we can also use the move command here, but then ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa will no longer be there: cp ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome.fa ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ mv ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/genome2.fa phix.fa ls ./PhiX/Illumina/RTA/Sequence/WholeGenomeFasta/ This functionality of mv is why it is used to rename files. Note how we copied the \u2018genome.fa\u2019 file to a different name: \u2018phix.fa\u2019 wc -l phix.fa count the number of lines in the file using \u2018wc\u2019 (word count) and parameter \u2018-l\u2019 (lines). We can use the \u2018grep\u2019 command to search for matches to patterns. \u2018grep\u2019 comes from \u2018globally search for a regular expression and print\u2019. grep -c '>' phix.fa #Only one FASTA sequence entry, since only one header line (\u2018>gi|somethingsomething\u2026\u2019) cat phix.fa This may not be useful for anything larger than a virus! Let\u2019s look at the start codon and the two following codons: grep --color \"ATG......\" phix.fa #\u2019.\u2019 characters are the single-character wildcards for grep. So \u201cATG\u2026\u2026\u201d matches any set of 9 characters that starts with ATG. #Use the \u2013color \u2018-o\u2019 option to only print the pattern matches, one per line grep -o \"ATG......\" phix.fa #Use the \u2018cut\u2019 command with \u2018-c\u2019 to select characters 4-6, the second codon grep --color -o \"ATG......\" phix.fa | cut -c4-6 \u2018sort\u2019 the second codon sequences ( default order is same as ASCII table ; see \u2018man ascii\u2019 ) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort #Combine successive identical sequences, but count them using the \u2018uniq\u2019 command with the \u2018-c\u2019 option grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c #Finally sort using reverse numeric order (\u2018-rn\u2019) grep --color -o \"ATG......\" phix.fa | cut -c4-6 | sort | uniq -c | sort -rn \u2026 which gives us the most common codons first This may not be a particularly useful thing to do with a genomic FASTA file, but it illustrates the process by which one can build up a string of operations, using pipes, in order to ask quantitative questions about sequence content. More generally than that, this process allows one to ask questions about files and file contents and the operating system, and verify at each step that the process so far is working as expected. The command line is, in this sense, really a modular workflow management system.","title":"Manipulation of a FASTA File"},{"location":"lab_session/basic_unix/#shell-scripts-file-permissions","text":"Often it\u2019s useful to define a whole string of commands to run on some input, so that (1) you can be sure you\u2019re running the same commands on all data, and (2) so you don\u2019t have to type the same commands in over and over! Let\u2019s use the \u2018nano\u2019 text editor program that\u2019s pretty reliably installed on most linux systems. nano test.sh","title":"Shell Scripts, File Permissions"},{"location":"lab_session/basic_unix/#insert-cli_figure7","text":"nano now occupies the whole screen; see commands at the bottom. Let\u2019s type in a few commands. First we need to put the following line at the top of the file: #!/bin/bash The \u201c#!\u201d at the beginning of a script tells the shell what language to use to interpret the rest of the script. In our case, we will be writing \u201cbash\u201d commands, so we specify the full path of the bash executable after the \u201c#!\u201d. Then, add some commands: #!/bin/bash echo \"Start script...\" pwd ls -l sleep 10 echo \"End script.\" Hit Cntl-O and then enter to save the file, and then Cntl-X to exit nano. Though there are ways to run the commands in test.sh right now, it\u2019s generally useful to give yourself (and others) \u2018execute\u2019 permissions for test.sh, really making it a shell script. Note the characters in the first (left-most) field of the file listing: ls -lh test.sh -rw-rw-r-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first \u2018-\u2018 becomes a \u2018d\u2019 if the \u2018file\u2019 is actually a directory. The next three characters represent read, write, and execute permissions for the file owner (you), followed by three characters for users in the owner\u2019s group, followed by three characters for all other users. Run the \u2018chmod\u2019 command to change permissions for the \u2018test.sh\u2019 file, adding execute permissions (\u2018+x\u2019) for the user (you) and your group (\u2018ug\u2019): chmod ug+x test.sh ls -lh test.sh -rwxr-xr-- 1 ubuntu workspace 79 Dec 19 15 :05 test.sh The first 10 characters of the output represent the file and permissions. The first character is the file type, the next three sets of three represent the file permissions for the user, group, and everyone respectively. r = read w = write x = execute So let\u2019s run this script. We have to provide a relative reference to the script \u2018./\u2019 because its not our our \u201cPATH\u201d.: #you can do either ./test.sh #or you can run it like this bash test.sh And you should see all the commands in the file run in sequential order in the terminal.","title":"insert cli_figure7"},{"location":"lab_session/basic_unix/#command-line-arguments-for-shell-scripts","text":"Now let\u2019s modify our script to use command line arguments, which are arguments that can come after the script name (when executing) to be part of the input inside the script. This allows us to use the same script with different inputs. In order to do so, we add variables $1, $2, $3, etc\u2026. in the script where we want our input to be. So, for example, use nano to modify your test.sh script to look like this: #!/bin/bash echo \"Start script...\" PWD = ` pwd ` echo \"The present working directory is $PWD \" ls -l $1 sleep $2 wc -l $3 echo \"End script.\" Now, rerun the script using command line arguments like this: ./test.sh genome.fa 15 PhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt Note that each argument is separated by a space, so $1 becomes \u201cgenome.fa\u201d, $2 becomes \u201c15\u201d, and $3 becomes \u201cPhiX/Illumina/RTA/Annotation/Archives/archive-2013-03-06-19-09-31/Genes/ChromInfo.txt\u201d. Then the commands are run using those values. Now rerun the script with some other values: ./test.sh .. 5 genome.fa Now, $1 becomes \u201c..\u201d, $2 is \u201c5\u201d, and $3 is \u201cgenome.fa\u201d.","title":"Command Line Arguments for Shell Scripts"},{"location":"lab_session/basic_unix/#pipes-and-loops-inside-scripts","text":"Open a new text file using the text editor \u201cnano\u201d: nano get_nucl_counts.sh Copy and Paste the following into the file: #!/bin/bash zcat $1 | sed -n '2~4p' | head - $2 | grep -o . | sort | uniq -c Save the file and exit. Change the permissions on the file to make it executable: chmod a+x get_nucl_counts.sh Now, we can run this script giving it different arguments every time. The first argument (i.e. the first text after the script name when it is run) will get put into the variable \u201c \\(1\u201d. The second argument (delimited by spaces) will get put into \u201c\\) 2\u201d. In this case, \u201c \\(1\u201d is the file name, and \u201c\\) 2\u201d is the number of reads we want to count. So, then we can run the script over and over again using different values and the command will run based on those values: ./get_nucl_counts.sh I561.subset.fq.gz 1000 ./get_nucl_counts.sh I561.subset.fq.gz 10000 ./get_nucl_counts.sh C61.subset.fq.gz 555 We can also put loops into a script. We\u2019ll take the loop we created earlier and put it into a file, breaking it up for readability and using backslashes for line continuation: nano get_nucl_counts_loop.sh Put this in the file and save it: #!/bin/bash ls -1 *.fq.gz | \\ while read x ; do \\ echo $x is being processed... ; \\ zcat $x | sed -n '2~4p' | head - $1 | \\ grep -o . | sort | uniq -c > ${ x %.fq.gz } .nucl_count.txt ; \\ done Make it executable: chmod a+x get_nucl_counts_loop.sh And now we can execute the entire loop using the script. Note that there is only one argument now, the number of reads to use: ./get_nucl_counts_loop.sh 100","title":"Pipes and Loops inside scripts"},{"location":"lab_session/basic_unix/#finally-a-good-summary-for-rounding-up-the-session","text":"TEN SIMPLE RULES FOR GETTING STARTED WITH COMMAND-LINE BIOINFORMATICS","title":"FINALLY - a good summary for rounding up the session"},{"location":"lab_session/basics_awk/","text":"AWK crash course for Bioinformatics \u00b6 An introduction on how to analyze table data without MS Excel \u00b6 What is AWK? \u00b6 AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs. It is a standard feature of most Unix-like operating systems. source: Wikipedia Why awk? \u00b6 You can replace a pipeline of 'stuff | grep | sed | cut...' with a single call to awk. For a simple script, most of the timelag is in loading these apps into memory, and it's much faster to do it all with one. This is ideal for something like an openbox pipe menu where you want to generate something on the fly. You can use awk to make a neat one-liner for some quick job in the terminal, or build an awk section into a shell script. Simple AWK commands \u00b6 An AWK program consists of a sequence of pattern-action statements and optional function definitions. It processes text files. AWK is a line oriented language. It divides a file into lines called records. Each line is broken up into a sequence of fields. The fields are accessed by special variables: $1 reads the first field, $2 the second and so on. The $0 variable refers to the whole record. The structure of an AWK program has the following form: pattern { action } The pattern is a test that is performed on each of the records. If the condition is met then the action is performed. Either pattern or action can be omitted, but not both. The default pattern matches each line and the default action is to print the record. awk -f program-file [ file-list ] awk program [ file-list ] An AWK program can be run in two basic ways: a) the program is read from a separate file; the name of the program follows the -f option, b) the program is specified on the command line enclosed by quote characters. Print all the lines from a file \u00b6 By default, awk prints all lines of a file , so to print every line of above created file use below command awk '{print}' file Note: In awk command \u2018{print}\u2019 is used print all fields along with their values. Print only specific field like 2 nd & 3 rd \u00b6 In awk command, we use $ (dollar) symbol followed by field number to prints field values. awk -F \",\" '{print $2, $3}' file In the above command we have used the option -F \u201c,\u201d which specifies that comma (,) is the field separator in the file. This is usually good practice when dealing with tables where the separators between each column could be single or multiple white-space (\" \") or tab (\"\\t\") or a colon (\":\") or a semicolon (\";\"). AWK one-liners \u00b6 AWK one-liners are simple one-shot programs run from the command line. Let us have the following text file: words.txt We want to print all words included in the words.txt file that are longer than five characters. awk 'length($1) > 5 {print $0}' words.txt storeroom existence ministerial falcon bookworm bookcase The AWK program is placed between two single quote characters. The first is the pattern; we specify that the length of the record is greater that five. The length function returns the length of the string. The $1 variable refers to the first field of the record; in our case there is only one field per record. The action is placed between curly brackets. awk 'length($1) > 5' words.txt storeroom existence ministerial falcon bookworm bookcase As we have specified earlier, the action can be omitted. In such a case a default action is performed \u2014 printing of the whole record. awk 'length($1) == 3' words.txt cup sky top war We print all words that have three characters. awk '!(length($1) == 3)' words.txt storeroom tree store book cloud existence ministerial falcon town bookworm bookcase With the ! operator, we can negate the condition; we print all lines that do not have three characters. awk '(length($1) == 3) || (length($1) == 4)' words.txt tree cup book town sky top war Next we apply conditions on numbers. We have a file with scores of students - scores.txt. awk '$2 >= 90 { print $0 }' scores.txt Lucia 95 Joe 92 Sophia 90 We print all students with scores 90+. awk '$2 >= 90 { print }' scores.txt Lucia 95 Joe 92 Sophia 90 If we omit an argument for the print function, the $0 is assumed. awk '$2 >= 90' scores.txt Lucia 95 Joe 92 Sophia 90 A missing { action } means print the matching line. awk '{ if ($2 >= 90) print }' scores.txt Lucia 95 Joe 92 Sophia 90 Instead of a pattern, we can also use an if condition in the action. awk '{sum += $2} END { printf(\"The average score is %.2f\\n\", sum/NR) }' scores.txt The average score is 77.56 This command calculates the average score. In the action block, we calculate the sum of scores. In the END block, we print the average score. We format the output with the built-in printf function. The %.2f is a format specifier; each specifier begins with the % character. The .2 is the precision -- the number of digits after the decimal point. The f expects a floating point value. The \\n is not a part of the specifier; it is a newline character. It prints a newline after the string is shown on the terminal. AWK working with pipes \u00b6 AWK can receive input and send output to other commands via the pipe. echo -e \"1 2 3 5\\n2 2 3 8\" | awk '{print $(NF)}' 5 8 In this case, AWK receives output from the echo command. It prints the values of last column. awk -F: '$7 ~ /bash/ {print $1}' /etc/passwd | wc -l 3 Here, the AWK program sends data to the wc command via the pipe. In the AWK program, we find out those users who use bash. Their names are passed to the wc command which counts them. In our case, there are three users using bash. AWK for Bioinformatics - looking at transcriptome data \u00b6 You can find a lot of online tutorials, but here we will try out a few steps which show how a bioinformatician analyses a GTF file using awk. GTF is a special file format that contains information about different regions of the genome and their associated annotations. More on that here - Ensembl File Formats-GTF . wget transcriptome.gtf head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Chaining AWK calls \u00b6 We will start with the AWK call that we were using before, and we will append a pipe | so it can be used as input for the next AWK call, this time using a space and a semicolon as the delimiter to define what a column is: awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head | less -S gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Now that we see what the third column looks like, we can filter for protein-coding genes awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 == \"gene_type \\\"protein_coding\\\"\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENSG00000237683.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1\" ; level 3 ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENSG00000235249.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29\" ; level 2 ; havana_gene \"OTTHUMG00000002860.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENSG00000185097.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16\" ; level 2 ; havana_gene \"OTTHUMG00000002581.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENSG00000269831.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1\" ; level 3 ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENSG00000269308.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2\" ; level 3 ; gene_id \"ENSG00000187634.6\" ; transcript_id \"ENSG00000187634.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"SAMD11\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"SAMD11\" ; level 2 ; havana_gene \"OTTHUMG00000040719.8\" ; gene_id \"ENSG00000268179.1\" ; transcript_id \"ENSG00000268179.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.1\" ; level 3 ; gene_id \"ENSG00000188976.6\" ; transcript_id \"ENSG00000188976.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NOC2L\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NOC2L\" ; level 2 ; havana_gene \"OTTHUMG00000040720.1\" ; gene_id \"ENSG00000187961.9\" ; transcript_id \"ENSG00000187961.9\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"KLHL17\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"KLHL17\" ; level 2 ; havana_gene \"OTTHUMG00000040721.6\" ; I added a space and a backslash (not to be confused with the regular slash /) after the first and second pipes to split the code into two lines; this makes it easier to read and it highlights that we are taking two separate steps. The double quotes around protein_coding are escaped (also with a backslash \\\") because they are already contained inside double quotes. To avoid the backslashing drama we can use the partial matching operator ~ instead of the total equality operator ==. awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S The output is the same as before: those lines that contain a protein_coding somewhere in their third column make the partial matching rule true, and they get printed (which is the default behavior when there are no curly braces). Now we have all the protein-coding genes, but how do we get to the genes that only have one exon? Well, we have to revisit our initial AWK call: we selected lines that corresponded to genes, but we actually want lines that correspond to exons. That\u2019s an easy fix, we just change the word \u201cgene\u201d for the word \u201cexon\u201d. Everything else stays the same. awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENST00000335137.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5-001\" ; exon_number 1 ; exon_id \"ENSE00002319515.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30547.1\" ; havana_gene \"OTTHUMG00000001094.1\" ; havana_transcript \"OTTHUMT00000003223.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 1 ; exon_id \"ENSE00002221580.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 2 ; exon_id \"ENSE00002314092.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENST00000426406.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29-001\" ; exon_number 1 ; exon_id \"ENSE00002316283.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41220.1\" ; havana_gene \"OTTHUMG00000002860.1\" ; havana_transcript \"OTTHUMT00000007999.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENST00000332831.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16-001\" ; exon_number 1 ; exon_id \"ENSE00002324228.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41221.1\" ; havana_gene \"OTTHUMG00000002581.1\" ; havana_transcript \"OTTHUMT00000007334.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 1 ; exon_id \"ENSE00003063549.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 2 ; exon_id \"ENSE00003084653.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 3 ; exon_id \"ENSE00003138540.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 1 ; exon_id \"ENSE00003079649.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 2 ; exon_id \"ENSE00003048391.1\" ; level 3 ; tag \"basic\" ; Cleaning up the output \u00b6 Before we try to count how many exons belong to the same protein-coding gene, let\u2019s simplify the output so we only get the gene names (which are in column 5). awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\" {print $5}' | \\ head gene_name \"OR4F5\" gene_name \"AL627309.1\" gene_name \"AL627309.1\" gene_name \"OR4F29\" gene_name \"OR4F16\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL645608.2\" gene_name \"AL645608.2\" This is sort of what we want. We could chain another AWK call using -F \" \", and pick the second column (which would get rid of the gene_name). Feel free to try that approach if you are curious. We can also take a shortcut by using the tr -d command, which deletes whatever characters appear in double quotes. For example, to remove every vowel from a sentence: echo \"This unix thing is cool\" | tr -d \"aeiou\" # Ths nx thng s cl Let\u2019s try deleting all the semicolons and quotes before the second AWK call: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" '$6 == \"protein_coding\" {print $10}' | \\ head OR4F5 AL627309.1 AL627309.1 OR4F29 OR4F16 AL669831.1 AL669831.1 AL669831.1 AL645608.2 AL645608.2 Run bash awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | tr -d \";\\\"\" | head to understand what the input to the second AWK call looks like. It\u2019s just words separated by spaces; the sixth word corresponds to the gene type, and the tenth word to the gene name. Counting genes \u00b6 There is one more concept we need to introduce before we start counting. AWK uses a special rule called END, which is only true once the input is over. See an example: echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { print $1 } END { print \"Done with letters!\" } ' a a b b b c Done with letters! The -e option tells echo to convert each \\n into a new line, which is a convenient way of printing multiple lines from a single character string. In AWK, any amount of whitespace is allowed between the initial and the final quote '. I separated the first rule from the END rule to make them easier to read. Now we are ready for counting. echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { counter[$1] += 1 } END { for (letter in counter){ print letter, counter[letter] } } ' a 2 b 3 c 1 Wow, what is all that madness? Instead of printing each letter, we manipulate a variable that we called counter. This variable is special because it is followed by brackets [ ], which makes it an associative array, a fancy way of calling a variable that stores key-value pairs. In this case we chose the values of the first column \\(1 to be the keys of the counter variable, which means there are 3 keys (\u201ca\u201d, \u201cb\u201d and \u201cc\u201d). The values are initialized to 0. For every line in the input, we add a 1 to the value in the array whose key is equal to \\(1. We use the addition operator +=, a shortcut for counter[\\) 1] = counter[\\) 1] + 1. When all the lines are read, the END rule becomes true, and the code between the curly braces { } is executed. The structure for (key in associate_array) { some_code } is called a for loop, and it executes some_code as many times as there are keys in the array. letter is the name that we chose for the variable that cycles through all the keys in counter, and counter[letter] gives the value stored in counter for each letter (which we we calculated in the previous curly brace chunk). Now we can apply this to the real example: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" ' $6 == \"protein_coding\" { gene_counter[$10] += 1 } END { for (gene_name in gene_counter){ print gene_name, gene_counter[gene_name] } }' > number_of_exons_by_gene.txt head number_of_exons_by_gene.txt CAPN6 24 ARL14EPL 3 DACH1 38 IFNA13 1 HSP90AB1 36 CAPN7 52 DACH2 84 IFNA14 1 LARS 188 CAPN8 78 If you are using the real transcriptome, it takes less than a minute to count up one million exons. Pretty impressive. We saved the output to a file, so now we can use AWK to see how many genes are made up of a single exon. awk '$2 == 1' number_of_exons_by_gene.txt | wc -l # 1362 I will suggest to follow the original tutorial if you need to refer to these steps later on for your own data: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"AWK"},{"location":"lab_session/basics_awk/#awk-crash-course-for-bioinformatics","text":"","title":"AWK crash course for Bioinformatics"},{"location":"lab_session/basics_awk/#an-introduction-on-how-to-analyze-table-data-without-ms-excel","text":"","title":"An introduction on how to analyze table data without MS Excel"},{"location":"lab_session/basics_awk/#what-is-awk","text":"AWK is an interpreted programming language designed for text processing and typically used as a data extraction and reporting tool. The AWK language is a data-driven scripting language consisting of a set of actions to be taken against streams of textual data - either run directly on files or used as part of a pipeline - for purposes of extracting or transforming text, such as producing formatted reports. The language extensively uses the string datatype, associative arrays (that is, arrays indexed by key strings), and regular expressions. AWK has a limited intended application domain, and was especially designed to support one-liner programs. It is a standard feature of most Unix-like operating systems. source: Wikipedia","title":"What is AWK?"},{"location":"lab_session/basics_awk/#why-awk","text":"You can replace a pipeline of 'stuff | grep | sed | cut...' with a single call to awk. For a simple script, most of the timelag is in loading these apps into memory, and it's much faster to do it all with one. This is ideal for something like an openbox pipe menu where you want to generate something on the fly. You can use awk to make a neat one-liner for some quick job in the terminal, or build an awk section into a shell script.","title":"Why awk?"},{"location":"lab_session/basics_awk/#simple-awk-commands","text":"An AWK program consists of a sequence of pattern-action statements and optional function definitions. It processes text files. AWK is a line oriented language. It divides a file into lines called records. Each line is broken up into a sequence of fields. The fields are accessed by special variables: $1 reads the first field, $2 the second and so on. The $0 variable refers to the whole record. The structure of an AWK program has the following form: pattern { action } The pattern is a test that is performed on each of the records. If the condition is met then the action is performed. Either pattern or action can be omitted, but not both. The default pattern matches each line and the default action is to print the record. awk -f program-file [ file-list ] awk program [ file-list ] An AWK program can be run in two basic ways: a) the program is read from a separate file; the name of the program follows the -f option, b) the program is specified on the command line enclosed by quote characters.","title":"Simple AWK commands"},{"location":"lab_session/basics_awk/#print-all-the-lines-from-a-file","text":"By default, awk prints all lines of a file , so to print every line of above created file use below command awk '{print}' file Note: In awk command \u2018{print}\u2019 is used print all fields along with their values.","title":"Print all the lines from a file"},{"location":"lab_session/basics_awk/#print-only-specific-field-like-2nd-3rd","text":"In awk command, we use $ (dollar) symbol followed by field number to prints field values. awk -F \",\" '{print $2, $3}' file In the above command we have used the option -F \u201c,\u201d which specifies that comma (,) is the field separator in the file. This is usually good practice when dealing with tables where the separators between each column could be single or multiple white-space (\" \") or tab (\"\\t\") or a colon (\":\") or a semicolon (\";\").","title":"Print only specific field like 2nd &amp; 3rd"},{"location":"lab_session/basics_awk/#awk-one-liners","text":"AWK one-liners are simple one-shot programs run from the command line. Let us have the following text file: words.txt We want to print all words included in the words.txt file that are longer than five characters. awk 'length($1) > 5 {print $0}' words.txt storeroom existence ministerial falcon bookworm bookcase The AWK program is placed between two single quote characters. The first is the pattern; we specify that the length of the record is greater that five. The length function returns the length of the string. The $1 variable refers to the first field of the record; in our case there is only one field per record. The action is placed between curly brackets. awk 'length($1) > 5' words.txt storeroom existence ministerial falcon bookworm bookcase As we have specified earlier, the action can be omitted. In such a case a default action is performed \u2014 printing of the whole record. awk 'length($1) == 3' words.txt cup sky top war We print all words that have three characters. awk '!(length($1) == 3)' words.txt storeroom tree store book cloud existence ministerial falcon town bookworm bookcase With the ! operator, we can negate the condition; we print all lines that do not have three characters. awk '(length($1) == 3) || (length($1) == 4)' words.txt tree cup book town sky top war Next we apply conditions on numbers. We have a file with scores of students - scores.txt. awk '$2 >= 90 { print $0 }' scores.txt Lucia 95 Joe 92 Sophia 90 We print all students with scores 90+. awk '$2 >= 90 { print }' scores.txt Lucia 95 Joe 92 Sophia 90 If we omit an argument for the print function, the $0 is assumed. awk '$2 >= 90' scores.txt Lucia 95 Joe 92 Sophia 90 A missing { action } means print the matching line. awk '{ if ($2 >= 90) print }' scores.txt Lucia 95 Joe 92 Sophia 90 Instead of a pattern, we can also use an if condition in the action. awk '{sum += $2} END { printf(\"The average score is %.2f\\n\", sum/NR) }' scores.txt The average score is 77.56 This command calculates the average score. In the action block, we calculate the sum of scores. In the END block, we print the average score. We format the output with the built-in printf function. The %.2f is a format specifier; each specifier begins with the % character. The .2 is the precision -- the number of digits after the decimal point. The f expects a floating point value. The \\n is not a part of the specifier; it is a newline character. It prints a newline after the string is shown on the terminal.","title":"AWK one-liners"},{"location":"lab_session/basics_awk/#awk-working-with-pipes","text":"AWK can receive input and send output to other commands via the pipe. echo -e \"1 2 3 5\\n2 2 3 8\" | awk '{print $(NF)}' 5 8 In this case, AWK receives output from the echo command. It prints the values of last column. awk -F: '$7 ~ /bash/ {print $1}' /etc/passwd | wc -l 3 Here, the AWK program sends data to the wc command via the pipe. In the AWK program, we find out those users who use bash. Their names are passed to the wc command which counts them. In our case, there are three users using bash.","title":"AWK working with pipes"},{"location":"lab_session/basics_awk/#awk-for-bioinformatics-looking-at-transcriptome-data","text":"You can find a lot of online tutorials, but here we will try out a few steps which show how a bioinformatician analyses a GTF file using awk. GTF is a special file format that contains information about different regions of the genome and their associated annotations. More on that here - Ensembl File Formats-GTF . wget transcriptome.gtf head transcriptome.gtf | less -S ##description: evidence-based annotation of the human genome (GRCh37), version 18 (Ensembl 73) ##provider: GENCODE ##contact: gencode@sanger.ac.uk ##format: gtf ##date: 2013-09-02 chr1 HAVANA exon 173753 173862 . - . gene_id \"ENSG00000241860.2\" ; transcript_id \"ENST00000466557.2\" ; gene_type \"processed_transcript\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.13\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"RP11-34P13.13-001\" ; exon_number 1 ; exon_id \"ENSE00001947154.2\" ; level 2 ; tag \"not_best_in_genome_evidence\" ; havana_gene \"OTTHUMG00000002480.3\" ; havana_transcript \"OTTHUMT00000007037.2\" ; chr1 HAVANA transcript 1246986 1250550 . - . gene_id \"ENSG00000127054.14\" ; transcript_id \"ENST00000478641.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"CPSF3L\" ; transcript_type \"retained_intron\" ; transcript_status \"KNOWN\" ; transcript_name \"CPSF3L-006\" ; level 2 ; havana_gene \"OTTHUMG00000003330.11\" ; havana_transcript \"OTTHUMT00000009365.1\" ; chr1 HAVANA CDS 1461841 1461911 . + 0 gene_id \"ENSG00000197785.9\" ; transcript_id \"ENST00000378755.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"ATAD3A\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"ATAD3A-003\" ; exon_number 13 ; exon_id \"ENSE00001664426.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS31.1\" ; havana_gene \"OTTHUMG00000000575.6\" ; havana_transcript \"OTTHUMT00000001365.1\" ; chr1 HAVANA exon 1693391 1693474 . - . gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000341991.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-002\" ; exon_number 3 ; exon_id \"ENSE00003487616.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30565.1\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002768.1\" ; chr1 HAVANA CDS 1688280 1688321 . - 0 gene_id \"ENSG00000008130.11\" ; transcript_id \"ENST00000497186.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NADK\" ; transcript_type \"nonsense_mediated_decay\" ; transcript_status \"KNOWN\" ; transcript_name \"NADK-008\" ; exon_number 2 ; exon_id \"ENSE00001856899.1\" ; level 2 ; tag \"mRNA_start_NF\" ; tag \"cds_start_NF\" ; havana_gene \"OTTHUMG00000000942.5\" ; havana_transcript \"OTTHUMT00000002774.3\" ; The transcriptome has 9 columns. The first 8 are separated by tabs and look reasonable (chromosome, annotation source, feature type, start, end, score, strand, and phase), the last one is kind of hairy: it is made up of key-value pairs separated by semicolons, some fields are mandatory and others are optional, and the values are surrounded in double quotes. That\u2019s no way to live a decent life. ( text copied from the source ) let's get only the lines that have gene in the 3 th column. $ awk -F '$3 == \"gene\"' transcriptome.gtf | head | less -S chr1 HAVANA gene 11869 14412 . + . gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; chr1 HAVANA gene 14363 29806 . - . gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; chr1 HAVANA gene 29554 31109 . + . gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; chr1 HAVANA gene 34554 36081 . - . gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; chr1 HAVANA gene 52473 54936 . + . gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; chr1 HAVANA gene 62948 63887 . + . gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; chr1 HAVANA gene 69091 70008 . + . gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; chr1 HAVANA gene 89295 133566 . - . gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; chr1 HAVANA gene 89551 91105 . - . gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; chr1 HAVANA gene 131025 134836 . + . gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; Perhaps filter a bit more and print the content of the 9 th column in the file. $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | head | less -S gene_id \"ENSG00000223972.4\" ; transcript_id \"ENSG00000223972.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"DDX11L1\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"DDX11L1\" ; level 2 ; havana_gene \"OTTHUMG00000000961.2\" ; gene_id \"ENSG00000227232.4\" ; transcript_id \"ENSG00000227232.4\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"WASH7P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"WASH7P\" ; level 2 ; havana_gene \"OTTHUMG00000000958.1\" ; gene_id \"ENSG00000243485.2\" ; transcript_id \"ENSG00000243485.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"MIR1302-11\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"MIR1302-11\" ; level 2 ; havana_gene \"OTTHUMG00000000959.2\" ; gene_id \"ENSG00000237613.2\" ; transcript_id \"ENSG00000237613.2\" ; gene_type \"lincRNA\" ; gene_status \"KNOWN\" ; gene_name \"FAM138A\" ; transcript_type \"lincRNA\" ; transcript_status \"KNOWN\" ; transcript_name \"FAM138A\" ; level 2 ; havana_gene \"OTTHUMG00000000960.1\" ; gene_id \"ENSG00000268020.2\" ; transcript_id \"ENSG00000268020.2\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G4P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G4P\" ; level 2 ; havana_gene \"OTTHUMG00000185779.1\" ; gene_id \"ENSG00000240361.1\" ; transcript_id \"ENSG00000240361.1\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"OR4G11P\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4G11P\" ; level 2 ; havana_gene \"OTTHUMG00000001095.2\" ; gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000238009.2\" ; transcript_id \"ENSG00000238009.2\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.7\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.7\" ; level 2 ; havana_gene \"OTTHUMG00000001096.2\" ; gene_id \"ENSG00000239945.1\" ; transcript_id \"ENSG00000239945.1\" ; gene_type \"lincRNA\" ; gene_status \"NOVEL\" ; gene_name \"RP11-34P13.8\" ; transcript_type \"lincRNA\" ; transcript_status \"NOVEL\" ; transcript_name \"RP11-34P13.8\" ; level 2 ; havana_gene \"OTTHUMG00000001097.2\" ; gene_id \"ENSG00000233750.3\" ; transcript_id \"ENSG00000233750.3\" ; gene_type \"pseudogene\" ; gene_status \"KNOWN\" ; gene_name \"CICP27\" ; transcript_type \"pseudogene\" ; transcript_status \"KNOWN\" ; transcript_name \"CICP27\" ; level 1 ; tag \"pseudo_consens\" ; havana_gene \"OTTHUMG00000001257.3\" ; What about if we want just a specific piece from this information? We can | the output from the first awk script in to a second one. Note that we will use different field separator \"; \" . $ awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\"","title":"AWK for Bioinformatics - looking at transcriptome data"},{"location":"lab_session/basics_awk/#chaining-awk-calls","text":"We will start with the AWK call that we were using before, and we will append a pipe | so it can be used as input for the next AWK call, this time using a space and a semicolon as the delimiter to define what a column is: awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | awk -F \"; \" '{ print $3 }' | head | less -S gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" gene_type \"pseudogene\" gene_type \"protein_coding\" gene_type \"lincRNA\" gene_type \"lincRNA\" gene_type \"pseudogene\" Now that we see what the third column looks like, we can filter for protein-coding genes awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 == \"gene_type \\\"protein_coding\\\"\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENSG00000186092.4\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5\" ; level 2 ; havana_gene \"OTTHUMG00000001094.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENSG00000237683.5\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1\" ; level 3 ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENSG00000235249.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29\" ; level 2 ; havana_gene \"OTTHUMG00000002860.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENSG00000185097.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16\" ; level 2 ; havana_gene \"OTTHUMG00000002581.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENSG00000269831.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1\" ; level 3 ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENSG00000269308.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2\" ; level 3 ; gene_id \"ENSG00000187634.6\" ; transcript_id \"ENSG00000187634.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"SAMD11\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"SAMD11\" ; level 2 ; havana_gene \"OTTHUMG00000040719.8\" ; gene_id \"ENSG00000268179.1\" ; transcript_id \"ENSG00000268179.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.1\" ; level 3 ; gene_id \"ENSG00000188976.6\" ; transcript_id \"ENSG00000188976.6\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"NOC2L\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"NOC2L\" ; level 2 ; havana_gene \"OTTHUMG00000040720.1\" ; gene_id \"ENSG00000187961.9\" ; transcript_id \"ENSG00000187961.9\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"KLHL17\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"KLHL17\" ; level 2 ; havana_gene \"OTTHUMG00000040721.6\" ; I added a space and a backslash (not to be confused with the regular slash /) after the first and second pipes to split the code into two lines; this makes it easier to read and it highlights that we are taking two separate steps. The double quotes around protein_coding are escaped (also with a backslash \\\") because they are already contained inside double quotes. To avoid the backslashing drama we can use the partial matching operator ~ instead of the total equality operator ==. awk -F \"\\t\" '$3 == \"gene\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S The output is the same as before: those lines that contain a protein_coding somewhere in their third column make the partial matching rule true, and they get printed (which is the default behavior when there are no curly braces). Now we have all the protein-coding genes, but how do we get to the genes that only have one exon? Well, we have to revisit our initial AWK call: we selected lines that corresponded to genes, but we actually want lines that correspond to exons. That\u2019s an easy fix, we just change the word \u201cgene\u201d for the word \u201cexon\u201d. Everything else stays the same. awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\"' | \\ head | less -S gene_id \"ENSG00000186092.4\" ; transcript_id \"ENST00000335137.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F5\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F5-001\" ; exon_number 1 ; exon_id \"ENSE00002319515.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS30547.1\" ; havana_gene \"OTTHUMG00000001094.1\" ; havana_transcript \"OTTHUMT00000003223.1\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 1 ; exon_id \"ENSE00002221580.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000237683.5\" ; transcript_id \"ENST00000423372.3\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"AL627309.1\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"AL627309.1-201\" ; exon_number 2 ; exon_id \"ENSE00002314092.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000235249.1\" ; transcript_id \"ENST00000426406.1\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F29\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F29-001\" ; exon_number 1 ; exon_id \"ENSE00002316283.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41220.1\" ; havana_gene \"OTTHUMG00000002860.1\" ; havana_transcript \"OTTHUMT00000007999.1\" ; gene_id \"ENSG00000185097.2\" ; transcript_id \"ENST00000332831.2\" ; gene_type \"protein_coding\" ; gene_status \"KNOWN\" ; gene_name \"OR4F16\" ; transcript_type \"protein_coding\" ; transcript_status \"KNOWN\" ; transcript_name \"OR4F16-001\" ; exon_number 1 ; exon_id \"ENSE00002324228.1\" ; level 2 ; tag \"basic\" ; tag \"CCDS\" ; ccdsid \"CCDS41221.1\" ; havana_gene \"OTTHUMG00000002581.1\" ; havana_transcript \"OTTHUMT00000007334.1\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 1 ; exon_id \"ENSE00003063549.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 2 ; exon_id \"ENSE00003084653.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269831.1\" ; transcript_id \"ENST00000599533.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL669831.1\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL669831.1-201\" ; exon_number 3 ; exon_id \"ENSE00003138540.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 1 ; exon_id \"ENSE00003079649.1\" ; level 3 ; tag \"basic\" ; gene_id \"ENSG00000269308.1\" ; transcript_id \"ENST00000594233.1\" ; gene_type \"protein_coding\" ; gene_status \"NOVEL\" ; gene_name \"AL645608.2\" ; transcript_type \"protein_coding\" ; transcript_status \"NOVEL\" ; transcript_name \"AL645608.2-201\" ; exon_number 2 ; exon_id \"ENSE00003048391.1\" ; level 3 ; tag \"basic\" ;","title":"Chaining AWK calls"},{"location":"lab_session/basics_awk/#cleaning-up-the-output","text":"Before we try to count how many exons belong to the same protein-coding gene, let\u2019s simplify the output so we only get the gene names (which are in column 5). awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ awk -F \"; \" '$3 ~ \"protein_coding\" {print $5}' | \\ head gene_name \"OR4F5\" gene_name \"AL627309.1\" gene_name \"AL627309.1\" gene_name \"OR4F29\" gene_name \"OR4F16\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL669831.1\" gene_name \"AL645608.2\" gene_name \"AL645608.2\" This is sort of what we want. We could chain another AWK call using -F \" \", and pick the second column (which would get rid of the gene_name). Feel free to try that approach if you are curious. We can also take a shortcut by using the tr -d command, which deletes whatever characters appear in double quotes. For example, to remove every vowel from a sentence: echo \"This unix thing is cool\" | tr -d \"aeiou\" # Ths nx thng s cl Let\u2019s try deleting all the semicolons and quotes before the second AWK call: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" '$6 == \"protein_coding\" {print $10}' | \\ head OR4F5 AL627309.1 AL627309.1 OR4F29 OR4F16 AL669831.1 AL669831.1 AL669831.1 AL645608.2 AL645608.2 Run bash awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | tr -d \";\\\"\" | head to understand what the input to the second AWK call looks like. It\u2019s just words separated by spaces; the sixth word corresponds to the gene type, and the tenth word to the gene name.","title":"Cleaning up the output"},{"location":"lab_session/basics_awk/#counting-genes","text":"There is one more concept we need to introduce before we start counting. AWK uses a special rule called END, which is only true once the input is over. See an example: echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { print $1 } END { print \"Done with letters!\" } ' a a b b b c Done with letters! The -e option tells echo to convert each \\n into a new line, which is a convenient way of printing multiple lines from a single character string. In AWK, any amount of whitespace is allowed between the initial and the final quote '. I separated the first rule from the END rule to make them easier to read. Now we are ready for counting. echo -e \"a\\na\\nb\\nb\\nb\\nc\" | \\ awk ' { counter[$1] += 1 } END { for (letter in counter){ print letter, counter[letter] } } ' a 2 b 3 c 1 Wow, what is all that madness? Instead of printing each letter, we manipulate a variable that we called counter. This variable is special because it is followed by brackets [ ], which makes it an associative array, a fancy way of calling a variable that stores key-value pairs. In this case we chose the values of the first column \\(1 to be the keys of the counter variable, which means there are 3 keys (\u201ca\u201d, \u201cb\u201d and \u201cc\u201d). The values are initialized to 0. For every line in the input, we add a 1 to the value in the array whose key is equal to \\(1. We use the addition operator +=, a shortcut for counter[\\) 1] = counter[\\) 1] + 1. When all the lines are read, the END rule becomes true, and the code between the curly braces { } is executed. The structure for (key in associate_array) { some_code } is called a for loop, and it executes some_code as many times as there are keys in the array. letter is the name that we chose for the variable that cycles through all the keys in counter, and counter[letter] gives the value stored in counter for each letter (which we we calculated in the previous curly brace chunk). Now we can apply this to the real example: awk -F \"\\t\" '$3 == \"exon\" { print $9 }' transcriptome.gtf | \\ tr -d \";\\\"\" | \\ awk -F \" \" ' $6 == \"protein_coding\" { gene_counter[$10] += 1 } END { for (gene_name in gene_counter){ print gene_name, gene_counter[gene_name] } }' > number_of_exons_by_gene.txt head number_of_exons_by_gene.txt CAPN6 24 ARL14EPL 3 DACH1 38 IFNA13 1 HSP90AB1 36 CAPN7 52 DACH2 84 IFNA14 1 LARS 188 CAPN8 78 If you are using the real transcriptome, it takes less than a minute to count up one million exons. Pretty impressive. We saved the output to a file, so now we can use AWK to see how many genes are made up of a single exon. awk '$2 == 1' number_of_exons_by_gene.txt | wc -l # 1362 I will suggest to follow the original tutorial if you need to refer to these steps later on for your own data: AWK GTF! How to Analyze a Transcriptome Like a Pro","title":"Counting genes"},{"location":"lectures/lectures/","text":"Lecture 1 : Sequencing evolution Introduction Exome sequencing vs WGS sequencing vs targeted sequencing PDF RNA sequencing vs targeted RNA sequening vs single cell sequencing PDF Lecture 2 : Cancer genome and mutational processes Mutational signatures Concept of cancer drivers Lecture 3 : Liquid biopsies A targeted sequencing assay Screening of localised disease Lecture 4 : Bioinformatics pipelines Snakemake Lecture 5 : HTC computing environments Slurm Lecture 6 : Calling somatic- and germline variation Copy-number alterations Ploidy Landmark CNA papers from ICGC/TCGA Lecture 7 : Variation for clinical use. How to curate somatic- and germline","title":"Day 1"}]}